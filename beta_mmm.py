# -*- coding: utf-8 -*-
"""Beta_MMM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14klC7g67_QAaPRaWFD-MODrI2GzTqHA8
"""

# Marketing Mix Modeling - Colab Testing Version (Optimized)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt # Uncommented for plotting
import seaborn as sns # Uncommented for plotting
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Add other necessary imports
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.optimize import minimize
from statsmodels.tsa.seasonal import seasonal_decompose
import matplotlib.ticker as mticker # Import for custom tick formatting

# Plotly imports (Commented out for potential optimization)
# import plotly.graph_objects as go
# import plotly.express as px
# from plotly.subplots import make_subplots # Import make_subplots
from IPython.display import display # Keep display for DataFrames

# Set style for plots
plt.style.use('seaborn-v0_8-darkgrid') # Use a seaborn style
sns.set_palette("husl")


# Function to convert Indian number format to float
def convert_indian_number(value):
    """Convert Indian number format string to float"""
    if isinstance(value, str):
        # Remove commas and strip whitespace
        cleaned_value = value.replace(',', '').strip()

        # Handle special cases like ' -   ' which should be treated as NaN
        if cleaned_value in ['-', ''] or cleaned_value.isspace():
            return np.nan

        try:
            return float(cleaned_value)
        except ValueError:
            print(f"Could not convert value: '{value}'")
            return np.nan
    return value

# Load and preprocess data
def load_and_preprocess_data(file_path):
    """Load and preprocess the marketing mix data"""
    # Load the data
    data = pd.read_csv(file_path)

    # First, identify all columns that might contain Indian number format
    # These are all columns except the date column
    all_columns = data.columns.tolist()
    date_column = 'Week_Ending'

    if date_column in all_columns:
        all_columns.remove(date_column)

    # Convert all numeric columns (including Sales) - Optimized conversion
    # Apply conversion only to object columns that are likely numbers
    for col in all_columns:
        if data[col].dtype == 'object':
            # Use a more robust check for potential numbers before applying conversion
            try:
                 # Attempt to convert a sample to detect if it's numeric strings
                 data[col].sample(min(10, len(data))).astype(str).str.replace(',', '').astype(float)
                 # If the sample conversion works, apply to the whole column
                 data[col] = data[col].apply(convert_indian_number)
            except (ValueError, AttributeError):
                 # If sample conversion fails, it's likely not a numeric column with commas
                 pass


    # Handle missing values in Paid Search Impressions
    if 'Paid Search Impressions' in data.columns:
        missing_count = data['Paid Search Impressions'].isna().sum()
        if missing_count > 0:
            print(f"Found {missing_count} missing values in 'Paid Search Impressions'. Imputing with 0.")
            data['Paid Search Impressions'] = data['Paid Search Impressions'].fillna(0)

    # Convert date column
    if 'Week_Ending' in data.columns:
        # Use infer_datetime_format=True and cache=True for potentially faster conversion
        data['Week_Ending'] = pd.to_datetime(data['Week_Ending'], format='%d-%m-%Y %H:%M', errors='coerce', infer_datetime_format=True, cache=True)
        data = data.sort_values('Week_Ending').reset_index(drop=True)

    return data


# Perform comprehensive EDA
def perform_comprehensive_eda(data, target_var='Sales'):
    """Perform comprehensive exploratory data analysis"""
    print("="*60)
    print("COMPREHENSIVE EXPLORATORY DATA ANALYSIS")
    print("="*60)

    # 1. Basic Information
    print("\n1. BASIC DATASET INFORMATION")
    print("="*40)
    print(f"Shape: {data.shape}")
    print(f"Columns: {list(data.columns)}")
    if 'Week_Ending' in data.columns:
        print(f"Date Range: {data['Week_Ending'].min()} to {data['Week_Ending'].max()}")
    print(f"Missing Values: {data.isnull().sum().sum()}")

    # Check if target variable is numeric
    if data[target_var].dtype == 'object':
         print(f"\nWARNING: Target variable '{target_var}' is not numeric after preprocessing.")


    # 2. Summary Statistics
    print("\n\n2. SUMMARY STATISTICS")
    print("="*40)

    # Numeric variables summary
    numeric_df = data.select_dtypes(include=[np.number])
    print("Numeric Variables Summary:")
    display(numeric_df.describe())

    # Add skewness and kurtosis
    skewness = numeric_df.skew().to_frame('Skewness')
    kurtosis = numeric_df.kurtosis().to_frame('Kurtosis')
    stats_df = pd.concat([skewness, kurtosis], axis=1)
    print("\nSkewness and Kurtosis:")
    display(stats_df)

    # 3. Univariate Analysis (Using Matplotlib/Seaborn)
    print("\n\n3. UNIVARIATE ANALYSIS")
    print("="*40)

    # Create distribution plots for all numeric variables
    numeric_cols = numeric_df.columns.tolist()
    for col in numeric_cols:
        plt.figure(figsize=(10, 6))
        sns.histplot(data=data, x=col, kde=True, bins=30)
        plt.title(f"Distribution of {col}")
        plt.ylabel("Frequency")

        # Add vertical lines for mean and median with improved formatting
        mean_val = data[col].mean()
        median_val = data[col].median()

        # Conditional formatting for labels and axis ticks based on column name
        if 'impressions' in col.lower() or 'clicks' in col.lower():
            plt.xlabel(f"{col} (Millions)") # Indicate units in label
            mean_label = f'Mean: {mean_val/1e6:.2f}M' # Show in millions for labels
            median_label = f'Median: {median_val/1e6:.2f}M'
            plt.ticklabel_format(style='plain', axis='x') # Turn off scientific notation for x-axis
            # Manually format x-axis ticks for millions
            plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))

        elif 'discount' in col.lower():
             plt.xlabel(f"{col} (%)") # Indicate units in label
             mean_label = f'Mean: {mean_val*100:.2f}%' # Show as percentage for labels
             median_label = f'Median: {median_val*100:.2f}%'
             # Format x-axis ticks as percentages
             plt.gca().xaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0)) # xmax=1.0 because data is 0-1 range

        elif col == target_var:
             plt.xlabel(col)
             mean_label = f'Mean: {mean_val/1e6:.2f}M' # Show in millions for labels
             median_label = f'Median: {median_val/1e6:.2f}M'
             plt.ticklabel_format(style='plain', axis='x') # Turn off scientific notation for x-axis
             plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # Format ticks in millions
        else:
            plt.xlabel(col)
            mean_label = f'Mean: {mean_val:,.2f}' # General formatting with commas
            median_label = f'Median: {median_val:,.2f}'
            plt.ticklabel_format(style='plain', axis='x') # Turn off scientific notation for x-axis


        plt.axvline(mean_val, color='red', linestyle='dashed', linewidth=1, label=mean_label)
        plt.axvline(median_val, color='green', linestyle='dashed', linewidth=1, label=median_label)
        plt.legend()
        plt.show()


    # 4. Bivariate Analysis (Using Matplotlib/Seaborn)
    print("\n\n4. BIVARIATE ANALYSIS: RELATIONSHIP WITH TARGET VARIABLE")
    print("="*40)

    # Create scatter plots against target variable
    if target_var in numeric_cols:
        numeric_cols_for_scatter = numeric_cols.copy()
        numeric_cols_for_scatter.remove(target_var)

    for col in numeric_cols_for_scatter:
        plt.figure(figsize=(10, 6))
        sns.scatterplot(data=data, x=col, y=target_var)
        plt.title(f"{target_var} vs {col}")

        # Conditional formatting for x-axis label and potentially ticks
        if 'impressions' in col.lower() or 'clicks' in col.lower():
            plt.xlabel(f"{col} (Millions)") # Indicate units in label
            plt.ticklabel_format(style='plain', axis='x')
            plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # Format ticks in millions
        elif 'discount' in col.lower():
             plt.xlabel(f"{col} (%)") # Indicate units in label
             plt.gca().xaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0)) # Format ticks as percentage

        else:
            plt.xlabel(col)
            plt.ticklabel_format(style='plain', axis='x') # Turn off scientific notation for x-axis


        # Conditional formatting for y-axis label (target variable) and ticks
        if target_var in numeric_df.columns: # Ensure target variable is numeric before formatting
            plt.ylabel(f"{target_var} (Millions)")
            plt.ticklabel_format(style='plain', axis='y')
            plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # Format ticks in millions
        else:
             plt.ylabel(target_var) # No formatting if target is not numeric


        # Add correlation coefficient as text
        correlation = data[col].corr(data[target_var])
        # Position the text box based on formatted axis limits (more complex, so using a simple approach first)
        # Simple approach: Use relative positioning within the plot area
        plt.text(data[col].min() + (data[col].max() - data[col].min()) * 0.05,
                 data[target_var].max() - (data[target_var].max() - data[target_var].min()) * 0.05,
                 f"r = {correlation:.3f}",
                 fontsize=12, bbox=dict(facecolor='white', alpha=0.8))
        plt.show()


    # 5. Time Series Analysis (Using Matplotlib/Seaborn)
    print("\n\n5. TIME SERIES ANALYSIS")
    print("="*40)

    if 'Week_Ending' in data.columns:
        # Plot target variable over time
        plt.figure(figsize=(12, 6))
        sns.lineplot(data=data, x='Week_Ending', y=target_var)
        plt.title(f"{target_var} Over Time")
        plt.xlabel("Date")
        plt.ylabel(f"{target_var} (Millions)") # Indicate units in label
        # Format y-axis to show values in millions
        plt.ticklabel_format(style='plain', axis='y') # Turn off scientific notation
        plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))
        plt.show()


        # Add seasonal decomposition
        print("Seasonal Decomposition:")
        try:
            # Ensure the data is sorted by date and set as index
            temp_df = data.set_index('Week_Ending').sort_index()
            # Use a smaller period if data is limited or seasonality is expected to be short
            # For weekly data, a period of 52 (for annual seasonality) or 4 (for monthly/quarterly) might be appropriate
            # Let's keep 4 as in the original code, assuming quarterly or shorter seasonality might be relevant
            decomposition = seasonal_decompose(temp_df[target_var], period=4, model='additive', extrapolate_trend='freq')

            fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)

            # Apply millions formatting to seasonal decomposition plots
            axes[0].plot(decomposition.observed)
            axes[0].set_ylabel("Observed (M)")
            axes[0].set_title("Seasonal Decomposition")
            axes[0].ticklabel_format(style='plain', axis='y')
            axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))


            axes[1].plot(decomposition.trend)
            axes[1].set_ylabel("Trend (M)")
            axes[1].ticklabel_format(style='plain', axis='y')
            axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))


            axes[2].plot(decomposition.seasonal)
            axes[2].set_ylabel("Seasonal (M)")
            axes[2].ticklabel_format(style='plain', axis='y')
            axes[2].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))


            axes[3].plot(decomposition.resid)
            axes[3].set_ylabel("Residual (M)")
            axes[3].ticklabel_format(style='plain', axis='y')
            axes[3].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))


            plt.xlabel("Date")
            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"Could not perform seasonal decomposition: {str(e)}")

    # 6. Correlation Analysis (Using Matplotlib/Seaborn)
    print("\n\n6. CORRELATION ANALYSIS")
    print("="*40)

    # Full correlation matrix
    print("Full Correlation Matrix:")
    corr = numeric_df.corr()
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", linewidths=".5")
    plt.title("Correlation Matrix - All Variables")
    plt.show()

    # Media variables correlation
    media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video']
    # Ensure media_cols are numeric before calculating correlation
    media_cols = [col for col in numeric_df.columns if any(keyword in col.lower() for keyword in media_keywords)]
    media_cols = [col for col in media_cols if col in numeric_df.columns] # Filter to ensure they are in numeric_df


    if media_cols and target_var in numeric_df.columns:
        print("Media Variables Correlation with Target:")
        # Ensure only numeric columns are included in correlation calculation
        media_corr = numeric_df[media_cols + [target_var]].corr()
        # Extract only correlations with target variable
        target_corr = media_corr[target_var].drop(target_var).sort_values(ascending=False)

        # Plot correlations as vertical bars
        plt.figure(figsize=(10, 6))
        sns.barplot(x=target_corr.index, y=target_corr.values)
        plt.title(f"Correlation of Media Variables with {target_var}")
        plt.xlabel("Media Variables")
        plt.ylabel("Correlation Coefficient")
        plt.xticks(rotation=-45, ha='left') # Rotate x-axis labels
        plt.tight_layout() # Adjust layout to prevent labels overlapping
        plt.show()


        # Also show as a table
        print(target_corr.round(3).to_frame("Correlation"))

        # 6.1 Media Execution Share - Change to Bar Chart (Using Matplotlib/Seaborn)
        print("\n\n6.1 MEDIA EXECUTION SHARE")
        print("="*40)

        # Calculate total media execution (sum of all media variables) - Ensure only numeric columns are summed
        media_totals = numeric_df[media_cols].sum()
        total_media = media_totals.sum()

        if total_media > 0:
            # Calculate share percentage
            media_share = (media_totals / total_media) * 100

            # Sort by share for better visualization
            media_share = media_share.sort_values(ascending=False)

            # Create bar chart instead of pie chart
            plt.figure(figsize=(10, 8))
            sns.barplot(x=media_share.index, y=media_share.values, palette='husl')
            plt.title("Media Execution Share by Channel")
            plt.xlabel("Channel")
            plt.ylabel("Share (%)")
            plt.xticks(rotation=-45, ha='left') # Rotate x-axis labels
            plt.tight_layout() # Adjust layout
            plt.show()


            # Display share percentages as table
            print("Media Execution Share Percentage:")
            display(media_share.round(2).to_frame("Share (%)"))
        else:
            print("Total media execution is 0. Cannot generate share chart.")


    # 7. Outlier Analysis (Using Matplotlib)
    print("\n\n7. OUTLIER ANALYSIS")
    print("="*40)

    # Check for outliers in target variable
    # Ensure target variable is numeric
    if target_var in numeric_df.columns:
        Q1 = data[target_var].quantile(0.25)
        Q3 = data[target_var].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Identify outliers
        outliers = data[(data[target_var] < lower_bound) | (data[target_var] > upper_bound)]
        normal_data = data[~((data[target_var] < lower_bound) | (data[target_var] > upper_bound))]
        print(f"Number of potential outliers in {target_var}: {len(outliers)}")

        if len(outliers) > 0:
            print("Outlier values:")
            display(outliers[['Week_Ending', target_var]])

            # Plot with outliers highlighted
            plt.figure(figsize=(12, 6))
            sns.lineplot(data=normal_data, x='Week_Ending', y=target_var, label='Normal Values', color='darkgreen', marker='o', markersize=5)
            sns.scatterplot(data=outliers, x='Week_Ending', y=target_var, color='red', label='Outliers', s=100, marker='X') # Use scatterplot for outliers

            plt.axhline(upper_bound, color='red', linestyle='dashed', linewidth=1, label='Upper Bound')
            plt.axhline(lower_bound, color='red', linestyle='dashed', linewidth=1, label='Lower Bound')

            plt.title(f"Outlier Detection in {target_var}")
            plt.xlabel("Date")
            plt.ylabel(f"{target_var} (Millions)") # Indicate units in label
            plt.legend()
            plt.ticklabel_format(style='plain', axis='y') # Turn off scientific notation
            plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # Format ticks in millions
            plt.show()
    else:
        print(f"Target variable '{target_var}' is not numeric. Cannot perform outlier analysis.")


    # Return data, numeric_df, and correlation matrix for interactive use
    # Ensure returned dataframes are copies to prevent unintentional modification outside the function
    return data.copy(), numeric_df.copy(), corr.copy()


# Main execution
if __name__ == "__main__":
    # Load and preprocess your data
    file_path = "/content/Book1.csv"  # Update this path to your file location
    data = load_and_preprocess_data(file_path)

    # Define the target variable here
    target_var = 'Sales'

    # Perform comprehensive EDA and get the data, numeric dataframe and correlation matrix
    data, numeric_df, correlations = perform_comprehensive_eda(data, target_var=target_var)

    # Save data to global scope
    # Use get_ipython().user_ns to store variables
    get_ipython().user_ns['data'] = data
    get_ipython().user_ns['numeric_df'] = numeric_df
    get_ipython().user_ns['correlations'] = correlations

    print("\nEDA Complete.")

# ================================================
# OPTIMIZED FEATURE ENGINEERING AND MEDIA PERFORMANCE REPORTING MODULE
# ================================================
import pandas as pd
import numpy as np
from statsmodels.tsa.seasonal import seasonal_decompose
from IPython.display import display, HTML
import warnings
warnings.filterwarnings('ignore')

# Define helper function for YoY formatting with colors and special cases
def format_yoy(yoy_val, base_val):
    """
    Formats YoY percentage with colors and handles special cases (NaN, Inf, 0).
    Checks base_val to differentiate between 0->0 and 0->Inf.
    """
    if pd.isna(yoy_val):
        return 'N/A'
    elif np.isinf(yoy_val):
        if pd.notna(base_val) and base_val == 0:
            return 'No previous data'
        else:
            return '<span style="color:green;">Inf%</span>'
    elif yoy_val == 0.0:
        return '0.00%'
    else:
        color = 'green' if yoy_val > 0 else 'red'
        formatted_val = f'{yoy_val:,.2f}%'
        return f'<span style="color:{color};">{formatted_val}</span>'

# Global dictionary to store original channel names
original_paid_media_mapping = {}

def feature_engineering_and_reporting_module(df):
    """
    Optimized feature engineering module for time series data.
    Includes media spend estimation and performance reporting.
    """
    print("\n===== FEATURE ENGG & MEDIA REPORTING MODULE =====")

    # Check for required columns
    required_cols = ['Week_Ending', 'Sales']
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"DataFrame must contain a '{col}' column")

    # Create a copy but avoid unnecessary deep copying
    df = df.copy(deep=False)

    # Track original columns at the start
    initial_columns = set(df.columns)

    # Ensure datetime index and sort - optimized
    if not pd.api.types.is_datetime64_any_dtype(df['Week_Ending']):
        df['Week_Ending'] = pd.to_datetime(df['Week_Ending'], errors='coerce')

    df = df.set_index('Week_Ending').sort_index()
    df.index.name = 'Week_Ending'  # Ensure index has proper name

    # Define constants
    TARGET = "Sales"
    WEEK_COL = "Week_Ending"

    # Define keywords for identifying engineered volume features
    engineered_volume_keywords = ['_pre', '_post', 'combined_var', 'super', '_volume']
    non_media_keywords = [TARGET, WEEK_COL, 'Discount', 'Total SKU', 'Gasoline Price', 'Average Price', 'SIndex', 'Holiday_']

    # --- MEDIA SPEND ESTIMATION ---
    print("\n--- MEDIA SPEND ESTIMATION ---")

    # Identify original paid media columns
    media_keywords_check = ['impressions', 'impr', 'clicks', 'social', 'search', 'video', 'tv', 'display', 'email', 'paid social']

    original_paid_media_cols = [
        col for col in initial_columns
        if any(keyword in col.lower() for keyword in media_keywords_check)
        and 'organic' not in col.lower()
        and not any(nmk.lower() in col.lower() for nmk in non_media_keywords)
        and col != TARGET and col != WEEK_COL
    ]

    # Store globally for later reference
    global original_paid_media_mapping
    original_paid_media_mapping = {col: col for col in original_paid_media_cols}

    print(f"\nIdentified Original Paid Media Channels for Spend Estimation: {original_paid_media_cols}")

    estimated_rates_yearly = {}
    estimated_spend_cols = []

    if original_paid_media_cols:
        print("\n===== ENTER ESTIMATED CPM/CPC RATES BY YEAR =====")
        print("Please provide the estimated rate for each original paid media channel, for each relevant year.")
        print("Rates should correspond to either Impressions (CPM) or Clicks (CPC).")
        print("Enter 0 if you don't have a rate for a specific channel/year.")

        # Get years present in the data - optimized
        years_in_data = sorted(df.index.year.unique().tolist())
        df['Fiscal_Year_Temp'] = df.index.year  # Temporary column for year mapping

        for channel in original_paid_media_cols:
            estimated_rates_yearly[channel] = {}

            # Determine metric type for prompting
            channel_lower = channel.lower()
            is_cpm_name = any(kw in channel_lower for kw in ['impressions', 'impr', 'video', 'tv', 'display'])
            is_cpc_name = 'clicks' in channel_lower

            prompt_metric_type = "Impressions (CPM - Cost Per 1000)" if is_cpm_name else \
                                "Clicks (CPC - Cost Per Click)" if is_cpc_name else \
                                "Volume (Enter appropriate rate, e.g., CPM/CPC)"

            for year in years_in_data:
                while True:
                    try:
                        # Check for predefined rates
                        predefined_rate_key = f'rate_{channel}_{year}'
                        ipython = get_ipython()
                        if ipython and predefined_rate_key in ipython.user_ns:
                            rate = ipython.user_ns[predefined_rate_key]
                            print(f"Using predefined rate for '{channel}' for FY {year}: {rate}")
                            estimated_rates_yearly[channel][year] = float(rate)
                            break

                        # Interactive input
                        rate_input = input(f"Enter rate for '{channel}' for FY {year} ({prompt_metric_type}): ").strip()
                        if not rate_input:
                            print(f"ℹ️ No rate entered for '{channel}' for FY {year}. Assuming rate is 0.")
                            estimated_rates_yearly[channel][year] = 0.0
                            break

                        rate = float(rate_input)
                        if rate < 0:
                            print("⚠️ Rate cannot be negative. Please enter a positive number or 0.")
                            continue

                        estimated_rates_yearly[channel][year] = rate
                        print(f"✅ Rate stored for '{channel}' for FY {year}: {rate}")
                        break
                    except ValueError:
                        print("⚠️ Invalid input. Please enter a numeric value (e.g., 0.5, 10.25).")
                    except EOFError:
                        print("\nInput ended. Skipping remaining rate inputs.")
                        raise

        # Calculate estimated spend columns for original channels - optimized vectorized approach
        print("\n===== CALCULATING ESTIMATED SPEND FOR ORIGINAL CHANNELS =====")

        for channel, rates_by_year in estimated_rates_yearly.items():
            if channel not in df.columns:
                print(f"⚠️ Warning: Original channel column '{channel}' not found in DataFrame. Cannot calculate spend.")
                continue

            spend_col_name = f"{channel}_Spend"
            channel_lower = channel.lower()

            # Determine calculation type
            is_cpm_calc = any(kw in channel_lower for kw in ['impressions', 'impr', 'video', 'tv', 'display'])
            is_cpc_calc = 'clicks' in channel_lower

            if not is_cpm_calc and not is_cpc_calc:
                print(f"⚠️ Warning: Channel '{channel}' type unclear for calculation. Skipping spend for this channel.")
                continue

            # Convert channel data to numeric, handling errors
            try:
                # First try to convert to numeric
                channel_data = pd.to_numeric(df[channel], errors='coerce')

                # Check if conversion resulted in many NaN values
                if channel_data.isna().sum() > 0:
                    print(f"⚠️ Warning: Channel '{channel}' has non-numeric values. Converting to 0.")
                    channel_data = channel_data.fillna(0)

                # Create a rate mapping series for vectorized calculation
                rate_series = df['Fiscal_Year_Temp'].map(rates_by_year).fillna(0)

                # Calculate spend based on type
                if is_cpm_calc:
                    df[spend_col_name] = (channel_data / 1000) * rate_series
                elif is_cpc_calc:
                    df[spend_col_name] = channel_data * rate_series

                # Check if any spend was calculated
                if (df[spend_col_name] > 0).any():
                    estimated_spend_cols.append(spend_col_name)
                    print(f"  - Calculated {spend_col_name} using yearly rates.")
                else:
                    df.drop(columns=[spend_col_name], inplace=True)
                    print(f"  - No positive rates provided for '{channel}' across years. {spend_col_name} not added.")

            except Exception as e:
                print(f"❌ Error calculating spend for '{channel}': {e}")
                if spend_col_name in df.columns:
                    df.drop(columns=[spend_col_name], inplace=True)

        # Drop the temporary column
        if 'Fiscal_Year_Temp' in df.columns:
            df.drop(columns=['Fiscal_Year_Temp'], inplace=True)

        # Display head of the DataFrame with new spend columns
        if estimated_spend_cols:
            print("\n📋 DataFrame head with new estimated spend columns for original channels:")
            display_cols = [col for col in df.columns if any(media_col in col for media_col in original_paid_media_cols)]
            display(df[display_cols].head())
        else:
            print("\nℹ️ No estimated spend columns were created for original channels.")
    else:
        print("\nℹ️ No Original Paid Media Channels identified for spend estimation.")

    # --- FEATURE ENGINEERING STEPS ---
    print("\n--- FEATURE ENGINEERING STEPS (Applied to Volume & Spend) ---")

    # 1. Seasonal Index (SIndex) - Interactive selection
    print("\n===== SEASONAL DECOMPOSITION =====")
    print("Select Seasonal Decomposition Period:")
    print("Options: [1] 4 (Quarterly), [2] 13 (Lunar/Quarterly), [3] 26 (Bi-annual), [4] 52 (Annual)")

    seasonal_period_options = {1: 4, 2: 13, 3: 26, 4: 52}
    selected_period = 52  # Default to annual

    try:
        period_choice = input(f"Enter choice (1-4) or press Enter for default ({selected_period}): ").strip()
        if period_choice and period_choice.isdigit():
            period_idx = int(period_choice)
            selected_period = seasonal_period_options.get(period_idx, selected_period)
    except (EOFError, Exception):
        pass  # Use default on any error

    print(f"✅ Selected seasonal decomposition period: {selected_period}.")

    # Apply seasonal decomposition with selected period
    if TARGET in df.columns:
        print("\nPerforming Seasonal Decomposition:")
        try:
            temp_series = df[TARGET].copy()

            # Handle missing values
            if temp_series.isna().any():
                temp_series = temp_series.fillna(method='ffill').fillna(method='bfill')

            # Perform seasonal decomposition
            decomposition = seasonal_decompose(temp_series, period=selected_period,
                                             model='additive', extrapolate_trend='freq')

            # Add seasonal component as SIndex
            df['SIndex'] = decomposition.seasonal.values
            print(f"✅ Seasonal Index (SIndex) created using period {selected_period}.")
        except Exception as e:
            print(f"❌ Could not perform seasonal decomposition: {str(e)}. SIndex not created.")
    else:
        print(f"⚠️ Target variable '{TARGET}' not found. Cannot perform seasonal decomposition.")

    # 2. Custom Dummy Variables - Interactive creation
    print("\n===== CUSTOM DUMMY VARIABLES =====")
    custom_dummy_cols = []

    try:
        dummy_choice = input("Do you want to add custom dummy variables? (Y/N): ").strip().lower()
        if dummy_choice == 'y':
            print("\nEnter comma-separated dates for the custom dummy variables (YYYY-MM-DD).")
            print("For example: '2022-11-26,2023-11-26,2024-01-01'")
            print("Dummy variables will be automatically named based on dates (dummy1_2022-11-26, dummy2_2023-11-26, etc.)")

            dates_input = input("Enter comma-separated dates: ").strip()

            if dates_input:
                try:
                    # Parse dates with proper error handling - optimized
                    date_strings = [d.strip() for d in dates_input.split(',') if d.strip()]
                    date_objects = pd.to_datetime(date_strings, format='%Y-%m-%d', errors='coerce')
                    valid_dates = date_objects[date_objects.notna()]

                    if len(valid_dates) > 0:
                        # Precompute index dates for vectorized comparison
                        index_dates = df.index.normalize()  # Normalize to remove time component

                        for i, date_obj in enumerate(valid_dates, 1):
                            dummy_name = f"dummy{i}_{date_obj.strftime('%Y-%m-%d')}"
                            # Vectorized creation of dummy variable
                            df[dummy_name] = (index_dates == date_obj).astype(int)
                            custom_dummy_cols.append(dummy_name)
                            print(f"✅ Custom dummy '{dummy_name}' created for date: {date_obj.strftime('%Y-%m-%d')}.")
                    else:
                        print("⚠️ No valid dates entered. Skipping custom dummy creation.")
                except Exception as e:
                    print(f"❌ An error occurred while creating dummies: {e}")
    except (EOFError, Exception):
        print("⚠️ Skipping custom dummy creation due to input issue.")

    # 3. Split Variable (Apply to Volume AND Spend)
    try:
        split_choice = input("\nDo you want to split a variable at a date? (Y/N): ").strip().lower()
        if split_choice == 'y':
            print("\nAvailable variables for splitting:")

            # Exclude certain columns from split options
            exclude_from_split = ['SIndex', TARGET, 'Fiscal_Year'] + custom_dummy_cols
            all_vars_for_split = [
                col for col in df.columns
                if col not in exclude_from_split
                and not col.startswith('Holiday_')
                and '_Spend' not in col
            ]

            for i, var in enumerate(all_vars_for_split, 1):
                print(f"{i}. {var}")

            try:
                var_idx = int(input("Select variable number to split: ")) - 1
                if 0 <= var_idx < len(all_vars_for_split):
                    var_name = all_vars_for_split[var_idx]

                    # Check if this variable has a spend counterpart
                    spend_counterpart = f"{var_name}_Spend"
                    has_spend_counterpart = spend_counterpart in df.columns

                    split_date_str = input("Enter split date (YYYY-MM-DD): ").strip()

                    try:
                        split_dt = pd.to_datetime(split_date_str)

                        # Check if the split date is within the data's date range
                        if split_dt < df.index.min() or split_dt > df.index.max():
                            print(f"⚠️ Split date {split_date_str} is outside the data range. Split may not be meaningful.")

                        # Ensure the original variable is numeric before splitting
                        if not pd.api.types.is_numeric_dtype(df[var_name]):
                            # Try to convert to numeric
                            df[var_name] = pd.to_numeric(df[var_name], errors='coerce').fillna(0)
                            print(f"⚠️ Converted '{var_name}' to numeric for splitting.")

                        # Vectorized splitting
                        pre_mask = df.index <= split_dt

                        # Split the volume column
                        df[f"{var_name}_pre"] = df[var_name].where(pre_mask, 0)
                        df[f"{var_name}_post"] = df[var_name].where(~pre_mask, 0)

                        # If there is a spend counterpart, split that too
                        if has_spend_counterpart:
                            # Ensure spend column is numeric
                            if not pd.api.types.is_numeric_dtype(df[spend_counterpart]):
                                df[spend_counterpart] = pd.to_numeric(df[spend_counterpart], errors='coerce').fillna(0)
                                print(f"⚠️ Converted '{spend_counterpart}' to numeric for splitting.")

                            df[f"{spend_counterpart}_pre"] = df[spend_counterpart].where(pre_mask, 0)
                            df[f"{spend_counterpart}_post"] = df[spend_counterpart].where(~pre_mask, 0)
                            print(f"✅ Split spend counterpart {spend_counterpart}")

                        # Drop the original variable(s)
                        cols_to_drop = [var_name]
                        if has_spend_counterpart and spend_counterpart in df.columns:
                            cols_to_drop.append(spend_counterpart)
                            if spend_counterpart in estimated_spend_cols:
                                estimated_spend_cols.remove(spend_counterpart)

                        df.drop(columns=cols_to_drop, inplace=True)
                        print(f"✅ Split {var_name} at {split_dt.date()} and dropped original variable(s)")
                    except ValueError:
                        print("⚠️ Invalid date format. Please use YYYY-MM-DD.")
                else:
                    print("⚠️ Invalid selection")
            except ValueError:
                print("⚠️ Invalid input for variable number.")
    except (EOFError, Exception):
        print("⚠️ Skipping variable splitting due to input issue.")

    # 4. Super Campaign (Apply to Volume AND Spend)
    try:
        super_choice = input("\nDo you want to create a super campaign? (Y/N): ").strip().lower()
        if super_choice == 'y':
            print("\nAvailable variables for combining:")

            # Exclude certain columns from super campaign options
            exclude_from_super = ['SIndex', TARGET, 'Fiscal_Year'] + custom_dummy_cols
            all_vars_for_super = [
                col for col in df.columns
                if col not in exclude_from_super
                and not col.startswith('Holiday_')
                and '_Spend' not in col
                and not col.endswith(('_pre', '_post'))
            ]

            for i, var in enumerate(all_vars_for_super, 1):
                print(f"{i}. {var}")

            try:
                selected = input("Enter variable numbers to combine (comma separated): ")
                var_indices = [
                    int(x.strip())-1 for x in selected.split(",")
                    if x.strip().isdigit() and 0 <= int(x.strip())-1 < len(all_vars_for_super)
                ]
                vars_to_combine = [all_vars_for_super[i] for i in var_indices]

                if vars_to_combine:
                    # Ask for custom name
                    name_choice = input("Do you want to provide a custom name for the Super Campaign? (Y/N): ").strip().lower()
                    if name_choice == 'y':
                        super_base_name = input("Enter the custom name (e.g., Super_Media): ").strip()
                        if not super_base_name:
                            super_base_name = "Super_Campaign"
                    else:
                        super_base_name = "Super_Campaign"

                    super_volume_col = f"{super_base_name}_Volume"
                    super_spend_col = f"{super_base_name}_Spend"

                    # Check for non-numeric variables and convert them
                    for var in vars_to_combine:
                        if not pd.api.types.is_numeric_dtype(df[var]):
                            df[var] = pd.to_numeric(df[var], errors='coerce').fillna(0)
                            print(f"⚠️ Converted '{var}' to numeric for super campaign.")

                    # Check for non-numeric variables
                    non_numeric_vars = [v for v in vars_to_combine if not pd.api.types.is_numeric_dtype(df[v])]
                    if non_numeric_vars:
                        print(f"⚠️ Cannot combine non-numeric variables: {non_numeric_vars}. Skipping Super Campaign.")
                    else:
                        # Check for spend counterparts
                        spend_vars_to_sum = [
                            f"{v}_Spend" for v in vars_to_combine
                            if f"{v}_Spend" in df.columns
                        ]

                        # Ensure spend columns are numeric
                        for spend_var in spend_vars_to_sum:
                            if not pd.api.types.is_numeric_dtype(df[spend_var]):
                                df[spend_var] = pd.to_numeric(df[spend_var], errors='coerce').fillna(0)
                                print(f"⚠️ Converted '{spend_var}' to numeric for super campaign.")

                        # Create Super Campaign Volume
                        df[super_volume_col] = df[vars_to_combine].sum(axis=1)

                        # Create Super Campaign Spend if any spend variables exist
                        if spend_vars_to_sum:
                            df[super_spend_col] = df[spend_vars_to_sum].sum(axis=1)
                            estimated_spend_cols.append(super_spend_col)
                            print(f"✅ Created Super Campaign Spend: {super_spend_col}")

                        # Drop the original volume variables
                        vars_to_drop = [v for v in vars_to_combine if v in df.columns]
                        if vars_to_drop:
                            df.drop(columns=vars_to_drop, inplace=True)
                            print(f"✅ Dropped original volume variable(s): {vars_to_drop}")
                else:
                    print("⚠️ No valid variables selected")
            except ValueError:
                print("⚠️ Invalid input for variable numbers.")
    except (EOFError, Exception):
        print("⚠️ Skipping super campaign creation due to input issue.")

    # --- MEDIA PERFORMANCE REPORTING ---
    print("\n\n===== MEDIA SPEND ESTIMATION & PERFORMANCE REPORTING =====")
    print("-" * 60)

    # Ensure 'Fiscal_Year' is in df for grouping
    if 'Fiscal_Year' not in df.columns:
        df['Fiscal_Year'] = df.index.year
        print("✅ Created 'Fiscal_Year' column from index.")

    # Identify columns for reporting
    media_keywords_check_reporting = ['impressions', 'impr', 'clicks', 'social', 'search', 'video', 'tv', 'display', 'email', 'paid social']
    non_media_keywords_reporting = [TARGET, WEEK_COL, 'Discount', 'Total SKU', 'Gasoline Price', 'Average Price', 'SIndex', 'Holiday_']

    # Get all volume media columns
    media_volume_reporting_candidates = [
        col for col in df.columns
        if any(keyword in col.lower() for keyword in media_keywords_check_reporting)
        and 'organic' not in col.lower()
        and col != TARGET and col != WEEK_COL
        and '_Spend' not in col
        and not any(nmk.lower() in col.lower() for nmk in non_media_keywords_reporting)
    ]

    # Add engineered volume features
    engineered_volume_reporting = [
        col for col in df.columns
        if any(keyword in col.lower() for keyword in engineered_volume_keywords)
        and '_Spend' not in col
    ]

    all_volume_media_cols_report = list(set(media_volume_reporting_candidates + engineered_volume_reporting))
    all_volume_media_cols_report = [col for col in all_volume_media_cols_report if col in df.columns]

    # Get all estimated spend media columns
    all_spend_media_cols_report = [col for col in df.columns if '_Spend' in col]
    aggregation_cols_report = all_volume_media_cols_report + all_spend_media_cols_report

    # Perform aggregation if we have columns to aggregate
    if aggregation_cols_report:
        # Ensure all aggregation columns are numeric
        for col in aggregation_cols_report:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

        # Filter to ensure only numeric columns are included
        numeric_aggregation_cols = [
            col for col in aggregation_cols_report
            if col in df.columns and pd.api.types.is_numeric_dtype(df[col])
        ]

        if numeric_aggregation_cols:
            # Group by Fiscal_Year and calculate sum
            yearly_aggregation = df.groupby('Fiscal_Year')[numeric_aggregation_cols].sum()
            print("\n✅ Data aggregated by Fiscal Year.")

            # Calculate year-over-year changes
            print("\n===== CALCULATING YEAR-OVER-YEAR (YoY) CHANGES =====")
            yoy_changes = pd.DataFrame(index=yearly_aggregation.columns)

            # Define years present in the aggregated data
            agg_years = yearly_aggregation.index.tolist()

            # Calculate YoY changes for available years
            if 2022 in agg_years and 2023 in agg_years:
                val_2022 = yearly_aggregation.loc[2022]
                val_2023 = yearly_aggregation.loc[2023]

                # Vectorized YoY calculation
                with np.errstate(divide='ignore', invalid='ignore'):
                    yoy_22_23 = np.where(
                        val_2022 != 0,
                        ((val_2023 - val_2022) / val_2022) * 100,
                        np.where(val_2023 != 0, np.inf, 0)
                    )

                yoy_changes['FY22_to_FY23_YoY_Pct'] = yoy_22_23

            if 2023 in agg_years and 2024 in agg_years:
                val_2023 = yearly_aggregation.loc[2023]
                val_2024 = yearly_aggregation.loc[2024]

                # Vectorized YoY calculation
                with np.errstate(divide='ignore', invalid='ignore'):
                    yoy_23_24 = np.where(
                        val_2023 != 0,
                        ((val_2024 - val_2023) / val_2023) * 100,
                        np.where(val_2024 != 0, np.inf, 0)
                    )

                yoy_changes['FY23_to_FYTD24_YoY_Pct'] = yoy_23_24

            print("\n✅ Year-over-Year changes calculated.")

            # Generate tabular report
            print("\n===== MEDIA PERFORMANCE REPORT BY FISCAL YEAR =====")

            # Create a combined report DataFrame
            yearly_agg_transposed = yearly_aggregation.T
            yearly_agg_transposed.index.name = 'Metric'

            # Rename columns for clarity
            year_cols_map = {year: f'FY {year}' for year in yearly_aggregation.index}
            if 2024 in year_cols_map:
                year_cols_map[2024] = 'FYTD 2024'
            yearly_agg_transposed = yearly_agg_transposed.rename(columns=year_cols_map)

            # Reset index to turn 'Metric' into a column
            yearly_agg_transposed = yearly_agg_transposed.reset_index()

            # Split 'Metric' into 'Channel' and 'Type'
            yearly_agg_transposed['Channel'] = yearly_agg_transposed['Metric'].str.replace('_Spend', '')
            yearly_agg_transposed['Type'] = yearly_agg_transposed['Metric'].apply(
                lambda x: 'Estimated Spend' if '_Spend' in x else 'Execution'
            )

            # Drop the original 'Metric' column
            yearly_agg_transposed = yearly_agg_transposed.drop(columns=['Metric'])

            # Set 'Channel' and 'Type' as index
            yearly_agg_transposed = yearly_agg_transposed.set_index(['Channel', 'Type'])

            # Merge with yoy_changes
            yoy_changes.index.name = 'Metric'
            yoy_changes = yoy_changes.reset_index()

            # Split 'Metric' into 'Channel' and 'Type'
            yoy_changes['Channel'] = yoy_changes['Metric'].str.replace(
                r'_Spend_.*|_.*_YoY_Pct', '', regex=True
            )
            yoy_changes['Type'] = yoy_changes['Metric'].apply(
                lambda x: 'Estimated Spend' if '_Spend' in x else 'Execution'
            )

            # Drop the original 'Metric' column
            yoy_changes = yoy_changes.drop(columns=['Metric'])

            # Set 'Channel' and 'Type' as index
            yoy_changes = yoy_changes.set_index(['Channel', 'Type'])

            # Merge the two dataframes
            combined_report = yearly_agg_transposed.merge(
                yoy_changes, left_index=True, right_index=True, how='left'
            )

            # Define the desired column order
            report_columns_order = []
            available_fy_cols = [col for col in ['FY 2022', 'FY 2023', 'FYTD 2024'] if col in combined_report.columns]
            report_columns_order.extend(available_fy_cols)

            available_yoy_cols = [
                col for col in ['FY22_to_FY23_YoY_Pct', 'FY23_to_FYTD24_YoY_Pct']
                if col in combined_report.columns
            ]

            # Insert YoY columns after their corresponding FY columns
            if 'FY 2023' in report_columns_order and 'FY22_to_FY23_YoY_Pct' in available_yoy_cols:
                report_columns_order.insert(report_columns_order.index('FY 2023') + 1, 'FY22_to_FY23_YoY_Pct')
            if 'FYTD 2024' in report_columns_order and 'FY23_to_FYTD24_YoY_Pct' in available_yoy_cols:
                report_columns_order.insert(report_columns_order.index('FYTD 2024') + 1, 'FY23_to_FYTD24_YoY_Pct')

            # Reindex the columns to the desired order
            combined_report = combined_report.reindex(columns=report_columns_order)

            # Sort by Channel and then Type
            combined_report = combined_report.sort_index(level=['Channel', 'Type'], ascending=[True, False])

            # Format the numeric values and handle Inf/NaN/0 for YoY
            final_report_display = combined_report.copy()

            # Format currency/volume columns
            for col in final_report_display.columns:
                if 'YoY_Pct' in col:
                    # Determine the base year column name
                    base_col_name = 'FY 2022' if 'FY22_to_FY23_YoY_Pct' in col else 'FY 2023'

                    if base_col_name in combined_report.columns:
                        # Apply formatting
                        formatted_col_data = []
                        for index_tuple in final_report_display.index:
                            base_val = combined_report.loc[index_tuple, base_col_name]
                            yoy_val = combined_report.loc[index_tuple, col]
                            formatted_col_data.append(format_yoy(yoy_val, base_val))

                        final_report_display[col] = formatted_col_data
                else:
                    # Format as integer with commas
                    final_report_display[col] = final_report_display[col].apply(
                        lambda x: f"{x:,.0f}" if pd.notna(x) else 'N/A'
                    )

            # Rename YoY percentage columns
            final_report_display = final_report_display.rename(columns={
                'FY22_to_FY23_YoY_Pct': 'YoY % (FY22 vs FY23)',
                'FY23_to_FYTD24_YoY_Pct': 'YoY % (FY23 vs FYTD24)'
            })

            # Display the final formatted report table
            print("\n📋 Media Performance Report by Fiscal Year:")
            display(HTML(final_report_display.to_html(index=True, escape=False)))
        else:
            print("⚠️ None of the identified aggregation columns are numeric.")
    else:
        print("⚠️ No Paid Media Volume or Estimated Spend columns found for aggregation.")

    # --- FINALIZED VARIABLES BY BUCKET (for Modeling) ---
    print("\n\n===== FINALIZED VARIABLES BY BUCKET (for Modeling) =====")
    print("-" * 50)

    # Get all current columns
    all_cols_final = df.columns.tolist()

    # Define buckets
    target_var_modeling = TARGET if TARGET in all_cols_final else None

    # Base/External Variables
    base_vars_modeling = ['Average Price', 'Gasoline Price', 'SIndex', 'Total SKU']
    holiday_vars = [col for col in all_cols_final if col.startswith('Holiday_')] + custom_dummy_cols
    base_vars_modeling.extend(holiday_vars)
    base_vars_modeling = list(set([col for col in base_vars_modeling if col in df.columns]))
    base_vars_modeling.sort()

    # Promotional Variables
    promo_vars_modeling = [col for col in all_cols_final if 'discount' in col.lower()]
    promo_vars_modeling = list(set([col for col in promo_vars_modeling if col in df.columns]))
    promo_vars_modeling.sort()

    # Media Variables (Volume/Execution only)
    media_volume_keywords = ['impressions', 'impr', 'clicks', 'social', 'search', 'video', 'tv', 'display', 'email', 'paid social']
    media_vars_modeling = [
        col for col in all_cols_final
        if (any(keyword in col.lower() for keyword in media_volume_keywords) or
            any(keyword in col.lower() for keyword in engineered_volume_keywords))
        and '_Spend' not in col
        and col not in base_vars_modeling
        and col not in promo_vars_modeling
        and col != TARGET and col != WEEK_COL and col != 'Fiscal_Year'
    ]
    media_vars_modeling = [col for col in media_vars_modeling if col in df.columns]
    media_vars_modeling.sort()

    # Print variable buckets
    print("TARGET VARIABLE (for Modeling):")
    print(f"• {target_var_modeling}" if target_var_modeling else "• None")

    print("\nBase/External Variables:")
    for var in base_vars_modeling:
        print(f"• {var}")

    print("\nMedia Variables (Volume/Execution Only):")
    for var in media_vars_modeling:
        print(f"• {var}")

    print("\nPromotional Variables:")
    for var in promo_vars_modeling:
        print(f"• {var}")

    print("\nSeasonal Index:")
    print("• None (Included in Base Variables)")

    print("\nHoliday Dummies:")
    print("• None (Included in Base Variables)")

    # Show additional features created
    new_features = [
        col for col in df.columns
        if col not in initial_columns
        and col != TARGET
        and col != WEEK_COL
    ]
    new_features = list(set(new_features + ['SIndex', 'Fiscal_Year'] + custom_dummy_cols))
    new_features = [col for col in new_features if col in df.columns]

    if new_features:
        print("\n📊 ADDITIONAL FEATURES CREATED (Volume, Spend, etc.):")
        print("=" * 50)

        # Create a summary table
        feature_info = []
        for col in new_features:
            feature_type = "Unknown"

            if '_Spend' in col:
                feature_type = "Estimated Spend"
            elif col == "SIndex":
                feature_type = "Seasonal Index"
            elif col.startswith("Holiday_") and col not in custom_dummy_cols:
                feature_type = "Holiday Dummy (Pre-defined)"
            elif col in custom_dummy_cols:
                feature_type = "Custom Dummy"
            elif col.endswith(('_pre', '_post')) and '_Spend' not in col:
                feature_type = "Split Variable (Volume)"
            elif ('super' in col.lower() or 'combined_var' in col.lower() or '_volume' in col.lower()) and '_Spend' not in col:
                feature_type = "Super Campaign (Volume)"
            elif col == 'Fiscal_Year':
                feature_type = 'Fiscal Year (Reporting/Grouping)'

            feature_info.append({
                "Feature Name": col,
                "Type": feature_type,
                "Data Type": str(df[col].dtype),
                "Non-Zero Values": f"{(df[col] != 0).sum()} / {len(df)}"
            })

        display(pd.DataFrame(feature_info))

        # Show sample of new features
        print("\n📋 SAMPLE OF NEW FEATURES:")
        display(df[new_features].head())
    else:
        print("\nℹ️ No additional features were created.")

    # --- FINAL MODELING VARIABLES ORGANIZATION ---
    print("\n" + "="*60)
    print("FINAL MODELING VARIABLES ORGANIZATION")
    print("="*60)

    # Create final variable buckets for modeling
    base_vars = ['Average Price', 'Gasoline Price', 'SIndex', 'Total SKU']
    # Add all dummy variables (holiday and custom)
    dummy_vars = [col for col in df.columns if col.startswith('Holiday_') or col.startswith('dummy')]
    base_vars.extend(dummy_vars)
    base_vars = [var for var in base_vars if var in df.columns]
    base_vars.sort()

    # Promo variables: Discount variables
    promo_vars = [col for col in df.columns if 'discount' in col.lower()]
    promo_vars = [var for var in promo_vars if var in df.columns]
    promo_vars.sort()

    # Media variables: Media volume variables only (excluding spend)
    media_keywords = ['impressions', 'impr', 'clicks', 'social', 'search', 'video', 'tv', 'display', 'email', 'paid social']
    engineered_keywords = ['_pre', '_post', 'super', '_volume']

    media_vars = [
        col for col in df.columns
        if (any(keyword in col.lower() for keyword in media_keywords) or
            any(keyword in col.lower() for keyword in engineered_keywords))
        and '_Spend' not in col
        and col not in base_vars
        and col not in promo_vars
        and col != 'Sales' and col != 'Week_Ending' and col != 'Fiscal_Year'
    ]
    media_vars = [var for var in media_vars if var in df.columns]
    media_vars.sort()

    # Target variable
    target_var = 'Sales' if 'Sales' in df.columns else None

    # Print the final organization
    print("\nFINAL VARIABLE BUCKETS FOR MODELING:")
    print("-" * 40)

    print("\nBASE VARIABLES:")
    if base_vars:
        for var in base_vars:
            print(f"• {var}")
    else:
        print("• None")

    print("\nPROMOTIONAL VARIABLES:")
    if promo_vars:
        for var in promo_vars:
            print(f"• {var}")
    else:
        print("• None")

    print("\nMEDIA VARIABLES:")
    if media_vars:
        for var in media_vars:
            print(f"• {var}")
    else:
        print("• None")

    print(f"\nTARGET VARIABLE:")
    print(f"• {target_var}" if target_var else "• None")

    # Create a new DataFrame with only the modeling variables
    modeling_vars = base_vars + promo_vars + media_vars
    if target_var:
        modeling_vars.append(target_var)

    df_model = df[modeling_vars].copy()

    print(f"\n✅ Created modeling DataFrame with {len(modeling_vars)} variables:")
    print(f"   - Base variables: {len(base_vars)}")
    print(f"   - Promotional variables: {len(promo_vars)}")
    print(f"   - Media variables: {len(media_vars)}")
    print(f"   - Target variable: {1 if target_var else 0}")

    # Display the first few rows of the modeling DataFrame
    print("\n📋 Modeling DataFrame (first 5 rows):")
    display(df_model.head())

    # Store the modeling buckets in a dictionary for easy access
    modeling_buckets = {
        'base_vars': base_vars,
        'promo_vars': promo_vars,
        'media_vars': media_vars,
        'target_var': target_var
    }

    # Store the modeling DataFrame in the global namespace
    get_ipython().user_ns['df_model'] = df_model
    get_ipython().user_ns['modeling_buckets'] = modeling_buckets

    print("\n✅ Modeling DataFrame stored as 'df_model'")
    print("✅ Modeling buckets dictionary stored as 'modeling_buckets'")
    print("\nYou can now use these variables for your model training:")
    print("   - df_model: Contains all modeling variables")
    print("   - modeling_buckets: Dictionary with variable lists for each bucket")

    return df


# Main execution block - Fixed to handle cases where data is not available
if __name__ == "__main__":
    try:
        # Try to get the IPython instance
        try:
            from IPython import get_ipython
            ipython = get_ipython()
            if ipython is not None:
                # Try to get data from IPython namespace
                data = ipython.user_ns.get('data', None)
                if data is None:
                    raise NameError("'data' DataFrame is not defined in the IPython namespace.")
            else:
                # Not in IPython environment
                raise NameError("Not running in IPython environment. 'data' DataFrame must be defined.")
        except ImportError:
            # IPython is not available
            raise NameError("IPython is not available. 'data' DataFrame must be defined in the global scope.")

        # Check if data is a valid DataFrame
        if not isinstance(data, pd.DataFrame) or data.empty:
            raise NameError("'data' is not a valid DataFrame or is empty.")

        # Run the feature engineering and reporting module
        df_features = feature_engineering_and_reporting_module(data)

        # Store the result back to IPython namespace if available
        if ipython is not None:
            ipython.user_ns['df_features'] = df_features
            print("\n✅ Final 'df_features' DataFrame stored in IPython namespace.")
        else:
            print("\n✅ Feature engineering completed successfully.")
            print("To use the result, assign it to a variable: df_features = feature_engineering_and_reporting_module(data)")

    except NameError as e:
        print(f"❌ ERROR: {e}")
        print("Please ensure you have loaded your data into a variable named 'data' before running this code.")
    except Exception as e:
        print(f"❌ An unexpected error occurred: {e}")
        raise

# ================================================
# MODEL SELECTION (INTERACTIVE) — NO TRAINING HERE
# ================================================
# What this cell does:
# 1) Lets the user choose a model from a dropdown: Ridge, Lasso, ElasticNet
# 2) Asks "why?" via a multi-select checklist + a free-text box
# 3) On confirm, saves a config dict -> MODEL_CHOICE (with default param grids)
# 4) Prints what will be used later. DOES NOT TRAIN.

from datetime import datetime

# Default hyperparameter grids you can tweak later (NOT used here)
_DEFAULT_GRIDS = {
    "ridge":      {"alpha": [0.01, 0.1, 1.0, 10.0, 100.0]},
    "lasso":      {"alpha": [0.0001, 0.001, 0.01, 0.1, 1.0], "max_iter": [10000]},
    "elasticnet": {"alpha": [0.0001, 0.001, 0.01, 0.1, 1.0], "l1_ratio": [0.2, 0.5, 0.8], "max_iter": [10000]},
}

# Short guidance text used in the UI
_GUIDE = {
    "ridge": (
        "Ridge (L2): good when many predictors are correlated and you want to keep them; "
        "shrinks coefficients but rarely to zero."
    ),
    "lasso": (
        "Lasso (L1): useful for feature selection (drives some coefficients to zero); "
        "can be unstable with strong multicollinearity."
    ),
    "elasticnet": (
        "ElasticNet (L1+L2): balances feature selection and stability; "
        "often a safe default for correlated media channels."
    ),
}

# Will be populated after confirmation
MODEL_CHOICE = None   # dict with keys: model, reasons, notes, grid, timestamp

# Try to use ipywidgets UI; if unavailable, fall back to CLI prompts.
try:
    import ipywidgets as widgets
    from IPython.display import display, clear_output

    # --- Widgets ---
    model_dd = widgets.Dropdown(
        options=[("ElasticNet", "elasticnet"), ("Ridge", "ridge"), ("Lasso", "lasso")],
        value="elasticnet",
        description="Model:",
        disabled=False,
    )

    reasons_ms = widgets.SelectMultiple(
        options=[
            "High multicollinearity",
            "Need feature selection",
            "Balance selection & stability",
            "Small dataset",
            "Interpretability matters",
            "Sparse true drivers expected",
            "Reduce overfitting risk",
        ],
        value=("Balance selection & stability",),
        description="Why?",
        rows=6,
        disabled=False,
        layout=widgets.Layout(width="50%"),
    )

    notes_txt = widgets.Textarea(
        placeholder="Write your hypothesis/business reasoning (e.g., 'channels are correlated; want stability + some selection').",
        description="Notes:",
        layout=widgets.Layout(width="90%", height="80px"),
    )

    explain_btn = widgets.Button(description="Explain choice", icon="info")
    confirm_btn = widgets.Button(description="Confirm selection", button_style="success", icon="check")
    out = widgets.Output()

    def _explain_choice(_btn=None):
        with out:
            clear_output(wait=True)
            m = model_dd.value
            print(f"Model selected: {m.title()}")
            print("-" * 60)
            print(_GUIDE[m])
            print("\nDefault grid (editable later):", _DEFAULT_GRIDS[m])

    def _confirm_choice(_btn=None):
        global MODEL_CHOICE
        m = model_dd.value
        MODEL_CHOICE = {
            "model": m,
            "reasons": list(reasons_ms.value),
            "notes": (notes_txt.value or "").strip(),
            "grid": _DEFAULT_GRIDS[m].copy(),
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "ready_to_train": False,   # enforce no training yet
        }
        with out:
            clear_output(wait=True)
            print("✅ Selection stored in MODEL_CHOICE (no model has been fit).")
            print("Summary:")
            print(f"  • Model: {MODEL_CHOICE['model'].title()}")
            print(f"  • Reasons: {MODEL_CHOICE['reasons'] or '(none)'}")
            print(f"  • Notes: {MODEL_CHOICE['notes'] or '(none)'}")
            print(f"  • Param grid: {MODEL_CHOICE['grid']}")
            print("\nNext step suggestion (when you’re ready):")
            print("  → Review/adjust the grid above, then pass MODEL_CHOICE to your training cell.")

    explain_btn.on_click(_explain_choice)
    confirm_btn.on_click(_confirm_choice)

    header = widgets.HTML("<h3>Model Selection — confirm before any training</h3>")
    ui = widgets.VBox([
        header,
        model_dd,
        reasons_ms,
        notes_txt,
        widgets.HBox([explain_btn, confirm_btn]),
        out
    ])
    display(ui)
    _explain_choice()  # show guidance initially

except Exception as _e:
    # ---- CLI Fallback (no widgets) ----
    print("Widgets not available; using simple prompts (no training will run).")
    print("Choose model: [1] ElasticNet  [2] Ridge  [3] Lasso")
    choice = input("Enter 1/2/3: ").strip()
    mapping = {"1": "elasticnet", "2": "ridge", "3": "lasso"}
    m = mapping.get(choice, "elasticnet")
    print("\nSelect reasons (comma-separated numbers):")
    opts = [
        "High multicollinearity",
        "Need feature selection",
        "Balance selection & stability",
        "Small dataset",
        "Interpretability matters",
        "Sparse true drivers expected",
        "Reduce overfitting risk",
    ]
    for i, o in enumerate(opts, 1):
        print(f"{i}. {o}")
    sel = input("Your choices: ").strip()
    picked = []
    if sel:
        for s in sel.split(","):
            s = s.strip()
            if s.isdigit() and 1 <= int(s) <= len(opts):
                picked.append(opts[int(s) - 1])
    notes = input("Notes / hypothesis (optional): ").strip()

    MODEL_CHOICE = {
        "model": m,
        "reasons": picked,
        "notes": notes,
        "grid": _DEFAULT_GRIDS[m].copy(),
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "ready_to_train": False,
    }

    print("\n✅ Selection stored in MODEL_CHOICE (no model has been fit).")
    print("Summary:")
    print(f"  • Model: {MODEL_CHOICE['model'].title()}")
    print(f"  • Reasons: {MODEL_CHOICE['reasons'] or '(none)'}")
    print(f"  • Notes: {MODEL_CHOICE['notes'] or '(none)'}")
    print(f"  • Param grid: {MODEL_CHOICE['grid']}")
    print("\nWhen ready, hand MODEL_CHOICE to your training cell.")

# ================================================
# ENHANCED MARKETING MIX MODELING WITH FISCAL YEAR ANALYSIS
# ================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error
from scipy.optimize import curve_fit
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from IPython.display import display, HTML
import warnings
warnings.filterwarnings('ignore')

# Define global variables if they are not already defined (e.g., from previous cells)
if 'modeling_buckets' in globals():
    BASE_VARS = modeling_buckets.get('base_vars', [])
    PROMO_VARS = modeling_buckets.get('promo_vars', [])
    MEDIA_VARS = modeling_buckets.get('media_vars', [])
    TARGET = modeling_buckets.get('target_var', 'Sales')
    print("✅ Using modeling buckets from previous feature engineering step.")
else:
    print("⚠️ modeling_buckets not found. Defining default variable lists.")
    # Define default variable lists if modeling_buckets is not available
    BASE_VARS = ['Average Price', 'Gasoline Price', 'SIndex', 'Total SKU']
    PROMO_VARS = ['Discount1', 'Discount2']
    MEDIA_VARS = ['Email Clicks', 'Modular Video Impressions', 'Organic Search Impressions', 'Paid Search Impressions', 'Paid Social Impressions'] # Use original names if transformed not available
    TARGET = 'Sales'

if 'HOLDOUT_WEEKS' not in globals():
    HOLDOUT_WEEKS = 12 # Default holdout weeks
    print(f"✅ Setting holdout period to the last {HOLDOUT_WEEKS} weeks.")

if 'ADSTOCK_DEFAULT_THETA' not in globals():
    ADSTOCK_DEFAULT_THETA = 0.4
    print(f"✅ Setting default adstock theta: {ADSTOCK_DEFAULT_THETA}")

if 'SAT_DEFAULT_BETA' not in globals():
    SAT_DEFAULT_BETA = 1e-6
    print(f"✅ Setting default saturation beta: {SAT_DEFAULT_BETA}")


# Check if we have the necessary data
if 'df_features' not in globals():
    raise ValueError("Please run the feature engineering module first to create df_features")

# Define fiscal year mapping function
def assign_fiscal_year(date_series):
    """
    Assign fiscal year based on date.
    Assuming fiscal year starts in April (Q2) - adjust as needed
    """
    fiscal_years = []
    for date in date_series:
        if date.month >= 4:  # April or later
            fiscal_years.append(f"FY{date.year % 100:02d}/{(date.year + 1) % 100:02d}")
        else:  # January to March
            fiscal_years.append(f"FY{(date.year - 1) % 100:02d}/{date.year % 100:02d}")
    return fiscal_years

# Enhanced adstock transformation with fiscal year awareness
def enhanced_adstock(series, theta=0.4, fiscal_years=None):
    """
    Apply adstock transformation with fiscal year reset
    """
    adstocked = []
    current_adstock = 0
    current_fy = None

    for i, value in enumerate(series):
        # Reset adstock at fiscal year boundaries if fiscal_years provided
        if fiscal_years is not None and i > 0 and fiscal_years[i] != fiscal_years[i-1]:
            current_adstock = 0

        current_adstock = value + theta * current_adstock
        adstocked.append(current_adstock)

    return pd.Series(adstocked, index=series.index)

# Enhanced saturation function
def enhanced_saturation(series, beta=1e-6, method='michaelis-menten'):
    """
    Apply saturation transformation with different curve options
    """
    if method == 'michaelis-menten':
        # Michaelis-Menten saturation
        return series / (beta + series)
    elif method == 'exponential':
        # Exponential saturation
        return 1 - np.exp(-beta * series)
    elif method == 'logistic':
        # Logistic saturation
        return 1 / (1 + np.exp(-beta * (series - series.mean())))
    else:
        raise ValueError("Unsupported saturation method")

# Enhanced transformation application
def apply_enhanced_transformations(df, media_vars, theta_params, beta_params, fiscal_years=None):
    """
    Apply adstock and saturation transformations with fiscal year awareness
    """
    df_transformed = df.copy()

    for var in media_vars:
        if var in df_transformed.columns:
            # Apply adstock with fiscal year reset
            theta = theta_params.get(var, 0.4)
            df_transformed[f"{var}_adstock"] = enhanced_adstock(
                df_transformed[var], theta, fiscal_years
            )

            # Apply saturation
            beta = beta_params.get(var, 1e-6)
            df_transformed[f"{var}_saturated"] = enhanced_saturation(
                df_transformed[f"{var}_adstock"], beta, 'michaelis-menten'
            )

    return df_transformed

# Fiscal year-based ROI calculation
def calculate_roi_by_fiscal_year(model, df_transformed, media_vars, fiscal_year_col='Fiscal_Year'):
    """
    Calculate ROI and MROI metrics aggregated by fiscal year
    """
    roi_results = {}

    # Get coefficients
    coef_dict = dict(zip(model.feature_names_in_, model.coef_))

    for fy in df_transformed[fiscal_year_col].unique():
        fy_mask = df_transformed[fiscal_year_col] == fy
        fy_data = df_transformed[fy_mask]

        fy_roi = {}
        for media_var in media_vars:
            saturated_var = f"{media_var}_saturated"
            if saturated_var in coef_dict:
                # Calculate media contribution for this fiscal year
                media_contribution = (fy_data[saturated_var] * coef_dict[saturated_var]).sum()

                # Get spend (assuming spend column follows naming convention)
                spend_var = f"{media_var}_Spend"
                if spend_var in fy_data.columns:
                    total_spend = fy_data[spend_var].sum()
                    roi = (media_contribution - total_spend) / total_spend if total_spend != 0 else float('nan')
                    fy_roi[media_var] = roi

        roi_results[fy] = fy_roi

    return pd.DataFrame(roi_results).T

# Media effectiveness calculation
def calculate_media_effectiveness(model, df_transformed, media_vars):
    """
    Calculate media effectiveness metrics (incremental volume per unit)
    """
    effectiveness_data = []
    coef_dict = dict(zip(model.feature_names_in_, model.coef_))

    for media_var in media_vars:
        saturated_var = f"{media_var}_saturated"
        if saturated_var in coef_dict:
            coef = coef_dict[saturated_var]

            # Calculate various effectiveness metrics
            metrics = {'Media_Channel': media_var}

            # Per 1M impressions
            if 'impression' in media_var.lower() and f"{media_var}_Spend" in df_transformed.columns:
                total_impressions = df_transformed[media_var].sum()
                total_contribution = (df_transformed[saturated_var] * coef).sum()
                metrics['Incremental_Volume_Per_1M_Impressions'] = (total_contribution / total_impressions) * 1e6 if total_impressions != 0 else float('nan')

            # Per 1000 clicks
            if 'click' in media_var.lower() and f"{media_var}_Spend" in df_transformed.columns:
                total_clicks = df_transformed[media_var].sum()
                total_contribution = (df_transformed[saturated_var] * coef).sum()
                metrics['Incremental_Volume_Per_1000_Clicks'] = (total_contribution / total_clicks) * 1000 if total_clicks != 0 else float('nan')

            # ROI
            spend_var = f"{media_var}_Spend"
            if spend_var in df_transformed.columns:
                total_spend = df_transformed[spend_var].sum()
                total_contribution = (df_transformed[saturated_var] * coef).sum()
                metrics['ROI'] = (total_contribution - total_spend) / total_spend if total_spend != 0 else float('nan')

            effectiveness_data.append(metrics)

    return pd.DataFrame(effectiveness_data)

# Visualization functions
def plot_avp_with_holdout(y_actual, y_pred, holdout_start, dates=None, title="Actual vs Predicted Sales"):
    """
    Plot Actual vs Predicted with holdout period highlighted
    """
    fig = go.Figure()

    # Add actual sales
    fig.add_trace(go.Scatter(
        x=dates if dates is not None else range(len(y_actual)),
        y=y_actual,
        mode='lines',
        name='Actual Sales',
        line=dict(color='blue')
    ))

    # Add predicted sales
    fig.add_trace(go.Scatter(
        x=dates if dates is not None else range(len(y_pred)),
        y=y_pred,
        mode='lines',
        name='Predicted Sales',
        line=dict(color='red', dash='dash')
    ))

    # Add holdout region
    if holdout_start < len(y_actual):
        # Determine the x-values for the holdout region boundaries
        if dates is not None:
            # Handle different index types (Series, DataFrame, DatetimeIndex)
            if isinstance(dates, (pd.Series, pd.DataFrame)):
                x0 = dates.iloc[holdout_start]
                x1 = dates.iloc[-1]
            elif isinstance(dates, pd.DatetimeIndex):
                 x0 = dates[holdout_start]
                 x1 = dates[-1]
            else: # Fallback to index-based if date type is unexpected
                 x0 = holdout_start
                 x1 = len(y_actual) - 1

        else: # Fallback to index-based if dates is None
            x0 = holdout_start
            x1 = len(y_actual) - 1

        fig.add_vrect(
            x0=x0,
            x1=x1,
            fillcolor="lightgray",
            opacity=0.3,
            layer="below",
            line_width=0,
            annotation_text="Holdout Period"
        )

    fig.update_layout(
        title=title,
        xaxis_title="Time",
        yaxis_title="Sales",
        hovermode="x unified"
    )

    return fig

def plot_contribution_by_bucket(df_transformed, model, bucket_vars, dates=None, title="Contribution by Bucket"):
    """
    Plot contribution over time by variable bucket
    """
    # Calculate contributions
    contributions = {}
    coef_dict = dict(zip(model.feature_names_in_, model.coef_))

    for bucket_name, vars_in_bucket in bucket_vars.items():
        bucket_contribution = pd.Series(0, index=df_transformed.index)
        for var in vars_in_bucket:
            if var in coef_dict:
                # Ensure the variable exists in the DataFrame before multiplying
                if var in df_transformed.columns:
                     bucket_contribution += df_transformed[var] * coef_dict[var]
                else:
                     print(f"⚠️ Warning: Variable '{var}' not found in df_transformed for contribution calculation.")

        contributions[bucket_name] = bucket_contribution

    # Create stacked area chart
    fig = go.Figure()

    # Add baseline contribution as a separate trace
    baseline_contrib = model.intercept_
    fig.add_trace(go.Scatter(
         x=dates if dates is not None else df_transformed.index,
         y=[baseline_contrib] * len(df_transformed), # Constant baseline over time
         mode='lines',
         name='Baseline'
    ))


    for bucket_name, contribution in contributions.items():
        fig.add_trace(go.Scatter(
            x=dates if dates is not None else contribution.index,
            y=contribution,
            mode='lines',
            stackgroup='one', # Stack other contributions on top of each other
            name=bucket_name
        ))

    fig.update_layout(
        title=title,
        xaxis_title="Time",
        yaxis_title="Contribution",
        hovermode="x unified",
        legend_title="Variable Bucket"
    )

    return fig

def plot_fy_comparison(contributions_df, title="Contribution by Fiscal Year"):
    """
    Plot bar chart comparing contributions by fiscal year
    """
    fig = px.bar(
        contributions_df,
        x=contributions_df.index,
        y=contributions_df.columns,
        title=title,
        barmode='group'
    )

    fig.update_layout(
        xaxis_title="Fiscal Year",
        yaxis_title="Contribution",
        legend_title="Variable Bucket"
    )

    return fig

def plot_fy_pie_charts(contributions_df, title_template="Incremental Contribution - {}"):
    """
    Plot pie charts showing contribution breakdown by fiscal year
    """
    years = contributions_df.index
    n_years = len(years)

    # Filter out the baseline contribution if it's in the dataframe for incremental pies
    incremental_contributions_df = contributions_df.drop(columns=['Baseline'], errors='ignore')

    fig = make_subplots(
        rows=1,
        cols=n_years,
        subplot_titles=[title_template.format(year) for year in years],
        specs=[[{"type": "domain"}]] * n_years
    )

    for i, year in enumerate(years, 1):
        fig.add_trace(
            go.Pie(
                labels=incremental_contributions_df.columns,
                values=incremental_contributions_df.loc[year].values,
                name=year
            ),
            row=1, col=i
        )

    fig.update_layout(title_text="Incremental Contribution by Fiscal Year (Excluding Baseline)")
    return fig

# Add this function to your enhanced MMM analysis section
def plot_media_response_curves(model, df_transformed, media_vars, theta_params, beta_params):
    """
    Plot response curves for paid media variables (excluding organic)
    """
    # Filter out organic media variables
    paid_media_vars = [var for var in media_vars if 'organic' not in var.lower()]

    if not paid_media_vars:
        print("No paid media variables found for response curves")
        return None

    # Create subplots
    n_cols = 2
    n_rows = (len(paid_media_vars) + 1) // n_cols
    fig = make_subplots(
        rows=n_rows,
        cols=n_cols,
        subplot_titles=[f"Response Curve: {var}" for var in paid_media_vars]
    )

    # Get coefficients
    coef_dict = dict(zip(model.feature_names_in_, model.coef_))

    for i, media_var in enumerate(paid_media_vars):
        row = (i // n_cols) + 1
        col = (i % n_cols) + 1

        # Get transformation parameters
        theta = theta_params.get(media_var, 0.4)
        beta = beta_params.get(media_var, 1e-6)

        # Create test values (0 to 2x max observed value)
        max_val = df_transformed[media_var].max()
        test_values = np.linspace(0, max_val * 2, 100)

        # Apply transformations
        adstocked = enhanced_adstock(pd.Series(test_values), theta)
        saturated = enhanced_saturation(adstocked, beta, 'michaelis-menten')

        # Calculate response (contribution)
        saturated_var = f"{media_var}_saturated"
        if saturated_var in coef_dict:
            response = saturated * coef_dict[saturated_var]
        else:
            response = np.zeros_like(test_values)

        # Add trace
        fig.add_trace(
            go.Scatter(
                x=test_values,
                y=response,
                mode='lines',
                name=media_var,
                hovertemplate=f"{media_var}: %{{x:.0f}}<br>Response: %{{y:.0f}}<extra></extra>"
            ),
            row=row, col=col
        )

        # Add vertical line at max observed value
        fig.add_vline(
            x=max_val,
            line_dash="dash",
            line_color="red",
            row=row, col=col
        )

    fig.update_layout(
        height=300 * n_rows,
        title_text="Media Response Curves (Red line = max observed value)",
        showlegend=False
    )

    fig.update_xaxes(title_text="Media Investment")
    fig.update_yaxes(title_text="Sales Contribution")

    return fig


# Main enhanced modeling process
def enhanced_mmm_analysis(df_features, model_choice, holdout_weeks=8):
    """
    Enhanced MMM analysis with fiscal year breakdown and advanced visualizations
    """
    # 1. Apply enhanced transformations
    print("Applying enhanced transformations...")
    # Ensure MEDIA_VARS is defined
    if 'MEDIA_VARS' not in globals():
         print("❌ ERROR: MEDIA_VARS is not defined. Please ensure modeling_buckets is available.")
         return None, None # Return None for scaler as well

    # Access ADSTOCK_DEFAULT_THETA and SAT_DEFAULT_BETA from the global scope
    global ADSTOCK_DEFAULT_THETA, SAT_DEFAULT_BETA

    theta_params = {var: ADSTOCK_DEFAULT_THETA for var in MEDIA_VARS}  # Default theta
    beta_params = {var: SAT_DEFAULT_BETA for var in MEDIA_VARS}  # Default beta

    # Ensure 'Fiscal_Year' is in df_features for transformation if fiscal year reset is used
    if 'Fiscal_Year' not in df_features.columns:
         if 'Week_Ending' in df_features.columns:
             df_features['Fiscal_Year'] = assign_fiscal_year(df_features['Week_Ending'])
             print("✅ Created 'Fiscal_Year' column for transformation.")
         else:
             print("⚠️ Cannot create 'Fiscal_Year' column. Fiscal year reset in adstock will not be applied.")
             fy_for_transform = None
    else:
        fy_for_transform = df_features['Fiscal_Year']


    df_transformed = apply_enhanced_transformations(
        df_features,
        MEDIA_VARS,
        theta_params,
        beta_params,
        fy_for_transform
    )

    # 2. Prepare modeling data
    print("Preparing modeling data...")
    # Ensure BASE_VARS, PROMO_VARS, TARGET are defined
    if 'BASE_VARS' not in globals() or 'PROMO_VARS' not in globals() or 'TARGET' not in globals():
         print("❌ ERROR: BASE_VARS, PROMO_VARS, or TARGET is not defined. Please ensure modeling_buckets is available.")
         return None, None # Return None for scaler as well

    transformed_media_vars = [f"{var}_saturated" for var in MEDIA_VARS]
    all_features = BASE_VARS + PROMO_VARS + transformed_media_vars
    all_features = [f for f in all_features if f in df_transformed.columns]

    X = df_transformed[all_features]
    y = df_transformed[TARGET]

    # Add scaling here
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index) # Convert back to DataFrame


    # 3. Time-based train-test split
    print("Splitting data into training and testing sets...")
    # Ensure HOLDOUT_WEEKS is defined
    global HOLDOUT_WEEKS
    if 'HOLDOUT_WEEKS' not in globals():
        HOLDOUT_WEEKS = 8
        print(f"ℹ️ HOLDOUT_WEEKS not found, using default: {HOLDOUT_WEEKS}")

    if len(X_scaled) <= HOLDOUT_WEEKS: # Use X_scaled length
         print(f"❌ ERROR: Not enough data ({len(X_scaled)} weeks) for a {HOLDOUT_WEEKS}-week holdout.")
         return None, None # Return None for scaler as well

    X_train_scaled = X_scaled[:-HOLDOUT_WEEKS]
    y_train = y[:-HOLDOUT_WEEKS]
    X_test_scaled = X_scaled[-HOLDOUT_WEEKS:]
    y_test = y[-HOLDOUT_WEEKS:]

    print(f"✅ Data split: {len(X_train_scaled)} training weeks, {len(X_test_scaled)} testing weeks (last {HOLDOUT_WEEKS} weeks).")


    # 4. Model training with time-series cross-validation
    print("Training model with time-series cross-validation...")
    # Ensure MODEL_CHOICE is defined and has 'model' and 'grid' keys
    if 'MODEL_CHOICE' not in globals() or 'model' not in MODEL_CHOICE or 'grid' not in MODEL_CHOICE:
         print("❌ ERROR: MODEL_CHOICE is not properly defined. Please run the model selection cell.")
         return None, None # Return None for scaler as well

    tscv = TimeSeriesSplit(n_splits=5) # Default n_splits


    if model_choice['model'] == 'ridge':
        model = Ridge()
    elif model_choice['model'] == 'lasso':
        model = Lasso()
    elif model_choice['model'] == 'elasticnet':
        model = ElasticNet()
    else:
         print(f"❌ ERROR: Unsupported model type '{model_choice['model']}'. Please choose Ridge, Lasso, or ElasticNet.")
         return None, None # Return None for scaler as well

    param_grid = model_choice.get('grid', {}) # Use grid from MODEL_CHOICE

    if not param_grid:
        print("⚠️ Warning: No parameter grid found in MODEL_CHOICE. Using default grid for Ridge.")
        param_grid = {'alpha': [0.1, 1.0, 10.0]}
        if model_choice['model'] == 'lasso':
             param_grid = {'alpha': [0.1, 1.0, 10.0]}
        elif model_choice['model'] == 'elasticnet':
             param_grid = {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.2, 0.5, 0.8]}


    try:
        grid_search = GridSearchCV(model, param_grid, cv=tscv, scoring='r2')
        grid_search.fit(X_train_scaled, y_train) # Fit on scaled training data

        best_model = grid_search.best_estimator_
        print(f"✅ Best parameters: {grid_search.best_params_}")
        print(f"✅ Best CV score: {grid_search.best_score_:.4f}")
    except Exception as e:
        print(f"❌ Error during Grid Search or model training: {e}")
        # Fallback: Train a model with default parameters if grid search fails
        try:
            print("Attempting to train a model with default parameters due to Grid Search error.")
            model.fit(X_train_scaled, y_train) # Fit on scaled training data
            best_model = model
            print("✅ Trained model with default parameters.")
        except Exception as train_e:
             print(f"❌ Error training model with default parameters: {train_e}")
             return None, None # Return None for scaler as well


    # Store the trained model and scaled test data in the global namespace for scenario analysis
    get_ipython().user_ns['model'] = best_model
    get_ipython().user_ns['X_test_scaled'] = X_test_scaled # Store X_test_scaled for scenario analysis
    get_ipython().user_ns['scaler'] = scaler # Store the fitted scaler


    # 5. Generate predictions
    y_pred_train = best_model.predict(X_train_scaled) # Predict on scaled training data
    y_pred_test = best_model.predict(X_test_scaled) # Predict on scaled test data
    y_pred_full = np.concatenate([y_pred_train, y_pred_test])

    # 6. Calculate metrics
    train_metrics = {
        'R2': r2_score(y_train, y_pred_train),
        'RMSE': np.sqrt(mean_squared_error(y_train, y_pred_train)),
        'MAPE': mean_absolute_percentage_error(y_train, y_pred_train)
    }

    test_metrics = {
        'R2': r2_score(y_test, y_pred_test),
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_test)),
        'MAPE': mean_absolute_percentage_error(y_test, y_pred_test)
    }

    print("\n📊 Model Performance Metrics:")
    print("Training:", train_metrics)
    # Format training metrics
    print(f"  R2: {train_metrics['R2']:.2%}")
    print(f"  RMSE: {train_metrics['RMSE']:.2f}")
    print(f"  MAPE: {train_metrics['MAPE']:.2%}")

    print("Test:", test_metrics)
    # Format test metrics
    print(f"  R2: {test_metrics['R2']:.2%}")
    print(f"  RMSE: {test_metrics['RMSE']:.2f}")
    print(f"  MAPE: {test_metrics['MAPE']:.2%}")

    print("✅ Model metrics calculated.")


    # 7. Generate enhanced visualizations and analyses
    print("\nGenerating enhanced visualizations and analyses...")

    # A. AVP chart with holdout period
    dates = df_features.index # Use the index of df_features as the dates
    avp_fig = plot_avp_with_holdout(
        y, y_pred_full, len(y_train), dates,
        "Actual vs Predicted Sales with Holdout Period"
    )
    print("✅ AVP chart generated.")

    # B. Contribution by bucket over time
    bucket_vars = {
        'Base': BASE_VARS,
        'Media': [f"{var}_saturated" for var in MEDIA_VARS],
        'Promo': PROMO_VARS
    }
    contribution_fig = plot_contribution_by_bucket(
        df_transformed, best_model, bucket_vars, dates,
        "Contribution by Variable Bucket Over Time"
    )
    print("✅ Contribution timeline chart generated.")


    # C. Fiscal year ROI analysis
    if 'Fiscal_Year' in df_transformed.columns:
        fy_roi_df = calculate_roi_by_fiscal_year(best_model, df_transformed, MEDIA_VARS)
        print("✅ Fiscal year ROI analysis calculated.")
    else:
        fy_roi_df = pd.DataFrame()
        print("⚠️ 'Fiscal_Year' column not available. Skipping fiscal year ROI analysis.")


    # D. Media effectiveness analysis
    effectiveness_df = calculate_media_effectiveness(best_model, df_transformed, MEDIA_VARS)
    print("✅ Media effectiveness analysis calculated.")


    # E. Fiscal year contribution comparison
    if 'Fiscal_Year' in df_transformed.columns:
        fy_contributions = {}
        coef_dict = dict(zip(best_model.feature_names_in_, best_model.coef_))

        # Include Baseline contribution
        baseline_contrib = best_model.intercept_

        for fy in df_transformed['Fiscal_Year'].unique():
            fy_mask = df_transformed['Fiscal_Year'] == fy
            fy_data = df_transformed[fy_mask]

            fy_contribution = {'Baseline': baseline_contrib} # Add baseline to each year's contribution

            for bucket_name, vars_in_bucket in bucket_vars.items():
                bucket_total = 0
                for var in vars_in_bucket:
                    if var in coef_dict and var in fy_data.columns:
                        # Use the transformed data (df_transformed) for contribution calculation
                        bucket_total += (fy_data[var] * coef_dict[var]).sum()
                    elif var in coef_dict and var not in fy_data.columns:
                        print(f"⚠️ Warning: Variable '{var}' not found in FY {fy} data for contribution calculation.")
                fy_contribution[bucket_name] = bucket_total

            fy_contributions[fy] = fy_contribution

        fy_contribution_df = pd.DataFrame(fy_contributions).T
        print("✅ Fiscal year contribution comparison calculated.")
    else:
        fy_contribution_df = pd.DataFrame()
        print("⚠️ 'Fiscal_Year' column not available. Skipping fiscal year contribution comparison.")


    # F. Generate comparison visualizations
    if not fy_contribution_df.empty:
        fy_bar_fig = plot_fy_comparison(fy_contribution_df, "Total Contribution by Fiscal Year")
        print("✅ Fiscal year contribution bar chart generated.")

        # Filter for FY 2022 and FY 2023 for pie charts
        pie_years = [fy for fy in ['FY22/23', 'FY23/24'] if fy in fy_contribution_df.index]
        if pie_years:
            fy_pie_fig = plot_fy_pie_charts(fy_contribution_df.loc[pie_years])
            print(f"✅ Fiscal year incremental contribution pie charts generated for: {pie_years}")
        else:
             fy_pie_fig = None
             print("⚠️ No data for FY22/23 or FY23/24 found for pie charts.")
    else:
        fy_bar_fig = None
        fy_pie_fig = None
        print("⚠️ Fiscal year contribution data not available. Skipping fiscal year contribution plots.")


    # Add this function to your enhanced MMM analysis section
    response_curve_fig = plot_media_response_curves(
        best_model,
        df_transformed,
        MEDIA_VARS,
        theta_params,
        beta_params
    )

    #Add to your results dictionary
    if response_curve_fig:
        results['visualizations']['response_curves'] = response_curve_fig
        print("✅ Media response curves generated.")


    # 8. Generate optimization recommendations (Placeholder - can be expanded)
    print("\n=== MEDIA OPTIMIZATION RECOMMENDATIONS ===")

    # Analyze media effectiveness for recommendations
    if not effectiveness_df.empty:
        # Find most effective media channels (e.g., by ROI if available)
        if 'ROI' in effectiveness_df.columns and effectiveness_df['ROI'].notna().any():
            top_roi = effectiveness_df.dropna(subset=['ROI']).nlargest(3, 'ROI')
            print("\nTop Media Channels by ROI:")
            if not top_roi.empty:
                for _, row in top_roi.iterrows():
                    print(f"- {row['Media_Channel']}: {row['ROI']:.2f} ROI")
            else:
                print("  No media channels with calculated ROI.")
        else:
            print("\n⚠️ ROI data not available for effectiveness recommendations.")


        # Simple heuristic for diminishing returns (based on average saturated volume)
        # This is a very basic check and can be refined.
        print("\nDiminishing Returns Check (Based on Average Saturated Volume > 80%):")
        dim_returns_check = {}
        for media_var in MEDIA_VARS:
            saturated_var = f"{media_var}_saturated"
            if saturated_var in df_transformed.columns:
                avg_saturated = df_transformed[saturated_var].mean()
                dim_returns_check[media_var] = avg_saturated > 0.8
                print(f"- {media_var}: {'Potential Diminishing Returns' if avg_saturated > 0.8 else 'Below 80% Saturation'} (Avg Saturated: {avg_saturated:.2f})")
            else:
                 print(f"- {media_var}: Saturated variable '{saturated_var}' not found.")
        if avg_saturated > 0.8:
            pass # Added pass to complete the if statement


    else:
        print("\n⚠️ Media effectiveness data not available for recommendations.")


    return {
        'model': best_model,
        'transformed_data': df_transformed,
        'metrics': {'train': train_metrics, 'test': test_metrics},
        'visualizations': {
            'avp_chart': avp_fig,
            'contribution_timeline': contribution_fig,
            'fy_contribution_bars': fy_bar_fig,
            'fy_contribution_pies': fy_pie_fig,
             'response_curves': response_curve_fig # Add response curves to results dictionary
        },
        'analyses': {
            'fy_roi': fy_roi_df,
            'media_effectiveness': effectiveness_df,
            'fy_contributions': fy_contribution_df
        }
    }, scaler # Return the scaler


# Run the enhanced analysis
if 'df_features' in globals() and 'MODEL_CHOICE' in globals():
    results, scaler = enhanced_mmm_analysis(df_features.copy(), MODEL_CHOICE, HOLDOUT_WEEKS) # Use a copy to avoid modifying original df_features

    # Display results if analysis was successful
    if results is not None:
        print("\n=== ENHANCED MMM ANALYSIS RESULTS ===")

        # Show metrics
        print("\nModel Performance Metrics:")
        print("Training:", results['metrics']['train'])
        # Format training metrics
        print(f"  R2: {results['metrics']['train']['R2']:.2%}")
        print(f"  RMSE: {results['metrics']['train']['RMSE']:.2f}")
        print(f"  MAPE: {results['metrics']['train']['MAPE']:.2%}")

        print("Test:", results['metrics']['test'])
        # Format test metrics
        print(f"  R2: {results['metrics']['test']['R2']:.2%}")
        print(f"  RMSE: {results['metrics']['test']['RMSE']:.2f}")
        print(f"  MAPE: {results['metrics']['test']['MAPE']:.2%}")

        print("✅ Model metrics calculated.")


        # Show fiscal year ROI
        if not results['analyses']['fy_roi'].empty:
            print("\nROI by Fiscal Year:")
            display(results['analyses']['fy_roi'])
        else:
            print("\nNo Fiscal Year ROI data to display.")


        # Show media effectiveness
        if not results['analyses']['media_effectiveness'].empty:
             print("\nMedia Effectiveness Metrics:")
             display(results['analyses']['media_effectiveness'])
        else:
             print("\nNo Media Effectiveness data to display.")


        # Show fiscal year contributions
        if not results['analyses']['fy_contributions'].empty:
            print("\nContributions by Fiscal Year:")
            display(results['analyses']['fy_contributions'])
        else:
            print("\nNo Fiscal Year Contribution data to display.")


        # Display visualizations
        print("\nGenerating visualizations...")
        if results['visualizations']['avp_chart']:
            results['visualizations']['avp_chart'].show()
        if results['visualizations']['contribution_timeline']:
            results['visualizations']['contribution_timeline'].show()
        if results['visualizations']['fy_contribution_bars']:
            results['visualizations']['fy_contribution_bars'].show()
        if results['visualizations']['fy_contribution_pies']:
            results['visualizations']['fy_contribution_pies'].show()
        if 'response_curves' in results['visualizations'] and results['visualizations']['response_curves']:
             results['visualizations']['response_curves'].show() # Uncomment this line

        print("\n✅ Enhanced MMM Analysis Complete.")

else:
    if 'df_features' not in globals():
        print("❌ Cannot run Enhanced MMM Analysis: 'df_features' DataFrame is not available.")
    if 'MODEL_CHOICE' not in globals():
        print("❌ Cannot run Enhanced MMM Analysis: 'MODEL_CHOICE' is not available.")

# ================================================
# SCENARIO ANALYSIS MODULE
# ================================================
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from sklearn.preprocessing import StandardScaler

def run_scenario(data, model, scenario_changes, scaler, X_test_scaled):
    """
    Apply scenario changes and re-predict outcomes using the trained model and scaler.

    Parameters:
    data (DataFrame): The input dataframe (should be the test set, X_test).
    model: The trained regression model.
    scenario_changes (dict): A dictionary where keys are column names (variables)
                             and values are the percentage change (e.g., 0.1 for +10%).
    scaler: The StandardScaler fitted on the training data.
    X_test_scaled (DataFrame): The scaled test set used for baseline prediction.

    Returns:
    DataFrame: A DataFrame with the original data, baseline predictions, and scenario predictions.
    """
    # Ensure the index of the input data matches the index of the scaled test data
    # Also ensure we start with a clean copy to avoid modifying original X_test data unintentionally
    scenario_data = data.copy().reindex(index=X_test_scaled.index)


    # Apply changes (scale media, promos etc.)
    print("\nApplying Scenario Changes:")
    if not scenario_changes:
        print("- No scenario changes specified. Running baseline simulation.")

    for var, change in scenario_changes.items():
        # Check if the variable exists in the DataFrame
        if var in scenario_data.columns:
            scenario_data[var] *= (1 + change)   # e.g. +0.2 for +20%
            print(f"- Increased/Decreased '{var}' by {change*100:.0f}%")
        else:
            print(f"⚠️ Warning: Variable '{var}' not found in the data. Skipping.")

    # Ensure the column order matches the training data before scaling
    # Use the columns from the scaled training data (X_test_scaled) to ensure order and consistency
    # Reindex scenario_data to match the columns used during training/scaling and handle potential NaNs introduced by reindexing
    scenario_data_aligned = scenario_data.reindex(columns=X_test_scaled.columns, fill_value=0)


    # Scale the scenario data using the *same* scaler fitted on the training data
    try:
        # Check for NaNs before scaling and fill if any are present (e.g., fillna(0)) - robust approach
        if scenario_data_aligned.isnull().sum().sum() > 0:
            print("⚠️  NaN values detected in scenario data before scaling. Filling with 0.")
            scenario_data_aligned = scenario_data_aligned.fillna(0)

        scenario_data_scaled = scaler.transform(scenario_data_aligned)
        print("✅ Scenario data scaled.")
    except Exception as e:
        print(f"❌ Error during scaling scenario data: {e}")
        return None


    # Predict new outcome
    try:
        # Check for NaNs in scaled data before prediction as a safeguard
        if np.isnan(scenario_data_scaled).sum() > 0:
             print("❌ NaN values detected in scaled scenario data before prediction.")
             # You might want to return None or handle this differently
             return None

        y_pred_scenario = model.predict(scenario_data_scaled)
        # Temporarily remove clipping to see raw predictions
        # y_pred_scenario = np.clip(y_pred_scenario, 0, None) # Commented out for diagnosis
        print("✅ Scenario predictions generated (clipping temporarily removed).")
    except Exception as e:
         print(f"❌ Error during scenario prediction: {e}")
         return None


    # Get baseline prediction on the test set (assuming X_test_scaled is available)
    y_base = model.predict(X_test_scaled)
    # Clip baseline predictions at 0 as sales cannot be negative
    y_base = np.clip(y_base, 0, None)
    print("✅ Baseline predictions generated (clipped at 0).")


    # Create a results DataFrame
    # Ensure the index is aligned with the predictions
    results_df = pd.DataFrame({
        'Baseline_Prediction': y_base,
        'Scenario_Prediction': y_pred_scenario # This will now contain unclipped values
    }, index=X_test_scaled.index) # Use the index from the scaled test data


    # Calculate Lift (percentage change from baseline)
    # Avoid division by zero or NaN baseline predictions
    results_df['Sales_Lift_Pct'] = np.nan # Initialize with NaN
    valid_baseline_mask = (results_df['Baseline_Prediction'] != 0) & (~results_df['Baseline_Prediction'].isna())

    results_df.loc[valid_baseline_mask, 'Sales_Lift_Pct'] = (
        (results_df.loc[valid_baseline_mask, 'Scenario_Prediction'] - results_df.loc[valid_baseline_mask, 'Baseline_Prediction']) /
        results_df.loc[valid_baseline_mask, 'Baseline_Prediction'] * 100
    )

    print("\n📊 Scenario Analysis Results:")
    print("="*40)
    print(f"Total Baseline Sales: ₹{results_df['Baseline_Prediction'].sum():,.0f}")
    print(f"Total Scenario Sales (Unclipped): ₹{results_df['Scenario_Prediction'].sum():,.0f}") # Indicate unclipped
    print(f"Total Sales Lift (Based on Unclipped Scenario): ₹{(results_df['Scenario_Prediction'].sum() - results_df['Baseline_Prediction'].sum()):,.0f}") # Indicate unclipped
    # Note: Sales Lift Percentage based on unclipped may be misleading if predictions are negative
    print(f"Average Weekly Sales Lift Percentage (Based on Unclipped Scenario): {results_df['Sales_Lift_Pct'].mean():.2f}%")


    return results_df

# --- Interactive Scenario Input ---
print("===== INTERACTIVE SCENARIO ANALYSIS =====")
print("Enter variable changes as percentage (e.g., 20 for +20%, -10 for -10%).")
print("Type 'done' to finish adding changes.")
print("Available variables for scenario testing:")

# Assuming feature_cols from the modeling cell is available
if 'feature_cols' in globals():
    for i, col in enumerate(feature_cols):
        print(f"{i+1}. {col}")
else:
    # Use columns from X_test_scaled which should be available
    if 'X_test_scaled' in globals():
        feature_cols = X_test_scaled.columns.tolist()
        for i, col in enumerate(feature_cols):
            print(f"{i+1}. {col}")
    else:
        print("⚠️ Warning: X_test_scaled not found. Cannot list available variables.")
        feature_cols = []


scenario_changes_input = {}
while True:
    try:
        user_input = input("Enter variable number and percentage change (e.g., '4 20' for 20% increase in Email Clicks) or 'done': ").strip()
        if user_input.lower() == 'done':
            break

        parts = user_input.split()
        if len(parts) == 2:
            try:
                var_index = int(parts[0]) - 1
                change_pct = float(parts[1])
                if 0 <= var_index < len(feature_cols):
                    var_name = feature_cols[var_index]
                    scenario_changes_input[var_name] = change_pct / 100.0
                    print(f"Added change: {var_name} by {change_pct:.0f}%")
                else:
                    print("⚠️ Invalid variable number.")
            except ValueError:
                print("⚠️ Invalid input format. Please use 'number percentage' or 'done'.")
        else:
            print("⚠️ Invalid input format. Please use 'number percentage' or 'done'.")
    except EOFError:
        print("\nInput ended. Running scenario with current changes.")
        break
    except Exception as e:
        print(f"An unexpected error occurred during input: {e}")
        break


# Ensure X_test_scaled, model, and scaler are available from previous cells
if 'X_test_scaled' not in globals():
    print("❌ ERROR: 'X_test_scaled' is not defined. Please run model preprocessing first.")
elif 'model' not in globals():
     print("❌ ERROR: 'model' is not defined. Please run model training first.")
# Removed the check for 'scaler' in globals() as it will be returned and stored by the previous cell
else:
    # Run the scenario analysis
    # Pass df_features[-HOLDOUT_WEEKS:] as the data parameter, which is the original (unscaled) test set
    # Make sure df_features is available globally
    if 'df_features' in globals():
        scenario_results = run_scenario(df_features[-HOLDOUT_WEEKS:].copy(), model, scenario_changes_input, scaler, X_test_scaled) # Use a copy
    else:
        print("❌ ERROR: 'df_features' is not defined. Cannot run scenario analysis.")


    # Display detailed results if available
    if scenario_results is not None:
        print("\n📋 SCENARIO RESULTS DATAFRAME (first 5 rows):")
        display(scenario_results.head())

        # Optional: Plot baseline vs scenario predictions
        # Use the index of the results_df (which is the date index) for the x-axis
        plot_xaxis = scenario_results.index
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=plot_xaxis, y=scenario_results['Baseline_Prediction'], mode='lines+markers', name='Baseline Prediction'))
        fig.add_trace(go.Scatter(x=plot_xaxis, y=scenario_results['Scenario_Prediction'], mode='lines+markers', name='Scenario Prediction (Unclipped)')) # Indicate unclipped in plot legend
        fig.update_layout(title="Baseline vs Scenario Predictions (Unclipped Scenario)", # Indicate unclipped in plot title
                          xaxis_title="Date",
                          yaxis_title="Predicted Sales")
        fig.show()

import subprocess
import sys

# Run pip list to get installed packages
# Decode as utf-8 to handle potential encoding issues
process = subprocess.run([sys.executable, '-m', 'pip', 'freeze'], capture_output=True, text=True, check=True)
requirements_content = process.stdout

print("Generated requirements.txt content:")
print("```")
print(requirements_content)
print("```")

# Optional: Save to a file
# with open("requirements.txt", "w") as f:
#     f.write(requirements_content)
# print("\nrequirements.txt file created.")