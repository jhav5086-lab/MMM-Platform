# -*- coding: utf-8 -*-
"""Beta_MMM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14klC7g67_QAaPRaWFD-MODrI2GzTqHA8
"""

import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Add necessary imports
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.optimize import minimize
from statsmodels.tsa.seasonal import seasonal_decompose

# Interactive visualization imports
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from IPython.display import display, HTML
import ipywidgets as widgets
from ipywidgets import interact, Dropdown, Text, Checkbox, IntText, FloatText, VBox, HBox, Button, Output, SelectMultiple

# --- Helper Functions ---

def format_number(x):
    if pd.isna(x): return ""
    # Ensure x is a number before applying abs()
    if isinstance(x, (int, float, np.number)):
        if abs(x) >= 1e9: return f'{x/1e9:.2f}B'
        elif abs(x) >= 1e6: return f'{x/1e6:.2f}M'
        elif abs(x) >= 1e3: return f'{x/1e3:.2f}K'
        else: return f'{x:.2f}'
    return str(x) # Return as string if not a number

def convert_indian_number(value):
    if isinstance(value, str):
        cleaned_value = value.replace(',', '').strip()
        if cleaned_value in ['-', ''] or cleaned_value.isspace(): return np.nan
        try: return float(cleaned_value)
        except ValueError: return np.nan
    if isinstance(value, (int, float, np.number)): return value
    return np.nan

def load_and_preprocess_data(file_path):
    data = pd.read_csv(file_path)
    data['Week_Ending'] = pd.to_datetime(data['Week_Ending'], format='%d-%m-%Y %H:%M')

    object_cols = data.select_dtypes(include=['object']).columns
    for col in object_cols:
        data[col] = data[col].apply(convert_indian_number)
        data[col] = pd.to_numeric(data[col], errors='coerce')

    return data

def apply_adstock(series, alpha):
    adstocked_series = series.copy()
    for i in range(1, len(series)):
        adstocked_series.iloc[i] = series.iloc[i] + alpha * adstocked_series.iloc[i-1]
    return adstocked_series

def apply_saturation(series, gamma):
    if gamma <= 0:
        warnings.warn("Saturation gamma should be positive. Using a small positive value.")
        gamma = 1e-6
    return series / (series + gamma)

def get_transformation_recommendations(numeric_df):
    numeric_df = numeric_df.select_dtypes(include=[np.number])
    if numeric_df.empty: return pd.DataFrame(columns=['Variable', 'Skewness', 'Kurtosis', 'Recommendation', 'Color'])

    skewness = numeric_df.skew()
    kurtosis = numeric_df.kurtosis()

    recommendations = []
    for col in numeric_df.columns:
        skew_val = skewness[col]
        kurt_val = kurtosis[col]

        if abs(skew_val) > 1.5:
            recommendation = "Log transformation (high right skew)" if skew_val > 1.5 else "Square root or Box-Cox transformation (high left skew)"
            color = "red"
        elif abs(skew_val) > 0.75:
            recommendation = "Consider mild transformation (moderate skew)"
            color = "orange"
        else:
            recommendation = "No transformation needed (approximately symmetric)"
            color = "green"

        if abs(kurt_val) > 3:
            recommendation += " | Potential outliers detected (high kurtosis)"
            if color != "red": color = "orange"

        recommendations.append({
            'Variable': col, 'Skewness': skew_val, 'Kurtosis': kurt_val,
            'Recommendation': recommendation, 'Color': color
        })

    return pd.DataFrame(recommendations)

def generate_seasonality_index(data, date_col, period=52):
    if date_col not in data.columns:
        print(f"Date column '{date_col}' not found in data.")
        return data

    temp_df = data.copy().sort_values(date_col)
    if not pd.api.types.is_numeric_dtype(temp_df['Sales']):
        print(f"Warning: 'Sales' column is not numeric. Cannot generate seasonality index.")
        return data

    seasonality_col = f'seasonality_index_{period}'
    temp_df[seasonality_col] = temp_df['Sales'].rolling(window=period, min_periods=1).mean()
    temp_df[seasonality_col] = temp_df.apply(lambda row: row['Sales'] / row[seasonality_col] if row[seasonality_col] != 0 and pd.notna(row['Sales']) else 1, axis=1)
    temp_df[seasonality_col] = temp_df[seasonality_col].fillna(1)

    print(f"Generated seasonality index with period {period}")
    return temp_df

def add_specific_date_dummies(data, date_col, specific_dates):
    if date_col not in data.columns:
        print(f"Date column '{date_col}' not found in data.")
        return data

    for date_str in specific_dates:
        try:
            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
            dummy_col = f'dummy_{date_str.replace("-", "_")}'
            data[dummy_col] = (data[date_col] == date_obj).astype(int)
            print(f"Added dummy variable for {date_str}")
        except ValueError:
            print(f"Invalid date format: {date_str}. Use YYYY-MM-DD format.")

    return data

def split_media_variable(data, date_col, media_var, split_date_str):
    if date_col not in data.columns or media_var not in data.columns:
        print(f"Date column '{date_col}' or media variable '{media_var}' not found in data.")
        return data, None, None, False

    try:
        split_date = datetime.strptime(split_date_str, '%Y-%m-%d')
        pre_col = f"{media_var}_pre_{split_date.strftime('%Y_%m_%d')}"
        post_col = f"{media_var}_post_{split_date.strftime('%Y_%m_%d')}"

        if not pd.api.types.is_numeric_dtype(data[media_var]):
            print(f"Warning: Media variable '{media_var}' is not numeric. Cannot split.")
            return data, None, None, False

        data[pre_col] = data.apply(lambda row: row[media_var] if row[date_col] < split_date else 0, axis=1)
        data[post_col] = data.apply(lambda row: row[media_var] if row[date_col] >= split_date else 0, axis=1)

        print(f"Split {media_var} into {pre_col} and {post_col} at {split_date_str}")
        return data, pre_col, post_col, True

    except ValueError:
        print(f"Invalid date format: {split_date_str}. Use YYYY-MM-DD format.")
        return data, None, None, False

def create_super_campaign_variable(data, component_vars, super_campaign_name):
    missing_vars = [var for var in component_vars if var not in data.columns]
    if missing_vars:
        print(f"Component variables not found in data: {missing_vars}")
        return data, None, False

    non_numeric_components = [var for var in component_vars if not pd.api.types.is_numeric_dtype(data[var])]
    if non_numeric_components:
        print(f"Warning: Non-numeric component variables found: {non_numeric_components}. Cannot create super campaign.")
        return data, None, False

    data[super_campaign_name] = data[component_vars].sum(axis=1)
    print(f"Created super campaign variable '{super_campaign_name}' from {component_vars}")
    return data, super_campaign_name, True

def perform_comprehensive_eda(data, target_var='Sales'):
    print("="*60)
    print("COMPREHENSIVE EXPLORATORY DATA ANALYSIS")
    print("="*60)

    print("\n1. BASIC DATASET INFORMATION")
    print("="*40)
    print(f"Shape: {data.shape}")
    print(f"Columns: {list(data.columns)}")
    if 'Week_Ending' in data.columns:
        print(f"Date Range: {data['Week_Ending'].min()} to {data['Week_Ending'].max()}")
    print(f"Total Missing Values: {data.isnull().sum().sum()}")

    print("\n\n1.1 MISSING VALUES")
    print("="*40)
    missing_values = data.isnull().sum()
    missing_values = missing_values[missing_values > 0].sort_values(ascending=False)
    if missing_values.empty:
        print("No missing values found in the dataset.")
    else:
        missing_values_df = missing_values.to_frame(name='Missing Count')
        missing_values_df['Missing Percentage (%)'] = (missing_values_df['Missing Count'] / len(data)) * 100
        print("Missing values per column:")
        display(missing_values_df)

    numeric_df = data.select_dtypes(include=[np.number])

    print("\n\n2. SUMMARY STATISTICS")
    print("="*40)

    formatted_stats = numeric_df.describe().copy()
    for col in formatted_stats.columns:
        if pd.api.types.is_numeric_dtype(numeric_df[col]):
            max_val = numeric_df[col].max()
            if pd.isna(max_val): continue
            if abs(max_val) >= 1e9: divisor, suffix = 1e9, 'B'
            elif abs(max_val) >= 1e6: divisor, suffix = 1e6, 'M'
            elif abs(max_val) >= 1e3: divisor, suffix = 1e3, 'K'
            else: divisor, suffix = 1, ''
            if divisor > 1: formatted_stats[col] = formatted_stats[col] / divisor

    display(formatted_stats)

    skewness = numeric_df.skew().to_frame('Skewness')
    kurtosis = numeric_df.kurtosis().to_frame('Kurtosis')
    stats_df = pd.concat([skewness, kurtosis], axis=1)
    print("\nSkewness and Kurtosis:")
    display(stats_df)

    transformation_df = get_transformation_recommendations(numeric_df)
    print("\nTransformation Recommendations:")

    html = '<table border="1" class="dataframe">'
    html += '<thead><tr style="text-align: right;">'
    for col in transformation_df.columns:
        if col != 'Color': html += f'<th>{col}</th>'
    html += '</tr></thead><tbody>'

    for _, row in transformation_df.iterrows():
        html += f'<tr style="background-color: {row["Color"]};">'
        for col in transformation_df.columns:
            if col != 'Color':
                value = row[col]
                html += f'<td>{value:.4f}</td>' if isinstance(value, (int, float)) else f'<td>{value}</td>'
        html += '</tr>'

    html += '</tbody></table>'
    display(HTML(html))

    print("\n\n3. UNIVARIATE ANALYSIS")
    print("="*40)

    for col in numeric_df.columns:
        if not pd.api.types.is_numeric_dtype(data[col]): continue

        mean_val = data[col].mean()
        median_val = data[col].median()

        fig = px.histogram(data, x=col, title=f"Distribution of {col}", nbins=30)

        if 'discount' in col.lower():
            mean_text, median_text = f"Mean: {mean_val:.2%}", f"Median: {median_val:.2%}"
        else:
            mean_text, median_text = f"Mean: {format_number(mean_val)}", f"Median: {format_number(median_val)}"

        # Adjust annotation positions to prevent overlap
        fig.add_vline(x=mean_val, line_dash="dash", line_color="red", annotation_text=mean_text, annotation_position="top right")
        fig.add_vline(x=median_val, line_dash="dash", line_color="green", annotation_text=median_text, annotation_position="bottom right")


        if 'discount' in col.lower():
            fig.update_xaxes(tickformat=".2%")
        else:
            max_val = data[col].max()
            if pd.api.types.is_numeric_dtype(data[col]):
                if max_val >= 1e9: fig.update_xaxes(tickformat=".2s", tickprefix="", ticksuffix="B")
                elif max_val >= 1e6: fig.update_xaxes(tickformat=".2s", tickprefix="", ticksuffix="M")
                elif max_val >= 1e3: fig.update_xaxes(tickformat=".2s", tickprefix="", ticksuffix="K")
                else: fig.update_xaxes(tickformat=",d")
            else: fig.update_xaxes(tickformat=",d")

        fig.show()

    print("\n\n4. BIVARIATE ANALYSIS: RELATIONSHIP WITH TARGET VARIABLE")
    print("="*40)

    if target_var in numeric_df.columns:
        numeric_cols_for_scatter = [col for col in numeric_df.columns if col != target_var]

        for col in numeric_cols_for_scatter:
            if not pd.api.types.is_numeric_dtype(data[col]) or not pd.api.types.is_numeric_dtype(data[target_var]): continue

            correlation = data[col].corr(data[target_var])
            fig = px.scatter(data, x=col, y=target_var, title=f"{target_var} vs {col} (r = {correlation:.3f})")

            if 'discount' in col.lower():
                fig.update_xaxes(tickformat=".2%")
            else:
                max_val_x = data[col].max()
                if max_val_x >= 1e9: fig.update_xaxes(tickformat=".2s", tickprefix="", ticksuffix="B")
                elif max_val_x >= 1e6: fig.update_xaxes(tickformat=".2s", tickprefix="", ticksuffix="M")
                elif max_val_x >= 1e3: fig.update_xaxes(tickformat=".2s", tickprefix="", ticksuffix="K")
                else: fig.update_xaxes(tickformat=",d")

            max_val_y = data[target_var].max()
            if max_val_y >= 1e9: fig.update_yaxes(tickformat=".2s", tickprefix="", ticksuffix="B")
            elif max_val_y >= 1e6: fig.update_yaxes(tickformat=".2s", tickprefix="", ticksuffix="M")
            elif max_val_y >= 1e3: fig.update_yaxes(tickformat=".2s", tickprefix="", ticksuffix="K")
            else: fig.update_yaxes(tickformat=",d")

            fig.show()

    print("\n\n5. TIME SERIES ANALYSIS")
    print("="*40)

    if 'Week_Ending' in data.columns and target_var in data.columns:
        if pd.api.types.is_numeric_dtype(data[target_var]):
            fig = px.line(data, x='Week_Ending', y=target_var, title=f"{target_var} Over Time")
            max_val = data[target_var].max()
            if max_val >= 1e9: fig.update_yaxes(tickformat=".2s", tickprefix="", ticksuffix="B")
            elif max_val >= 1e6: fig.update_yaxes(tickformat=".2s", tickprefix="", ticksuffix="M")
            elif max_val >= 1e3: fig.update_yaxes(tickformat=".2s", tickprefix="", ticksuffix="K")
            else: fig.update_yaxes(tickformat=",d")
            fig.show()

            print("Seasonal Decomposition:")
            try:
                temp_df = data.set_index('Week_Ending').sort_index()
                if len(temp_df) >= 2 * 52:
                    decomposition = seasonal_decompose(temp_df[target_var], period=52, model='multiplicative', extrapolate_trend='freq')
                    fig = make_subplots(rows=4, cols=1, subplot_titles=('Observed', 'Trend', 'Seasonal', 'Residual'))
                    fig.add_trace(go.Scatter(x=decomposition.observed.index, y=decomposition.observed, mode='lines'), row=1, col=1)
                    fig.add_trace(go.Scatter(x=decomposition.trend.index, y=decomposition.trend, mode='lines'), row=2, col=1)
                    fig.add_trace(go.Scatter(x=decomposition.seasonal.index, y=decomposition.seasonal, mode='lines'), row=3, col=1)
                    fig.add_trace(go.Scatter(x=decomposition.resid.index, y=decomposition.resid, mode='lines'), row=4, col=1)
                    max_val = data[target_var].max()
                    tick_format = ",d"
                    if max_val >= 1e9: tick_format, suffix = ".2s", "B"
                    elif max_val >= 1e6: tick_format, suffix = ".2s", "M"
                    elif max_val >= 1e3: tick_format, suffix = ".2s", "K"
                    for i in range(1, 5): fig.update_yaxes(tickformat=tick_format, row=i, col=1)
                    fig.update_layout(height=800, title_text="Seasonal Decomposition (Period 52)")
                    fig.show()
                else:
                    print(f"Not enough data points ({len(temp_df)}) to perform seasonal decomposition with period 52. Need at least {2*52}.")
            except Exception as e: print(f"Could not perform seasonal decomposition: {str(e)}")

    print("\n\n6. CORRELATION ANALYSIS")
    print("="*40)

    numeric_df_for_corr = data.select_dtypes(include=[np.number])
    if not numeric_df_for_corr.empty:
        corr = numeric_df_for_corr.corr()
        fig = px.imshow(corr, text_auto=".2f", aspect="auto", title="Correlation Matrix - All Variables")
        fig.show()

        media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video']
        media_cols = [col for col in numeric_df_for_corr.columns if any(keyword in col.lower() for keyword in media_keywords)]

        if media_cols and target_var in numeric_df_for_corr.columns:
            media_corr = numeric_df_for_corr[media_cols + [target_var]].corr()
            target_corr = media_corr[target_var].drop(target_var).sort_values(ascending=False)
            fig = px.bar(x=target_corr.index, y=target_corr.values, title=f"Correlation of Media Variables with {target_var}")
            fig.update_layout(xaxis_title="Media Variables", yaxis_title="Correlation Coefficient")
            # REMOVED: fig.update_yaxes(ticksuffix="%")
            # REMOVED: fig.update_traces(texttemplate='%{y:.1f}%', textposition='outside')
            fig.update_traces(texttemplate='%{y:.3f}', textposition='outside') # Added to show 3 decimal places as in the table
            fig.show()
            print(target_corr.round(3).to_frame("Correlation"))

            print("\n\n6.1 MEDIA EXECUTION SHARE")
            print("="*40)
            media_totals = numeric_df_for_corr[media_cols].sum()
            total_media = media_totals.sum()
            if total_media > 0:
                media_share = (media_totals / total_media) * 100
                media_share = media_share.sort_values(ascending=False)
                fig = px.bar(x=media_share.index, y=media_share.values, title="Media Execution Share by Channel")
                fig.update_layout(xaxis_title="Channel", yaxis_title="Share (%)")
                fig.update_yaxes(ticksuffix="%")
                fig.update_traces(texttemplate='%{y:.1f}%', textposition='outside')
                fig.show()
                print("Media Execution Share Percentage:")
                display(media_share.round(2).to_frame("Share (%)"))

    print("\n\n7. OUTLIER ANALYSIS")
    print("="*40)

    if target_var in numeric_df.columns and pd.api.types.is_numeric_dtype(data[target_var]):
        Q1 = data[target_var].quantile(0.25)
        Q3 = data[target_var].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = data[(data[target_var] < lower_bound) | (data[target_var] > upper_bound)]
        normal_data = data[~((data[target_var] < lower_bound) | (data[target_var] > upper_bound))].copy()
        print(f"Number of potential outliers in {target_var}: {len(outliers)}")

        if len(outliers) > 0:
            print("Outlier values:")
            display(outliers[['Week_Ending', target_var]])
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=normal_data['Week_Ending'], y=normal_data[target_var], mode='lines+markers', name='Normal Values', marker=dict(size=5)))
            fig.add_trace(go.Scatter(x=outliers['Week_Ending'], y=outliers[target_var], mode='markers', name='Outliers', marker=dict(size=10, color='red')))
            fig.add_hline(y=upper_bound, line_dash="dash", line_color="red", annotation_text=f"Upper Bound: {format_number(upper_bound)}")
            fig.add_hline(y=lower_bound, line_dash="dash", line_color="red", annotation_text=f"Lower Bound: {format_number(lower_bound)}")
            max_val = data[target_var].max()
            if max_val >= 1e9: fig.update_yaxes(tickformat=".2s", tickprefix="", ticksuffix="B")
            elif max_val >= 1e6: fig.update_yaxes(tickformat=".2s", tickprefix="", ticksuffix="M")
            elif max_val >= 1e3: fig.update_yaxes(tickformat=".2s", tickprefix="", ticksuffix="K")
            else: fig.update_yaxes(tickformat=",d")
            fig.update_layout(title=f"Outlier Detection in {target_var}", xaxis_title="Date", yaxis_title=target_var)
            fig.show()

    print("\nEDA Complete.")
    print("="*60)
    return data.copy(), numeric_df.copy(), corr.copy()

def perform_feature_engineering(data):
    print("\n" + "="*60)
    print("FEATURE ENGINEERING")
    print("="*60)

    print("\n--- Seasonality Index Creation ---")
    while True:
        seasonality_input = input("Enter comma-separated seasonality periods (e.g., 52, 26) or press Enter to skip: ").strip()
        if not seasonality_input:
            seasonality_periods = []
            print("Skipping seasonality index creation.")
            break
        else:
            try:
                seasonality_periods = [int(p.strip()) for p in seasonality_input.split(',') if p.strip()]
                if all(p > 0 for p in seasonality_periods):
                     break
                else:
                     print("All seasonality periods must be positive integers.")
            except ValueError:
                print("Invalid input. Please enter comma-separated integers.")

    for period in seasonality_periods:
        data = generate_seasonality_index(data, 'Week_Ending', period=period)
        print("-" * 30)

    print("\n--- Dummy Variable Creation ---")
    while True:
        dummy_dates_input = input("Enter comma-separated specific dates for dummy variables (YYYY-MM-DD) or press Enter to skip: ").strip()
        if not dummy_dates_input:
            specific_dates = []
            print("Skipping dummy variable creation.")
            break
        else:
            specific_dates = [d.strip() for d in dummy_dates_input.split(',') if d.strip()]
            # Validate date format
            valid_dates = []
            invalid_dates = []
            for date_str in specific_dates:
                 try:
                     datetime.strptime(date_str, '%Y-%m-%DD')
                     valid_dates.append(date_str)
                 except ValueError:
                     invalid_dates.append(date_str)

            if invalid_dates:
                 print(f"Invalid date format for: {invalid_dates}. Please use YYYY-MM-DD format.")
                 continue
            specific_dates = valid_dates
            break

    data = add_specific_date_dummies(data, 'Week_Ending', specific_dates)
    print("-" * 30)

    print("\n--- Variable Splitting ---")
    while True:
        split_input = input("Enter media variable to split and split date (e.g., 'Paid Search Impressions, 2023-01-01') or press Enter to skip: ").strip()
        if not split_input:
            print("Skipping variable splitting.")
            break
        else:
            parts = [p.strip() for p in split_input.split(',') if p.strip()]
            if len(parts) == 2:
                media_var_to_split = parts[0]
                split_date_str = parts[1]
                if media_var_to_split in data.columns:
                    data, pre_col, post_col, success = split_media_variable(data, 'Week_Ending', media_var_to_split, split_date_str)
                    if success: break # Break if splitting was successful
                else:
                    print(f"Variable '{media_var_to_split}' not found in the data.")
            else:
                print("Invalid input format. Please enter 'media_variable, YYYY-MM-DD'.")
        if not split_input: break # Break if user chose to skip


    print("-" * 30)

    print("\n--- Super Campaign Creation ---")
    while True:
        super_campaign_input = input("Enter comma-separated component variables and super campaign name (e.g., 'Email Clicks, Paid Social Impressions, Super_Media') or press Enter to skip: ").strip()
        if not super_campaign_input:
            print("Skipping super campaign creation.")
            break
        else:
            parts = [p.strip() for p in super_campaign_input.split(',') if p.strip()]
            if len(parts) >= 2:
                component_vars = parts[:-1]
                super_campaign_name = parts[-1]
                data, super_campaign_col, success = create_super_campaign_variable(data, component_vars, super_campaign_name)
                if success: break # Break if creation was successful
            else:
                print("Invalid input format. Please enter comma-separated component variables followed by the super campaign name.")
        if not super_campaign_input: break # Break if user chose to skip

    print("-" * 30)

    print("\nFeature Engineering Complete.")
    print("="*60)
    return data.copy()

def interactive_cost_input(media_channels, fy_list):
    media_costs_dict = {}
    print("\n--- Interactive Media Cost Input ---")
    print("For each paid media channel, specify the cost type (CPM or CPC) and then the values for each fiscal year.")

    # Filter out 'Organic Search Impressions' from interactive input
    paid_media_channels_for_input = [channel for channel in media_channels if 'organic search impressions' not in channel.lower()]

    if not paid_media_channels_for_input:
        print("No paid media channels (excluding Organic Search) found for interactive cost input.")
        return {}

    for channel in paid_media_channels_for_input:
        media_costs_dict[channel] = {}
        print(f"\n--- {channel} ---")
        while True:
            cost_type = input(f"Cost type for {channel} (CPM/CPC): ").strip().upper()
            if cost_type in ['CPM', 'CPC']: break
            else: print("Invalid input. Please enter either 'CPM' or 'CPC'.")

        for fy in fy_list:
            print(f"\nFY: {fy}")
            while True:
                try:
                    cost_value = float(input(f"Cost value for {channel} in {fy}: ").strip())
                    if cost_value >= 0: break
                    else: print("Cost value must be non-negative.")
                except ValueError: print("Invalid input. Please enter a numeric value.")
            media_costs_dict[channel][fy] = {'type': cost_type, 'value': cost_value}

    return media_costs_dict

def calculate_estimated_weekly_spend(data, media_vars_list, media_costs_dict, fy_ranges_dict):
    estimated_weekly_spend = {}
    # Include 'Organic Search Impressions' for spend calculation, assuming its cost is zero (as it's organic)
    # Make sure to handle cases where 'Organic Search Impressions' might not be in the cost dict.
    spend_calculation_vars = [col for col in data.columns if any(keyword in col.lower() for keyword in ['email clicks', 'paid social impressions', 'paid search impressions', 'modular video impressions', 'organic search impressions', '_pre_', '_post_', 'super_campaign'])]

    for channel in spend_calculation_vars:
        weekly_spend = []
        original_channel_for_cost = channel
        is_split = False

        if '_pre_' in channel or '_post_' in channel:
            is_split = True
            found_original_cost_key = False
            for original_key in media_costs_dict.keys():
                # Use the original key to find cost, even if the channel is split
                if original_key.lower() in channel.lower().replace('_pre_', '').replace('_post_', ''):
                     original_channel_for_cost = original_key
                     found_original_cost_key = True
                     break
            if not found_original_cost_key:
                # Attempt to reconstruct original channel name from split name
                parts = channel.split('_')
                if len(parts) > 3 and (parts[-2].lower() in ['pre', 'post']):
                    try:
                        split_index = min([i for i, part in enumerate(parts) if part.lower() in ['pre', 'post']])
                        original_channel_for_cost = '_'.join(parts[:split_index])
                        if original_channel_for_cost not in media_costs_dict and 'organic search impressions' not in original_channel_for_cost.lower(): # Also check if it's organic search
                             # print(f"Warning: Reconstructed original channel name '{original_channel_for_cost}' not found in cost dictionary for '{channel}'. Skipping weekly spend calculation.")
                            estimated_weekly_spend[channel] = [0] * len(data)
                            continue
                    except ValueError:
                        # print(f"Warning: Could not determine original channel for cost for '{channel}'. Skipping weekly spend calculation.")
                        estimated_weekly_spend[channel] = [0] * len(data)
                        continue
                else:
                    # print(f"Warning: Could not determine original channel for cost for '{channel}'. Skipping weekly spend calculation.")
                    estimated_weekly_spend[channel] = [0] * len(data)
                    continue


        is_super_campaign = 'super_campaign' in channel.lower()
        if is_super_campaign:
            estimated_weekly_spend[channel] = [0] * len(data)
            continue

        cost_info = None
        for index, row in data.iterrows():
            week_ending = row['Week_Ending']
            activity = row[channel]
            current_fy = None

            for fy, date_range in fy_ranges_dict.items():
                try:
                    start_date = datetime.strptime(date_range[0], '%Y-%m-%d')
                    end_date = datetime.strptime(date_range[1], '%Y-%m-%d')
                    if start_date <= week_ending <= end_date:
                        current_fy = fy
                        break
                except ValueError:
                    print(f"Warning: Invalid date format in fy_ranges_dict for FY {fy}: {date_range}. Skipping FY match for this week.")
                    continue

            cost_type, cost_value = None, 0

            # Prioritize finding cost info from the original channel key if split, then check the channel key directly
            if current_fy:
                if original_channel_for_cost in media_costs_dict and current_fy in media_costs_dict[original_channel_for_cost]:
                    cost_info = media_costs_dict[original_channel_for_cost][current_fy]
                    cost_type, cost_value = cost_info['type'], cost_info['value']
                elif channel in media_costs_dict and current_fy in media_costs_dict[channel]: # Fallback to channel name itself
                     cost_info = media_costs_dict[channel][current_fy]
                     cost_type, cost_value = cost_info['type'], cost_info['value']
                # Handle Organic Search specifically - assuming zero cost
                elif 'organic search impressions' in channel.lower():
                     cost_type, cost_value = 'CPM', 0 # Organic is free
                else:
                     # If no cost info found, default to zero cost
                     cost_type, cost_value = None, 0


            if cost_type == 'CPM': spend = (activity / 1000) * cost_value if cost_value > 0 else 0
            elif cost_type == 'CPC': spend = activity * cost_value if cost_value > 0 else 0
            else: spend = activity # If no cost type, assume activity IS spend (e.g., if a spend column was passed directly)

            weekly_spend.append(spend)

        estimated_weekly_spend[channel] = weekly_spend

    super_campaign_vars = [col for col in estimated_weekly_spend.keys() if 'super_campaign' in col.lower()]
    if super_campaign_vars:
        print("\n--- Calculating Super Campaign Spend ---")
        component_spend_cols = [col for col in estimated_weekly_spend.keys() if 'super_campaign' not in col.lower()]
        for super_campaign_col in super_campaign_vars:
            print(f"Note: Assuming Super Campaign '{super_campaign_col}' spend is sum of all other estimated spend columns for calculation purposes.")
            super_campaign_spend_values = [0] * len(data)
            for comp_col in component_spend_cols:
                if comp_col in estimated_weekly_spend:
                    super_campaign_spend_values = [sum(x) for x in zip(super_campaign_spend_values, estimated_weekly_spend[comp_col])]
            estimated_weekly_spend[super_campaign_col] = super_campaign_spend_values

    return estimated_weekly_spend

def create_media_spend_report(data, estimated_weekly_spend_dict, fy_ranges_dict):
    media_execution_cols = [col for col in data.columns if any(keyword in col.lower() for keyword in ['impressions', 'clicks', 'social', 'search', 'email', 'video', '_pre_', '_post_', 'super_campaign']) and '_estimated_spend' not in col.lower()]

    estimated_fy_spend = {}
    fy_execution_data = {}

    for channel, weekly_spends in estimated_weekly_spend_dict.items():
        estimated_fy_spend[channel] = {}
        temp_df = data[['Week_Ending']].copy()
        temp_df['Spend'] = weekly_spends
        for fy, date_range in fy_ranges_dict.items():
            try:
                start_date = datetime.strptime(date_range[0], '%Y-%m-%d')
                end_date = datetime.strptime(date_range[1], '%Y-%m-%d')
                fy_spend = temp_df[(temp_df['Week_Ending'] >= start_date) & (temp_df['Week_Ending'] <= end_date)]['Spend'].sum()
                estimated_fy_spend[channel][fy] = fy_spend
            except ValueError:
                print(f"Warning: Invalid date format in fy_ranges_dict for FY {fy}: {date_range}. Skipping FY spend calculation for this FY.")

    for channel in media_execution_cols:
        fy_execution_data[channel] = {}
        temp_df = data[['Week_Ending']].copy()
        if not pd.api.types.is_numeric_dtype(data[channel]): continue
        temp_df['Execution'] = data[channel]
        for fy, date_range in fy_ranges_dict.items():
            try:
                start_date = datetime.strptime(date_range[0], '%Y-%m-%d')
                end_date = datetime.strptime(date_range[1], '%Y-%m-%d')
                fy_execution = temp_df[(temp_df['Week_Ending'] >= start_date) & (temp_df['Week_Ending'] <= end_date)]['Execution'].sum()
                fy_execution_data[channel][fy] = fy_execution
            except ValueError:
                print(f"Warning: Invalid date format in fy_ranges_dict for FY {fy}: {date_range}. Skipping FY execution calculation for this FY.")

    channel_yoy_change = {}
    channel_yoy_execution_change = {} # Added for execution YoY
    fys = list(fy_ranges_dict.keys())

    for channel, fy_spends in estimated_fy_spend.items():
        channel_yoy_change[channel] = {}
        for i in range(1, len(fys)):
            current_fy, previous_fy = fys[i], fys[i-1]
            current_spend, previous_spend = fy_spends.get(current_fy, 0), fy_spends.get(previous_fy, 0)
            period_label = f'{previous_fy} to {current_fy}'
            if previous_spend != 0:
                yoy_change = ((current_spend - previous_spend) / previous_spend) * 100
                channel_yoy_change[channel][period_label] = yoy_change
            else:
                channel_yoy_change[channel][period_label] = float('inf') if current_spend > 0 else 0

    # Calculate YoY change for Execution
    for channel, fy_executions in fy_execution_data.items():
        channel_yoy_execution_change[channel] = {}
        for i in range(1, len(fys)):
            current_fy, previous_fy = fys[i], fys[i-1]
            current_execution, previous_execution = fy_executions.get(current_fy, 0), fy_executions.get(previous_fy, 0)
            period_label = f'{fys[i-1]} to {fys[i]}' # Corrected period label to match format
            if previous_execution != 0:
                yoy_change = ((current_execution - previous_execution) / previous_execution) * 100
                channel_yoy_execution_change[channel][period_label] = yoy_change
            else:
                channel_yoy_execution_change[channel][period_label] = float('inf') if current_execution > 0 else 0


    comprehensive_report_data = []
    all_channels = sorted(list(set(estimated_fy_spend.keys()).union(set(fy_execution_data.keys()))))

    for channel in all_channels:
        row = {'Channel': channel}
        if channel in estimated_fy_spend:
            for fy in fy_ranges_dict.keys():
                row[f'{fy} Spend (Est)'] = estimated_fy_spend[channel].get(fy, 0)
        else:
            for fy in fy_ranges_dict.keys(): row[f'{fy} Spend (Est)'] = 0

        if channel in fy_execution_data:
            for fy in fy_ranges_dict.keys():
                row[f'{fy} Execution'] = fy_execution_data[channel].get(fy, 0)
        else:
            for fy in fy_ranges_dict.keys(): row[f'{fy} Execution'] = 0

        if channel in channel_yoy_change:
            for period, change in channel_yoy_change[channel].items():
                row[f'{period} YoY Spend Change (%)'] = change
                if change == float('inf'): comment = "Significant increase in spend from the previous period (previous spend was zero)."
                elif change > 10: comment = f"Strong growth in spend ({change:.2f}% increase)."
                elif change > 0: comment = f"Moderate increase in spend ({change:.2f}% increase)."
                elif change < -10: comment = f"Significant decrease in spend ({abs(change):.2f}% decrease)."
                elif change < 0: comment = f"Slight decrease in spend ({abs(change):.2f}% decrease)."
                else: comment = "Spend remained relatively stable."
                row[f'{period} Spend Comment'] = comment
        else:
            for i in range(1, len(fys)):
                period = f'{fys[i-1]} to {fys[i]}'
                row[f'{period} YoY Spend Change (%)'] = np.nan
                row[f'{period} Spend Comment'] = "YoY change data not available."

        # Add Execution YoY Change and Comment
        if channel in channel_yoy_execution_change:
             for period, change in channel_yoy_execution_change[channel].items():
                 row[f'{period} YoY Execution Change (%)'] = change
                 if change == float('inf'): comment = "Significant increase in execution from the previous period (previous execution was zero)."
                 elif change > 10: comment = f"Strong growth in execution ({change:.2f}% increase)."
                 elif change > 0: comment = f"Moderate increase in execution ({change:.2f}% increase)."
                 elif change < -10: comment = f"Significant decrease in execution ({abs(change):.2f}% decrease)."
                 elif change < 0: comment = f"Slight decrease in execution ({abs(change):.2f}% decrease)."
                 else: comment = "Execution remained relatively stable."
                 row[f'{period} Execution Comment'] = comment
        else:
            for i in range(1, len(fys)):
                 period = f'{fys[i-1]} to {fys[i]}'
                 row[f'{period} YoY Execution Change (%)'] = np.nan
                 row[f'{period} Execution Comment'] = "YoY change data not available."


        comprehensive_report_data.append(row)

    comprehensive_media_report_df = pd.DataFrame(comprehensive_report_data)
    ordered_columns = ['Channel']
    for fy in fy_ranges_dict.keys():
        ordered_columns.append(f'{fy} Spend (Est)')
        ordered_columns.append(f'{fy} Execution')
    for i in range(1, len(fys)):
        period = f'{fys[i-1]} to {fys[i]}'
        ordered_columns.append(f'{period} YoY Spend Change (%)')
        ordered_columns.append(f'{period} Spend Comment')
        ordered_columns.append(f'{period} YoY Execution Change (%)') # Added execution YoY cols
        ordered_columns.append(f'{period} Execution Comment') # Added execution YoY cols


    comprehensive_media_report_df = comprehensive_media_report_df.reindex(columns=ordered_columns)

    # Apply format_number only to the raw spend and execution columns
    for col in comprehensive_media_report_df.columns:
        if 'Spend (Est)' in col or 'Execution' in col:
             comprehensive_media_report_df[col] = comprehensive_media_report_df[col].apply(format_number)

    # Format the YoY change columns as percentages after calculation, handle infinite values
    for i in range(1, len(fys)):
        period = f'{fys[i-1]} to {fys[i]}'
        yoy_spend_col = f'{period} YoY Spend Change (%)'
        yoy_exec_col = f'{period} YoY Execution Change (%)'

        # Handle infinite values as strings, format others as percentages
        comprehensive_media_report_df[yoy_spend_col] = comprehensive_media_report_df[yoy_spend_col].apply(
            lambda x: "Infinite" if x == float('inf') else (f"{x:.2f}%" if pd.notna(x) and isinstance(x, (int, float)) else str(x))
        )
        comprehensive_media_report_df[yoy_exec_col] = comprehensive_media_report_df[yoy_exec_col].apply(
            lambda x: "Infinite" if x == float('inf') else (f"{x:.2f}%" if pd.notna(x) and isinstance(x, (int, float)) else str(x))
        )


    return comprehensive_media_report_df, channel_yoy_change

def color_code_yoy_change(val):
    if isinstance(val, str):
        if 'Infinite' in val: return 'color: green'
        try:
            # Extract the numeric value from the string
            change = float(val.replace('%', '').replace(',', ''))
            if change > 10: return 'color: green' # Strong growth
            elif change > 0: return 'color: olive' #'color: lightgreen' # Moderate increase (using olive for better visibility)
            elif change < -10: return 'color: darkred' #'color: red' # Significant decrease (using darkred for better visibility)
            elif change < 0: return 'color: orange' #'color: salmon' # Slight decrease (using orange for better visibility)
            else: return '' # Stable
        except (ValueError, AttributeError): return ''
    return ''

def bucket_variables(df):
    base_vars, macro_external_vars, distribution_product_vars, price_vars = [], [], [], []
    seasonality_trend_vars, promo_vars, media_vars_transformed, other_vars = [], [], [], []

    promo_keywords = ['discount', 'promo']
    price_keywords = ['price']
    distribution_keywords = ['sku']

    # Identify transformed media variables: those with both '_adstocked_' and '_saturated_' in their name
    media_vars_transformed = [col for col in df.columns if '_adstocked_' in col.lower() and '_saturated_' in col.lower()]

    # Identify other variables by exclusion
    excluded_cols = ['Sales', 'Week_Ending'] + media_vars_transformed

    for col in df.columns:
        col_lower = col.lower()
        if col in excluded_cols: continue

        if any(keyword in col_lower for keyword in promo_keywords): promo_vars.append(col)
        elif any(keyword in col.lower() for keyword in price_keywords):
            if 'average price' in col_lower: price_vars.append(col)
            elif 'gasoline price' in col_lower: macro_external_vars.append(col)
        elif any(keyword in col_lower for keyword in distribution_keywords): distribution_product_vars.append(col)
        elif 'seasonality_index' in col_lower or 'dummy_' in col_lower: seasonality_trend_vars.append(col)
        else: other_vars.append(col) # Catch anything else not in excluded_cols and not categorized


    dependent_var = 'Sales' # Defined as a string here
    # Combine all relevant independent variables
    independent_vars = base_vars + macro_external_vars + distribution_product_vars + price_vars + seasonality_trend_vars + promo_vars + media_vars_transformed + other_vars

    print("Dependent Variable:", dependent_var)
    print("Independent Variables:")
    print("  Baseline & Base Drivers (General):", base_vars) # This will likely be empty with the current logic, but kept for structure
    print("  Macroeconomic / External:", macro_external_vars)
    print("  Distribution / Product:", distribution_product_vars)
    print("  Price:", price_vars)
    print("  Seasonality & Trend:", seasonality_trend_vars)
    print("  Promotions:", promo_vars)
    print("  Media (Adstocked & Saturated):", media_vars_transformed)
    if other_vars: print("  Other Variables (not explicitly categorized):", other_vars)

    # Return the refined lists
    return dependent_var, macro_external_vars + distribution_product_vars + price_vars + seasonality_trend_vars, media_vars_transformed, promo_vars, other_vars, independent_vars


def prepare_data_for_modeling_and_bucket(data):
    print("\n" + "="*60)
    print("DATA PREPARATION FOR MODELING (BUCKETING)")
    print("="*60)

    target_var = 'Sales'
    # Fix: Unpack all 6 values returned by bucket_variables
    dependent_var, final_base_vars, final_media_vars_transformed, final_promo_vars, final_other_vars, all_independent_vars = bucket_variables(data)

    print("\n--- Final Variable Bucketing for Modeling ---")
    print("Dependent Variable:", dependent_var) # This was a list before, now a string
    print("Independent Variables:")
    # Filtering out 'Week_Ending' and target_var should happen *before* returning from bucket_variables or here
    # Let's ensure they are not in the independent lists
    final_base_vars = [var for var in final_base_vars if var != 'Week_Ending' and var != target_var]
    final_media_vars_transformed = [var for var in final_media_vars_transformed if var != 'Week_Ending' and var != target_var]
    final_promo_vars = [var for var in final_promo_vars if var != 'Week_Ending' and var != target_var]
    final_other_vars = [var for var in final_other_vars if var != 'Week_Ending' and var != target_var]


    print("  Base & Non-Marketing:", final_base_vars)
    print("  Media (Adstocked & Saturated):", final_media_vars_transformed)
    print("  Promotions:", final_promo_vars)
    if final_other_vars: print("  Other Variables (not explicitly categorized):", final_other_vars)

    print("-" * 30)
    print("--- Variables for Modeling (Tabular) ---")
    variable_list_data = []
    # dependent_var is now a string, no need for [0]
    if dependent_var in data.columns: variable_list_data.append({'Category': 'Dependent Variable', 'Variable Name': dependent_var})

    for var in final_base_vars: variable_list_data.append({'Category': 'Independent Variables (Base & Non-Marketing)', 'Variable Name': var})
    for var in final_media_vars_transformed: variable_list_data.append({'Category': 'Independent Variables (Media - Adstocked & Saturated)', 'Variable Name': var})
    for var in final_promo_vars: variable_list_data.append({'Category': 'Independent Variables (Promo)', 'Variable Name': var})
    for var in final_other_vars: variable_list_data.append({'Category': 'Independent Variables (Other)', 'Variable Name': var})

    variable_list_df = pd.DataFrame(variable_list_data)
    display(variable_list_df)
    print("-" * 30)

    print("\nData Preparation for Modeling Complete.")
    print("="*60)

    # Return only the selected features for modeling
    final_independent_vars = final_base_vars + final_media_vars_transformed + final_promo_vars + final_other_vars
    return data.copy(), final_base_vars, final_media_vars_transformed, final_promo_vars, final_other_vars, final_independent_vars


# --- Main Execution ---
file_path = "/content/Book1.csv"
data = load_and_preprocess_data(file_path)
target_var = 'Sales'

print("--- Data Loading and Preprocessing Complete ---")
print(f"Data shape: {data.shape}")
print(f"Columns: {list(data.columns)}")
print("-" * 30)

data, numeric_df, correlations = perform_comprehensive_eda(data, target_var=target_var)
data = perform_feature_engineering(data)

paid_media_channels = [col for col in data.columns if any(keyword in col.lower() for keyword in ['impressions', 'clicks', 'social', 'search', 'email', 'video'])
                       and not any(keyword in col.lower() for keyword in ['_pre_', '_post_', 'super_campaign'])]

fy_ranges_dict = {
    'FY22': ['2022-01-01', '2022-12-31'],
    'FY23': ['2023-01-01', '2023-12-31'],
    'FY24': ['2024-01-01', '2024-12-31']
}

media_costs_dict = interactive_cost_input(paid_media_channels, list(fy_ranges_dict.keys()))
estimated_weekly_spend_dict = calculate_estimated_weekly_spend(data, [], media_costs_dict, fy_ranges_dict)
comprehensive_media_report_df, channel_yoy_change = create_media_spend_report(data, estimated_weekly_spend_dict, fy_ranges_dict)

print("\n--- Media Spend and Execution Report (Estimated) ---")
styled_report = comprehensive_media_report_df.style.applymap(color_code_yoy_change, subset=[col for col in comprehensive_media_report_df.columns if 'YoY Spend Change (%)' in col or 'YoY Execution Change (%)' in col])
display(styled_report)
print("-" * 30)

# Fix: Unpack all 6 values returned by prepare_data_for_modeling_and_bucket
data, final_base_vars, final_media_vars_transformed, final_promo_vars, final_other_vars, final_independent_vars = prepare_data_for_modeling_and_bucket(data)

print("\n--- MMM Pipeline (excluding modeling) Execution Complete ---")

# Find the date range for missing values in 'Paid Search Impressions'
# This check might still be useful even after filling, to understand the original data issue
paid_search_missing_dates = data[data['Paid Search Impressions'].isnull()]['Week_Ending']

if not paid_search_missing_dates.empty:
    start_date_missing = paid_search_missing_dates.min()
    end_date_missing = paid_search_missing_dates.max()
    print(f"Original missing values in 'Paid Search Impressions' were present between {start_date_missing.strftime('%Y-%m-%d')} and {end_date_missing.strftime('%Y-%m-%d')}.")
else:
    print("No original missing values found in 'Paid Search Impressions'.")

# Fill missing values in 'Paid Search Impressions' with 0
# This was done earlier but is kept here to ensure it's done before adstock/saturation if the user runs cells out of order
# However, adstock/saturation functions now check for NaNs and skip
# Let's explicitly fill here to be safe before any modeling steps that might not handle NaNs
initial_paid_search_null_count = data['Paid Search Impressions'].isnull().sum()
if initial_paid_search_null_count > 0:
    data['Paid Search Impressions'] = data['Paid Search Impressions'].fillna(0)
    print(f"\nFilled {initial_paid_search_null_count} missing values in 'Paid Search Impressions' with 0.")

# Verify that missing values have been filled
print("\nMissing values after filling (verify Paid Search Impressions):")
print(data['Paid Search Impressions'].isnull().sum())

# --- Adstock Transformation (Applied to Paid Media - Adstock Only) ---

print("\n" + "="*60)
print("ADSTOCK TRANSFORMATION (PAID MEDIA - ADSTOCK ONLY)")
print("="*60)

# Ensure final_media_vars_transformed is available from previous steps
if 'final_media_vars_transformed' not in globals():
    print("Error: 'final_media_vars_transformed' list not found. Please run the data preparation step first.")
    adstocked_media_vars = [] # Ensure defined
    media_half_lives = {} # Ensure defined
else:
    # Identify paid media variables from final_media_vars_transformed to apply adstock to.
    # Exclude 'Organic Search Impressions' as requested.
    paid_media_keywords = ['email clicks', 'paid social', 'paid search', 'modular video']
    # Filter variables for paid media keywords and explicitly exclude 'Organic Search Impressions'
    # Also exclude variables that have already been adstocked or saturated
    adstock_candidate_vars = [
        col for col in final_media_vars_transformed
        if any(keyword in col.lower() for keyword in paid_media_keywords)
        and 'organic search impressions' not in col.lower()
        and '_adstocked_' not in col.lower() # Exclude already adstocked variables
        and '_saturated_' not in col.lower() # Exclude already saturated variables
        and 'super_campaign' not in col.lower() # Exclude super campaigns, adstock individually
    ]

    # Check for variables that were split but their original names are in the candidate list
    # Ensure we only adstock the base original paid media variables before any splitting happens.
    # This requires checking the original column names in the initial `data` DataFrame.
    # Let's rebuild adstock_candidate_vars based on the original column names if they are in the bucketed 'other' vars
    # and are paid media, and not already transformed.
    # A simpler approach is to identify the base paid media vars from the initial data columns that are also in final_other_vars (or other relevant buckets)
    # that haven't been adstocked/saturated yet.

    # Identify base paid media variables from original columns that are *not* yet transformed media variables
    base_paid_media_vars = [
        col for col in data.columns
        if any(keyword in col.lower() for keyword in paid_media_keywords)
        and 'organic search impressions' not in col.lower()
        and col not in [t_col for t_col in data.columns if '_adstocked_' in t_col.lower() and '_saturated_' in t_col.lower()] # Exclude if already fully transformed
         and 'super_campaign' not in col.lower() # Exclude super campaigns
    ]

    # Filter out variables with missing values before applying adstock
    adstock_vars_no_nan = [var for var in base_paid_media_vars if data[var].isnull().sum() == 0]


    if not adstock_vars_no_nan:
        print("\nNo base paid media variables (excluding Organic Search) without missing values found for adstock transformation.")
        adstocked_media_vars = [] # Ensure this list is defined even if empty
        media_half_lives = {} # Ensure defined
    else:
        print("\nBase paid media variables (excluding Organic Search) without missing values identified for adstock transformation:")
        for var in adstock_vars_no_nan:
            print(f"- {var}")
        print("-" * 30)

        adstocked_media_vars = []
        media_half_lives = {} # Dictionary to store HL values used

        print("\n--- Applying Adstock Transformation ---")
        for media_var in adstock_vars_no_nan: # Iterate over filtered list
            # Prompt user for Half-Life value
            while True:
                hl_input = input(f"Enter Half-Life (HL) value for adstock of '{media_var}' (typical ranges: 0.3, 0.4, 0.6, 0.8, or integers 1-12): ").strip()
                try:
                    hl = float(hl_input)
                    if hl > 0:
                        break
                    else:
                        print("Half-Life value must be positive.")
                except ValueError:
                    print("Invalid input. Please enter a numeric value for Half-Life.")

            # Calculate Retention Rate (RR)
            rr = (0.5)**(1/hl)
            decay_rate = 1 - rr # Calculate decay rate

            # Create new column name using the input string for consistency
            adstocked_col_name = f'{media_var}_adstocked_HL{hl_input.replace(".", "")}'

            # Ensure the media variable exists in data and is numeric before applying adstock
            if media_var in data.columns and pd.api.types.is_numeric_dtype(data[media_var]):
                 data[adstocked_col_name] = apply_adstock(data[media_var], rr)
                 adstocked_media_vars.append(adstocked_col_name)
                 # Store the actual hl value used
                 media_half_lives[adstocked_col_name] = hl
                 # MODIFICATION: Print RR and Decay Rate as percentages and show example decay
                 print(f"Applied Adstock to '{media_var}' with HL={hl}")
                 print(f"  - Retention Rate (RR): {rr:.2%} (Effect retained each period)")
                 print(f"  - Decay Rate: {decay_rate:.2%} (Effect lost each period)")
                 print(f"  - Example Decay (Effect Remaining):")
                 # Show decay for first few periods (e.g., 0 to 3)
                 decay_example_periods = np.arange(0, min(5, int(hl) + 3)) # Show up to HL + 3 periods or first 5
                 decay_values = [rr**p for p in decay_example_periods]
                 decay_strings = [f"{v:.2f}" for v in decay_values]
                 print(f"    Periods: {list(decay_example_periods)}")
                 print(f"    Effect:  [{', '.join(decay_strings)}]")

            elif media_var not in data.columns:
                 print(f"Warning: Media variable '{media_var}' not found in DataFrame. Skipping adstock transformation.")
            else: # Not numeric
                 print(f"Warning: Media variable '{media_var}' is not numeric. Skipping adstock transformation.")
            print("-" * 30) # Add a separator after processing each variable


print("\nAdstock Transformation Complete.")
print("="*60)

# Explicitly print the head of the data after adstock transformation
print("\n--- Data Head After Adstock Transformation ---")
display(data.head())
print("-" * 30)

# Explicitly print the list of adstocked variables created
print("\n--- Adstocked Variables Created ---")
print(adstocked_media_vars)
print("-" * 30)

# --- Generate Adstock Decay Curves from Newly Created Adstocked Variables ---

print("\n" + "="*60)
print("GENERATING ADSTOCK DECAY CURVES FROM NEWLY CREATED ADSTOCKED VARIABLES")
print("="*60)

# Identify columns that were just created as adstocked variables
adstocked_vars_for_plotting = adstocked_media_vars

if not adstocked_vars_for_plotting:
    print("No new adstocked variables were created to plot decay curves.")
else:
    print("\nFound the following newly created adstocked variables for plotting decay curves:")
    for var in adstocked_vars_for_plotting:
        print(f"- {var}")
    print("-" * 30)

    for adstocked_var_name in adstocked_vars_for_plotting:
        # Retrieve the stored Half-Life (HL) value from the dictionary
        hl_value = media_half_lives.get(adstocked_var_name)

        if hl_value is None or hl_value <= 0:
            print(f"Warning: Could not retrieve a valid Half-Life for variable '{adstocked_var_name}' from media_half_lives dictionary. Cannot plot decay curve.")
            continue # Skip plotting for this variable

        # Calculate the Retention Rate (RR) from the extracted HL
        rr_for_curve = (0.5)**(1/hl_value)

        # Generate points for the decay curve
        # Plot decay over several periods (e.g., up to 10 periods or until decay is very low)
        periods = np.arange(0, 11) # Plot for 0 to 10 periods (weeks)
        decay_values = [rr_for_curve**p for p in periods] # Effect of an impulse at time 0

        fig = go.Figure(data=go.Scatter(x=periods, y=decay_values, mode='lines+markers', name='Adstock Decay'))

        # Add reference points (optional)
        # Find period where effect is halved (should be close to HL)
        # Effect = RR^period = 0.5 => period * log(RR) = log(0.5) / log(RR)
        if rr_for_curve > 0 and rr_for_curve < 1:
             estimated_hl_from_rr = np.log(0.5) / np.log(rr_for_curve)
             if estimated_hl_from_rr <= periods.max():
                  fig.add_trace(go.Scatter(x=[estimated_hl_from_rr], y=[0.5], mode='markers', name='50% Effect (HL)', marker=dict(color='green', size=10)))
                  fig.add_vline(x=estimated_hl_from_rr, line_dash="dash", line_color="green", annotation_text=f"HL (~{estimated_hl_from_rr:.2f} periods)", annotation_position="top right")


        # Extract original raw name from adstocked variable name for title
        original_raw_name = adstocked_var_name.split('_adstocked_')[0] if '_adstocked_' in adstocked_var_name else "Unknown"

        # Use the retrieved hl_value directly in the title
        fig.update_layout(
            title=f"Adstock Decay Curve for {original_raw_name} (HL={hl_value}, RR={rr_for_curve:.2%})", # Updated RR format
            xaxis_title="Periods (Weeks) Since Impulse",
            yaxis_title="Decay Factor (Effect Remaining)",
            yaxis_range=[0, 1.1], # Y-axis from 0 to slightly above 1
            hovermode='x unified'
        )

        fig.show()

print("\nAdstock Decay Curve Generation Complete.")
print("="*60)

# --- Apply Saturation (Hill Function) and Plot Saturation Curves ---

print("\n" + "="*60)
print("APPLYING SATURATION & PLOTTING SATURATION CURVES")
print("="*60)

# Use the list of adstocked variables created in the previous step
if 'adstocked_media_vars' not in globals() or not adstocked_media_vars:
    print("No adstocked variables found from the previous Adstock Transformation step.")
    print("Saturation transformation and plotting skipped.")
    saturated_media_vars_created = []
else:
    # Filter to ensure variables exist in data
    adstocked_vars_for_saturation = [var for var in adstocked_media_vars if var in data.columns]

    if not adstocked_vars_for_saturation:
        print("No adstocked variables found in the DataFrame that were created in the previous step.")
        print("Saturation transformation and plotting skipped.")
        saturated_media_vars_created = []
    else:
        print("\nFound the following adstocked variables for saturation and plotting:")
        for var in adstocked_vars_for_saturation:
            print(f"- {var}")
        print("-" * 30)

        # Step 1: Collect all gamma values first
        print("\n--- Collecting Gamma Values ---")
        print("Guidance for gamma values:")
        print("- High gamma (2.0-5.0): C-Shaped curve - Good for direct response channels")
        print("- Medium gamma (1.0-2.0): Moderate S-Shape - Good for social/short-consideration")
        print("- Low gamma (0.1-1.0): S-Shaped curve - Good for brand building channels")
        print("-" * 40)

        gamma_values = {}
        for adstocked_var in adstocked_vars_for_saturation:
            # Extract channel name for better guidance
            channel_name = adstocked_var.split('_adstocked_')[0]

            # Provide channel-specific guidance
            print(f"\nSelecting gamma for {channel_name}:")
            if any(x in channel_name.lower() for x in ['search', 'click', 'direct']):
                print("This appears to be a direct response channel.")
                print("Suggested gamma: 2.0-5.0 (C-Shaped curve)")
                default_gamma = 3.0
            elif any(x in channel_name.lower() for x in ['social', 'email', 'digital']):
                print("This appears to be a consideration channel.")
                print("Suggested gamma: 1.0-2.0 (Moderate S-Shape)")
                default_gamma = 1.5
            elif any(x in channel_name.lower() for x in ['video', 'tv', 'brand', 'display']):
                print("This appears to be a brand building channel.")
                print("Suggested gamma: 0.3-1.0 (S-Shaped curve)")
                default_gamma = 0.7
            else:
                print("Channel type not recognized. Please select gamma based on guidance above.")
                default_gamma = 1.0

            # Prompt user for gamma value
            while True:
                try:
                    gamma_input = input(f"Enter gamma value for saturation of '{adstocked_var}' [or press Enter for default {default_gamma}]: ").strip()

                    if gamma_input == "":
                        gamma = default_gamma
                        print(f"Using default gamma={gamma}")
                    else:
                        gamma = float(gamma_input)

                    if gamma <= 0:
                        print("Gamma value must be positive.")
                        continue

                    # Store the gamma value
                    gamma_values[adstocked_var] = gamma
                    break
                except ValueError:
                    print("Invalid input. Please enter a numeric value for gamma.")

        # Step 2: Apply saturation and generate plots (without user input)
        print("\n--- Applying Saturation and Generating Curves ---")
        saturated_media_vars_created = []
        media_gammas_used = {}

        for adstocked_var, gamma in gamma_values.items():
            # Apply Saturation
            saturated_col_name = f'{adstocked_var}_saturated_gamma{gamma}'

            if pd.api.types.is_numeric_dtype(data[adstocked_var]):
                data[saturated_col_name] = apply_saturation(data[adstocked_var], gamma)
                saturated_media_vars_created.append(saturated_col_name)
                media_gammas_used[saturated_col_name] = gamma
                print(f"Applied Saturation to '{adstocked_var}' with gamma={gamma} -> '{saturated_col_name}'")

                # Plot Saturation Curve
                print(f"Generating Saturation Curve for {saturated_col_name}...")

                # Generate appropriate x-axis range
                if gamma > 0:
                    max_input_range = max(gamma * 10, (0.99 * gamma) / (1 - 0.99))
                else:
                    max_input_range = data[adstocked_var].max() * 1.2

                input_values_range = np.linspace(0, max_input_range, 200)
                saturated_values_range = input_values_range / (input_values_range + gamma + 1e-9)

                # Create and show plot
                fig = go.Figure(data=go.Scatter(x=input_values_range, y=saturated_values_range, mode='lines', name='Saturation Curve'))

                # Add reference points
                # 50% saturation: x = gamma
                x_50 = gamma
                y_50 = x_50 / (x_50 + gamma + 1e-9)
                if x_50 <= max_input_range:
                    fig.add_trace(go.Scatter(x=[x_50], y=[y_50], mode='markers', name='50% Saturation', marker=dict(color='green', size=10)))
                    fig.add_vline(x=x_50, line_dash="dash", line_color="green", annotation_text=f"50% Saturation (~{x_50:.2f} Input)")

                # 80% saturation: x = (0.8 * gamma) / (1 - 0.8) = 4 * gamma
                x_80 = 4 * gamma
                y_80 = x_80 / (x_80 + gamma + 1e-9)
                if x_80 <= max_input_range:
                    fig.add_trace(go.Scatter(x=[x_80], y=[y_80], mode='markers', name='80% Saturation', marker=dict(color='red', size=10)))
                    fig.add_vline(x=x_80, line_dash="dash", line_color="red", annotation_text=f"80% Saturation (~{x_80:.2f} Input)")

                fig.update_layout(
                    title=f"Saturation Curve for {adstocked_var} (Gamma={gamma})",
                    xaxis_title="Adstocked Input Value",
                    yaxis_title="Saturated Output Value",
                    yaxis_range=[0, 1.1]
                )

                fig.show()
                print("-" * 30)
            else:
                print(f"Warning: Adstocked variable '{adstocked_var}' is not numeric. Skipping.")

        # Update final_media_vars list
        if 'final_media_vars' in globals():
            final_media_vars.extend(saturated_media_vars_created)
            print("Updated final_media_vars list with saturated variables.")
        else:
            final_media_vars = saturated_media_vars_created

print("\nSaturation Application and Plotting Complete.")
print("="*60)

# Display results
print("\n--- Data Head After Saturation ---")
display(data.head())
print("\n--- Saturated Variables Created ---")
print(saturated_media_vars_created)

print("\n" + "="*60)
print("FINAL VARIABLE BUCKETING FOR MODELING (RECAP)")
print("="*60)

# Ensure the final variable lists are available from previous steps
# We will also dynamically identify the adstocked and saturated media variables
if 'final_base_vars' not in globals() or 'final_promo_vars' not in globals() or 'final_other_vars' not in globals() or 'target_var' not in globals() or 'data' not in globals():
    print("Error: Required variables (final_base_vars, final_promo_vars, final_other_vars, target_var, or data DataFrame) not found. Please ensure previous steps were run.")
else:
    print("--- Variables for Modeling (Tabular) ---")
    variable_list_data = []

    # Add Dependent Variable
    dependent_var = target_var
    variable_list_data.append({'Category': 'Dependent Variable', 'Variable Name': dependent_var})

    # Add Independent Variables by Bucket
    for var in final_base_vars:
        variable_list_data.append({'Category': 'Independent Variables (Base & Non-Marketing)', 'Variable Name': var})

    # Identify adstocked and saturated media variables from the DataFrame columns
    # Look for columns containing both '_adstocked_' and '_saturated_'
    final_media_vars_transformed = [col for col in data.columns if '_adstocked_' in col.lower() and '_saturated_' in col.lower()]

    # Media variables with adstock & saturation transformations
    if final_media_vars_transformed:
        for var in final_media_vars_transformed:
            variable_list_data.append({'Category': 'Independent Variables (Media - Adstocked & Saturated)', 'Variable Name': var})
    else:
        print("Warning: No adstocked and saturated media variables found in the DataFrame for the modeling recap.")


    for var in final_promo_vars:
        variable_list_data.append({'Category': 'Independent Variables (Promo)', 'Variable Name': var})

    for var in final_other_vars:
        variable_list_data.append({'Category': 'Independent Variables (Other)', 'Variable Name': var})

    # Create and display DataFrame
    variable_list_df = pd.DataFrame(variable_list_data)
    display(variable_list_df)

    print("\nModeling Variables Summary:")
    print(f"Base Variables: {len(final_base_vars)}")
    print(f"Transformed Media Variables: {len(final_media_vars_transformed)}")
    print(f"Promo Variables: {len(final_promo_vars)}")
    print(f"Other Variables: {len(final_other_vars)}")
    print(f"Total Independent Variables: {len(final_base_vars) + len(final_media_vars_transformed) + len(final_promo_vars) + len(final_other_vars)}")
    print("-" * 60)

print("\nFinal Variable Bucketing Recap Complete.")
print("="*60)

import pandas as pd

print("\n" + "="*60)
print("SUMMARY OF ADSTOCK AND SATURATION PARAMETERS")
print("="*60)

# Ensure the dictionaries from previous steps are available
if 'media_half_lives' not in globals() or 'media_gammas_used' not in globals():
    print("Error: Adstock and Saturation parameter dictionaries not found. Please ensure previous steps were run.")
else:
    summary_data = []

    # Iterate through the media variables that were adstocked and saturated
    # The keys in media_gammas_used are the adstocked variables that were saturated
    transformed_vars = list(media_gammas_used.keys())

    if not transformed_vars:
        print("No adstocked and saturated media variables found to summarize parameters.")
    else:
        for saturated_var_name in transformed_vars:
            # Extract the adstocked variable name from the saturated variable name
            # Example: 'Modular Video Impressions_adstocked_HL08_saturated_gamma0.7' -> 'Modular Video Impressions_adstocked_HL08'
            adstocked_var_name = saturated_var_name.split('_saturated_')[0] if '_saturated_' in saturated_var_name else saturated_var_name # Fallback if naming convention changes

            # Extract original raw channel name
            original_channel_name = adstocked_var_name.split('_adstocked_')[0] if '_adstocked_' in adstocked_var_name else adstocked_var_name


            # Get the HL and RR from media_half_lives using the adstocked variable name
            hl_value = media_half_lives.get(adstocked_var_name)
            rr_value = (0.5)**(1/hl_value) if hl_value is not None and hl_value > 0 else None

            # Get the Gamma from media_gammas_used using the saturated variable name
            gamma_value = media_gammas_used.get(saturated_var_name)

            summary_data.append({
                'Original Channel': original_channel_name,
                'Transformed Variable': saturated_var_name,
                'Half-Life (HL)': hl_value,
                'Retention Rate (RR)': f"{rr_value:.2%}" if rr_value is not None else None,
                'Gamma': gamma_value
            })

        summary_df = pd.DataFrame(summary_data)

        # Format the DataFrame for display
        # Add a column for the adstocked variable name for clarity if needed, but current columns seem sufficient.

        print("\n--- Adstock and Saturation Parameters per Transformed Media Variable ---")
        display(summary_df)

print("\nSummary Generation Complete.")
print("="*60)

"""## Model selection interface

### Subtask:
Create an interactive interface to allow the user to select one of three models: Ridge, Lasso, or Elastic Net.

**Reasoning**:
Create and display the dropdown widget for model selection as requested by the instructions.
"""

# Create a dropdown widget for model selection
model_selector = Dropdown(
    options=['Ridge', 'Lasso', 'ElasticNet'],
    value='Ridge',  # Default value
    description='Select Model:',
    disabled=False,
)

# Display the widget
display(model_selector)

# Model Execution

# Time series split: Use the last 10% of data as the holdout set
holdout_percentage = 0.10
holdout_size = int(len(data) * holdout_percentage)
split_index = len(data) - holdout_size

# Assuming 'Sales' is the target variable and the remaining columns are features
# This is a simplified approach based on user instruction;
# a more robust approach would involve explicit feature selection and handling missing values.

# Identify the columns to be used for modeling based on the bucketing from the previous step
# This requires the lists final_base_vars, final_media_vars_transformed, final_promo_vars, final_other_vars
# to be available in the global scope after running the first cell.
# We will explicitly construct the list of independent variables here,
# ensuring only the fully transformed media variables are included.

independent_vars_for_modeling = []

# Add base variables (macroeconomic, distribution/product, price, seasonality/trend)
if 'final_base_vars' in globals():
    independent_vars_for_modeling.extend(final_base_vars)
    print(f"Including {len(final_base_vars)} base variables.")
else:
    print("Warning: final_base_vars not found. Base variables not included.")

# Add ONLY the transformed media variables (adstocked and saturated)
# Identify these directly from the DataFrame columns as they are created dynamically
transformed_media_cols = [col for col in data.columns if '_adstocked_' in col.lower() and '_saturated_' in col.lower()]
if transformed_media_cols:
    independent_vars_for_modeling.extend(transformed_media_cols)
    print(f"Including {len(transformed_media_cols)} transformed media variables.")
else:
    print("Warning: No adstocked and saturated media variables found in data columns.")


# Add promo variables
if 'final_promo_vars' in globals():
    independent_vars_for_modeling.extend(final_promo_vars)
    print(f"Including {len(final_promo_vars)} promo variables.")
else:
    print("Warning: final_promo_vars not found. Promo variables not included.")

# Add other variables (excluding original media variables)
# Need to make sure 'other_variables' in the bucket_variables function
# correctly excludes the original media variables as well, which it should if
# the transformed media are explicitly excluded first.
if 'final_other_vars' in globals():
    # Double-check that original media variables are not in final_other_vars
    media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video']
    filtered_other_vars = [
        col for col in final_other_vars
        if not any(keyword in col.lower() for keyword in media_keywords)
    ]
    independent_vars_for_modeling.extend(filtered_other_vars)
    print(f"Including {len(filtered_other_vars)} filtered other variables.")
else:
    print("Warning: final_other_vars not found. Other variables not included.")


# Filter the DataFrame to include only the selected independent variables
# Ensure all selected columns exist in the data DataFrame
valid_independent_vars = [col for col in independent_vars_for_modeling if col in data.columns]
missing_independent_vars = [col for col in independent_vars_for_modeling if col not in data.columns]

if missing_independent_vars:
    print(f"Warning: The following independent variables were not found in the DataFrame and will be excluded: {missing_independent_vars}")

X = data[valid_independent_vars]
y = data['Sales']


X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

# Keep track of dates for train and test sets for plotting if needed later
dates_train = data.iloc[:split_index]['Week_Ending']
dates_test = data.iloc[split_index:]['Week_Ending']


print("--- Data Splitting Complete ---")
print(f"Original data shape: {data.shape}")
print(f"Training set shape (X_train, y_train): {X_train.shape}, {y_train.shape}")
print(f"Holdout set shape (X_test, y_test): {X_test.shape}, {X_test.shape}") # Corrected shape for X_test
if not y_test.empty:
    print(f"Holdout start date: {dates_test.iloc[0].strftime('%Y-%m-%d')}")
print("-" * 30)

# Check for missing values in training and testing sets
print("\n--- Checking for Missing Values in Training and Testing Sets ---")
print("Missing values in X_train:")
print(X_train.isnull().sum()[X_train.isnull().sum() > 0])
print("\nMissing values in X_test:")
print(X_test.isnull().sum()[X_test.isnull().sum() > 0])
print("-" * 30)

print("\n--- Final Independent Variables Used for Modeling ---")
print(X_train.columns)
print("-" * 30)

# --- Scale the Data ---

print("\n" + "="*60)
print("SCALING INDEPENDENT VARIABLES")
print("="*60)

# Ensure X_train and X_test are available
if 'X_train' not in globals() or 'X_test' not in globals():
    print("Error: X_train or X_test not found. Please run the data splitting step first.")
else:
    # Initialize StandardScaler
    scaler = StandardScaler()

    # Fit the scaler on the training data and transform the training data
    X_train_scaled = scaler.fit_transform(X_train)

    # Transform the testing data using the scaler fitted on the training data
    X_test_scaled = scaler.transform(X_test)

    # Convert the scaled arrays back to DataFrames for easier handling, preserving column names
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)


    print("\nIndependent variables scaled successfully using StandardScaler.")
    print("-" * 30)

    # Display the head of the scaled dataframes to verify
    print("\n--- X_train_scaled Head ---")
    display(X_train_scaled.head())

    print("\n--- X_test_scaled Head ---")
    display(X_test_scaled.head())

    print("\nScaling Complete.")
    print("="*60)

# --- Define Parameter Grid for Hyperparameter Tuning ---

print("\n" + "="*60)
print("DEFINING PARAMETER GRID FOR TUNING")
print("="*60)

# Define the parameter grid for alpha
# We'll test a range of alpha values, common for Ridge, Lasso, and ElasticNet
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000]}

print("\nDefined parameter grid for alpha:")
print(param_grid)
print("-" * 30)

print("\nParameter Grid Definition Complete.")
print("="*60)

# --- Find the Best Alpha using GridSearchCV ---

print("\n" + "="*60)
print("FINDING BEST ALPHA USING GRIDSEARCHCV")
print("="*60)

# Import TimeSeriesSplit
from sklearn.model_selection import TimeSeriesSplit

# Ensure scaled data and parameter grid are available
if 'X_train_scaled' not in globals() or 'y_train' not in globals() or 'param_grid' not in globals():
    print("Error: Scaled training data (X_train_scaled, y_train) or parameter grid (param_grid) not found. Please run previous steps.")
elif 'model_selector' not in globals():
     print("Error: Model selector widget not found. Please run the cell to create the model selector.")
else:
    # Get the selected model from the dropdown
    selected_model_name = model_selector.value
    print(f"Selected Model from dropdown: {selected_model_name}")
    print("-" * 30)

    # Initialize the selected model
    if selected_model_name == 'Ridge':
        model = Ridge()
        # Ridge and Lasso don't use l1_ratio in GridSearchCV param_grid
        current_param_grid = {'alpha': param_grid['alpha']}
        # For ElasticNet, we need to define l1_ratio as well
    elif selected_model_name == 'Lasso':
        model = Lasso()
        current_param_grid = {'alpha': param_grid['alpha']}
    elif selected_model_name == 'ElasticNet':
        model = ElasticNet()
        # For ElasticNet, we also need to tune l1_ratio
        # Let's add a range of l1_ratio values to the grid
        current_param_grid = {
            'alpha': param_grid['alpha'],
            'l1_ratio': [0, 0.25, 0.5, 0.75, 1] # 0 is Ridge, 1 is Lasso
        }
        print("Using parameter grid including 'l1_ratio' for ElasticNet.")
        print(current_param_grid)
    else:
        print(f"Error: Unknown model selected: {selected_model_name}")
        model = None
        current_param_grid = None


    if model:
        # Initialize GridSearchCV
        # Use TimeSeriesSplit for cross-validation since it's time series data
        # Set n_splits appropriately based on data size, but default 5 is usually okay
        # Set scoring to 'r2' or 'neg_mean_squared_error'
        tscv = TimeSeriesSplit(n_splits=min(5, len(X_train_scaled) // 2)) # Use smaller splits if data is small
        grid_search = GridSearchCV(estimator=model, param_grid=current_param_grid,
                                   scoring='r2', cv=tscv, n_jobs=-1)

        print(f"\nPerforming GridSearchCV for {selected_model_name} with {tscv.n_splits} splits...")

        # Fit GridSearchCV to the scaled training data
        grid_search.fit(X_train_scaled, y_train)

        # Get the best parameters and best score
        best_alpha = grid_search.best_params_.get('alpha')
        best_l1_ratio = grid_search.best_params_.get('l1_ratio') # Will be None for Ridge/Lasso
        best_score = grid_search.best_score_

        print("\n--- GridSearchCV Results ---")
        print(f"Best parameters found: {grid_search.best_params_}")
        print(f"Best cross-validation R-squared score: {best_score:.4f}")
        print("-" * 30)

        print("\nGridSearchCV Complete.")
        print("="*60)

"""# Task
Train the final model, generate predictions, calculate residuals, visualize actual vs. predicted and residuals, calculate and display model metrics (R-squared, MAPE, DW) with comments, calculate and display VIF, display coefficients, and summarize all model parameters used.

## Train final model

### Subtask:
Train the selected model (Ridge, Lasso, or ElasticNet) using the best hyperparameters found by `GridSearchCV` on the scaled training data (`X_train_scaled`, `y_train`).

**Reasoning**:
Train the selected model using the best hyperparameters found by GridSearchCV.
"""

print("\n" + "="*60)
print("TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS")
print("="*60)

# Check if required variables are available
if 'grid_search' not in globals() or 'selected_model_name' not in globals() or 'X_train_scaled' not in globals() or 'y_train' not in globals():
    print("Error: Required variables (grid_search, selected_model_name, X_train_scaled, or y_train) not found. Please ensure previous steps were run.")
else:
    # Get the best model estimator from grid_search
    final_model = grid_search.best_estimator_

    # Fit the best model to the scaled training data
    final_model.fit(X_train_scaled, y_train)

    print(f"\nFinal {selected_model_name} model trained successfully using best hyperparameters.")
    print("-" * 30)

print("\nFinal Model Training Complete.")
print("="*60)

"""## Make predictions

### Subtask:
Generate predictions for both the scaled training data (`X_train_scaled`) and the scaled holdout data (`X_test_scaled`).

**Reasoning**:
Generate predictions for both the scaled training and testing data using the trained final model.
"""

print("\n" + "="*60)
print("GENERATING PREDICTIONS")
print("="*60)

# Ensure the final_model and scaled data are available
if 'final_model' not in globals() or 'X_train_scaled' not in globals() or 'X_test_scaled' not in globals():
    print("Error: Final model (final_model) or scaled data (X_train_scaled, X_test_scaled) not found. Please ensure previous steps were run.")
else:
    # Make predictions on the scaled training data
    y_train_pred_scaled = final_model.predict(X_train_scaled)

    # Make predictions on the scaled testing data
    y_test_pred_scaled = final_model.predict(X_test_scaled)

    print("\nPredictions generated for both training and testing sets.")
    print("-" * 30)

print("\nPrediction Generation Complete.")
print("="*60)

"""**Reasoning**:
The predictions have been generated as requested. The next step is to calculate the residuals for both training and testing sets.


"""

print("\n" + "="*60)
print("CALCULATING RESIDUALS")
print("="*60)

# Ensure actual and predicted values are available
if 'y_train' not in globals() or 'y_train_pred_scaled' not in globals() or 'y_test' not in globals() or 'y_test_pred_scaled' not in globals():
    print("Error: Actual or predicted values not found. Please ensure previous steps were run.")
else:
    # Calculate residuals for the training set
    residuals_train = y_train - y_train_pred_scaled

    # Calculate residuals for the testing set
    residuals_test = y_test - y_test_pred_scaled

    print("\nResiduals calculated for both training and testing sets.")
    print("-" * 30)

    # Display the first few training residuals
    print("\n--- Training Residuals Head ---")
    display(residuals_train.head())

    # Display the first few testing residuals
    print("\n--- Testing Residuals Head ---")
    display(residuals_test.head())


print("\nResidual Calculation Complete.")
print("="*60)

"""**Reasoning**:
Visualize the actual vs. predicted values and the residuals for both the training and testing sets using the calculated predictions and residuals.


"""

print("\n" + "="*60)
print("VISUALIZING ACTUAL VS. PREDICTED AND RESIDUALS")
print("="*60)

# Ensure actual values, predicted values, residuals, and dates are available
if ('y_train' not in globals() or 'y_train_pred_scaled' not in globals() or
    'y_test' not in globals() or 'y_test_pred_scaled' not in globals() or
    'residuals_train' not in globals() or 'residuals_test' not in globals() or
    'dates_train' not in globals() or 'dates_test' not in globals()):
    print("Error: Required data (actual values, predicted values, residuals, or dates) not found. Please ensure previous steps were run.")
else:
    # --- Plot Actual vs. Predicted ---
    print("\n--- Plotting Actual vs. Predicted ---")
    fig_actual_pred = make_subplots(rows=2, cols=1,
                                    subplot_titles=('Actual vs. Predicted (Training)', 'Actual vs. Predicted (Holdout)'),
                                    shared_xaxes=False) # Set shared_xaxes to False

    # Training data
    fig_actual_pred.add_trace(go.Scatter(x=dates_train, y=y_train, mode='lines', name='Actual (Train)', line=dict(color='blue')), row=1, col=1)
    fig_actual_pred.add_trace(go.Scatter(x=dates_train, y=y_train_pred_scaled, mode='lines', name='Predicted (Train)', line=dict(color='orange')), row=1, col=1)

    # Holdout data
    fig_actual_pred.add_trace(go.Scatter(x=dates_test, y=y_test, mode='lines', name='Actual (Holdout)', line=dict(color='green')), row=2, col=1)
    fig_actual_pred.add_trace(go.Scatter(x=dates_test, y=y_test_pred_scaled, mode='lines', name='Predicted (Holdout)', line=dict(color='red')), row=2, col=1)

    # Update layout and axis titles
    fig_actual_pred.update_layout(height=800, title_text="Actual vs. Predicted Sales Over Time")
    fig_actual_pred.update_xaxes(title_text="Date", row=1, col=1)
    fig_actual_pred.update_yaxes(title_text="Sales", row=1, col=1)
    fig_actual_pred.update_xaxes(title_text="Date", row=2, col=1)
    fig_actual_pred.update_yaxes(title_text="Sales", row=2, col=1)

    # Format y-axis labels for better readability (e.g., using format_number if available)
    # Assuming 'format_number' is available from the helper functions cell
    max_sales_val = max(y_train.max(), y_test.max())
    tick_format = ",d"
    if max_sales_val >= 1e9: tick_format, suffix = ".2s", "B"
    elif max_sales_val >= 1e6: tick_format, suffix = ".2s", "M"
    elif max_sales_val >= 1e3: tick_format, suffix = ".2s", "K"

    fig_actual_pred.update_yaxes(tickformat=tick_format, row=1, col=1)
    fig_actual_pred.update_yaxes(tickformat=tick_format, row=2, col=1)

    fig_actual_pred.show()

    # --- Plot Residuals Over Time ---
    print("\n--- Plotting Residuals Over Time ---")
    fig_residuals = make_subplots(rows=2, cols=1,
                                  subplot_titles=('Residuals (Training)', 'Residuals (Holdout)'),
                                  shared_xaxes=False) # Set shared_xaxes to False

    # Training residuals
    fig_residuals.add_trace(go.Scatter(x=dates_train, y=residuals_train, mode='lines', name='Residuals (Train)', line=dict(color='purple')), row=1, col=1)
    fig_residuals.add_hline(y=0, line_dash="dash", line_color="gray", row=1, col=1)

    # Holdout residuals
    fig_residuals.add_trace(go.Scatter(x=dates_test, y=residuals_test, mode='lines', name='Residuals (Holdout)', line=dict(color='brown')), row=2, col=1)
    fig_residuals.add_hline(y=0, line_dash="dash", line_color="gray", row=2, col=1)

    # Update layout and axis titles
    fig_residuals.update_layout(height=800, title_text="Model Residuals Over Time")
    fig_residuals.update_xaxes(title_text="Date", row=1, col=1)
    fig_residuals.update_yaxes(title_text="Residuals", row=1, col=1)
    fig_residuals.update_xaxes(title_text="Date", row=2, col=1)
    fig_residuals.update_yaxes(title_text="Residuals", row=2, col=1)

    # Format y-axis labels for better readability
    max_residual_val = max(abs(residuals_train).max(), abs(residuals_test).max())
    tick_format_res = ",d"
    if max_residual_val >= 1e9: tick_format_res, suffix_res = ".2s", "B"
    elif max_residual_val >= 1e6: tick_format_res, suffix_res = ".2s", "M"
    elif max_residual_val >= 1e3: tick_format_res, suffix_res = ".2s", "K"

    fig_residuals.update_yaxes(tickformat=tick_format_res, row=1, col=1)
    fig_residuals.update_yaxes(tickformat=tick_format_res, row=2, col=1)


    fig_residuals.show()

    # --- Plot Residuals Distribution ---
    print("\n--- Plotting Residuals Distribution ---")
    fig_hist_residuals = make_subplots(rows=1, cols=2,
                                       subplot_titles=('Residuals Distribution (Training)', 'Residuals Distribution (Holdout)'))

    fig_hist_residuals.add_trace(go.Histogram(x=residuals_train, name='Residuals (Train)', marker=dict(color='purple')), row=1, col=1)
    fig_hist_residuals.add_trace(go.Histogram(x=residuals_test, name='Residuals (Holdout)', marker=dict(color='brown')), row=1, col=2)

    fig_hist_residuals.update_layout(title_text="Distribution of Model Residuals")
    fig_hist_residuals.update_xaxes(title_text="Residual Value", row=1, col=1)
    fig_hist_residuals.update_yaxes(title_text="Frequency", row=1, col=1)
    fig_hist_residuals.update_xaxes(title_text="Residual Value", row=1, col=2)
    fig_hist_residuals.update_yaxes(title_text="Frequency", row=1, col=2)

    # Format x-axis labels for better readability (residuals)
    fig_hist_residuals.update_xaxes(tickformat=tick_format_res, row=1, col=1)
    fig_hist_residuals.update_xaxes(tickformat=tick_format_res, row=1, col=2)


    fig_hist_residuals.show()

print("\nVisualization Complete.")
print("="*60)

"""**Reasoning**:
Calculate and display model evaluation metrics (R-squared, MAPE, Durbin-Watson) for both the training and testing sets, including comments on the interpretation of these metrics.


"""

print("\n" + "="*60)
print("MODEL EVALUATION METRICS")
print("="*60)

# Ensure actual values, predicted values, and residuals are available
if ('y_train' not in globals() or 'y_train_pred_scaled' not in globals() or
    'y_test' not in globals() or 'y_test_pred_scaled' not in globals() or
    'residuals_train' not in globals() or 'residuals_test' not in globals()):
    print("Error: Required data (actual values, predicted values, or residuals) not found. Please ensure previous steps were run.")
else:
    # --- Calculate R-squared ---
    r2_train = r2_score(y_train, y_train_pred_scaled) * 100 # Convert to percentage
    r2_test = r2_score(y_test, y_test_pred_scaled) * 100 # Convert to percentage
    r2_diff = abs(r2_train - r2_test)

    print("\n--- R-squared Scores ---")
    print(f"Training R-squared: {r2_train:.2f}%")
    print(f"Holdout R-squared:  {r2_test:.2f}%")
    print(f"R-squared Difference: {r2_diff:.2f}%")

    # Comment on R-squared
    print("\nCommentary on R-squared:")
    if r2_test < 0:
        print("- The Holdout R-squared is negative, indicating that the model performs worse than a simple horizontal line (mean of the target) on the holdout data. This is a sign of poor generalization.")
    elif r2_test < 50:
        print("- The Holdout R-squared is low (< 50%), suggesting the model explains only a small portion of the variance in the holdout data. The fit on unseen data is weak.")
    elif r2_test < 75:
        print("- The Holdout R-squared is moderate (50-75%), indicating the model explains a fair amount of variance in the holdout data. The fit is reasonable but could be improved.")
    else:
        print("- The Holdout R-squared is high (> 75%), suggesting the model explains a large portion of the variance in the holdout data. The fit on unseen data is strong.")

    if r2_diff > 15: # Arbitrary threshold for significant difference
        print(f"- There is a significant difference ({r2_diff:.2f}%) between training and holdout R-squared, potentially indicating overfitting to the training data.")
    elif r2_diff > 5:
         print(f"- There is a moderate difference ({r2_diff:.2f}%) between training and holdout R-squared, which might suggest some minor overfitting.")
    else:
        print(f"- The difference ({r2_diff:.2f}%) between training and holdout R-squared is small, suggesting the model generalizes well to the holdout data.")
    print("-" * 30)


    # --- Calculate Mean Absolute Percentage Error (MAPE) ---
    # Avoid division by zero for MAPE
    mape_train = np.mean(np.abs((y_train - y_train_pred_scaled) / y_train)) * 100
    mape_test = np.mean(np.abs((y_test - y_test_pred_scaled) / y_test)) * 100

    print("\n--- Mean Absolute Percentage Error (MAPE) ---")
    print(f"Training MAPE: {mape_train:.2f}%")
    print(f"Holdout MAPE:  {mape_test:.2f}%")

    # Comment on MAPE
    print("\nCommentary on MAPE:")
    # Standard interpretation ranges (approximate and context-dependent)
    if mape_test < 10:
        print("- The Holdout MAPE is low (< 10%), indicating highly accurate predictions on average relative to actual values.")
    elif mape_test < 20:
        print("- The Holdout MAPE is moderate (10-20%), indicating reasonably accurate predictions on average.")
    elif mape_test < 50:
        print("- The Holdout MAPE is high (20-50%), suggesting significant percentage errors in predictions on average.")
    else:
        print("- The Holdout MAPE is very high (> 50%), indicating poor accuracy in predictions on average.")

    if mape_train < mape_test:
        print("- Training MAPE is lower than Holdout MAPE, which is expected but a large difference might suggest overfitting.")
    elif mape_train > mape_test:
         print("- Training MAPE is higher than Holdout MAPE, which is unusual and might indicate issues with the training process or data.")
    else:
        print("- Training and Holdout MAPE are similar, suggesting consistent performance across sets.")

    # Compare MAPE to average sales
    average_sales = y.mean()
    print(f"\nContext: Average Sales value is approximately {format_number(average_sales)}.")
    print(f"A MAPE of {mape_test:.2f}% means the average prediction error is approximately {format_number(average_sales * (mape_test/100))}.")

    print("-" * 30)

    # --- Calculate Durbin-Watson Statistic ---
    # The Durbin-Watson statistic tests for autocorrelation in the residuals.
    # Values typically range from 0 to 4.
    # A value near 2 suggests no autocorrelation.
    # Values below 2 suggest positive autocorrelation (residuals tend to be followed by residuals of the same sign).
    # Values above 2 suggest negative autocorrelation (residuals tend to be followed by residuals of the opposite sign).
    from statsmodels.stats.stattools import durbin_watson

    # Ensure residuals are Series or arrays for durbin_watson
    dw_train = durbin_watson(residuals_train)
    dw_test = durbin_watson(residuals_test)

    print("\n--- Durbin-Watson Statistic ---")
    print(f"Training Durbin-Watson: {dw_train:.4f}")
    print(f"Holdout Durbin-Watson:  {dw_test:.4f}")

    # Comment on Durbin-Watson
    print("\nCommentary on Durbin-Watson:")
    print("- The Durbin-Watson statistic checks for autocorrelation in the residuals.")
    print("- Values near 2 suggest no significant autocorrelation.")
    print("- Values significantly below 2 (e.g., < 1.5) suggest positive autocorrelation.")
    print("- Values significantly above 2 (e.g., > 2.5) suggest negative autocorrelation.")
    print("\nInterpretation:")
    if abs(dw_train - 2) < 0.5: # Thresholds are approximate; formal testing required for definitive conclusion
        print(f"- Training DW ({dw_train:.4f}) is close to 2, suggesting little to no significant autocorrelation in training residuals.")
    elif dw_train < 2:
        print(f"- Training DW ({dw_train:.4f}) is below 2, suggesting potential positive autocorrelation in training residuals. This might indicate missing time-based features or model structure issues.")
    else:
        print(f"- Training DW ({dw_train:.4f}) is above 2, suggesting potential negative autocorrelation in training residuals.")

    if abs(dw_test - 2) < 0.5:
         print(f"- Holdout DW ({dw_test:.4f}) is close to 2, suggesting little to no significant autocorrelation in holdout residuals.")
    elif dw_test < 2:
        print(f"- Holdout DW ({dw_test:.4f}) is below 2, suggesting potential positive autocorrelation in holdout residuals. This is common in time series models if trends or seasonality are not fully captured.")
    else:
        print(f"- Holdout DW ({dw_test:.4f}) is above 2, suggesting potential negative autocorrelation in holdout residuals.")

    print("-" * 30)


print("\nModel Evaluation Metrics Complete.")
print("="*60)

"""**Reasoning**:
Calculate and display Variance Inflation Factor (VIF) for the independent variables in the scaled training data to check for multicollinearity.


"""

print("\n" + "="*60)
print("VARIANCE INFLATION FACTOR (VIF)")
print("="*60)

# Ensure scaled training data is available and it's a DataFrame
if 'X_train_scaled' not in globals() or not isinstance(X_train_scaled, pd.DataFrame):
    print("Error: Scaled training data (X_train_scaled) not found or is not a DataFrame. Please ensure previous steps were run.")
else:
    # Import required libraries
    import statsmodels.api as sm
    from statsmodels.stats.outliers_influence import variance_inflation_factor

    # Calculate VIF for each independent variable
    # Need to add a constant to the independent variables matrix for VIF calculation
    X_train_scaled_const = sm.add_constant(X_train_scaled)

    vif_data = pd.DataFrame()
    vif_data["feature"] = X_train_scaled_const.columns

    # Calculate VIF, handling potential division by zero for constant or perfect multicollinearity
    vif_data["VIF"] = [variance_inflation_factor(X_train_scaled_const.values, i)
                       if np.isfinite(variance_inflation_factor(X_train_scaled_const.values, i)) else np.nan
                       for i in range(X_train_scaled_const.shape[1])]

    # Sort by VIF in descending order, excluding the 'const' row
    vif_data = vif_data[vif_data['feature'] != 'const'].sort_values(by="VIF", ascending=False).reset_index(drop=True)


    print("\n--- Variance Inflation Factor (VIF) per Feature ---")
    display(vif_data)

    # Commentary on VIF
    print("\nCommentary on VIF:")
    print("- VIF measures how much the variance of the estimated regression coefficient is increased due to multicollinearity.")
    print("- Rule of thumb:")
    print("  - VIF = 1: No correlation between the variable and others.")
    print("  - 1 < VIF < 5: Moderately correlated.")
    print("  - VIF >= 5 (or 10 depending on source): Highly correlated (potential multicollinearity issue).")

    print("\nInterpretation:")
    high_vif_vars = vif_data[vif_data['VIF'] >= 5]
    if high_vif_vars.empty:
        print("- No variables show signs of high multicollinearity (VIF >= 5).")
    else:
        print("- The following variables show potential signs of multicollinearity (VIF >= 5):")
        display(high_vif_vars)
        print("  Consider addressing these variables by: dropping one of the correlated variables, combining them, or using regularization methods (which the selected model already is).")

    print("-" * 30)


print("\nVariance Inflation Factor Calculation Complete.")
print("="*60)

"""**Reasoning**:
Display the coefficients of the trained model and summarize all model parameters used, including the selected model type, best hyperparameters, and the adstock and saturation parameters.


"""

print("\n" + "="*60)
print("MODEL COEFFICIENTS AND PARAMETER SUMMARY")
print("="*60)

# Ensure final_model, X_train_scaled, selected_model_name, grid_search, media_half_lives, media_gammas_used are available
if ('final_model' not in globals() or 'X_train_scaled' not in globals() or
    'selected_model_name' not in globals() or 'grid_search' not in globals() or
    'media_half_lives' not in globals() or 'media_gammas_used' not in globals()):
    print("Error: Required data (final_model, X_train_scaled, selected_model_name, grid_search, media_half_lives, or media_gammas_used) not found. Please ensure previous steps were run.")
else:
    # --- Display Coefficients ---
    print("\n--- Model Coefficients ---")

    # Get coefficients and feature names
    coefficients = final_model.coef_
    features = X_train_scaled.columns

    # Create a DataFrame for coefficients
    coefficients_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})

    # Add intercept
    intercept = final_model.intercept_
    intercept_df = pd.DataFrame({'Feature': ['Intercept'], 'Coefficient': [intercept]})

    # Combine intercept and coefficients
    coefficients_df = pd.concat([intercept_df, coefficients_df], ignore_index=True)

    # Sort coefficients by absolute value for easier interpretation
    coefficients_df['Abs_Coefficient'] = coefficients_df['Coefficient'].abs()
    coefficients_df = coefficients_df.sort_values(by='Abs_Coefficient', ascending=False).drop(columns='Abs_Coefficient')

    display(coefficients_df)
    print("-" * 30)

    # --- Summarize Model Parameters ---
    print("\n--- Summary of Model Parameters ---")

    print(f"Selected Model Type: {selected_model_name}")
    print(f"Best Hyperparameters (from GridSearchCV): {grid_search.best_params_}")

    print("\nAdstock and Saturation Parameters Used:")
    if not media_half_lives and not media_gammas_used:
        print("No adstock or saturation parameters were applied.")
    else:
        summary_data = []
        # Use the keys from media_gammas_used as they represent the final transformed media variables
        transformed_vars = list(media_gammas_used.keys())

        if not transformed_vars:
             print("No adstocked and saturated media variables found to summarize parameters.")
        else:
            for saturated_var_name in transformed_vars:
                # Extract the adstocked variable name
                adstocked_var_name = saturated_var_name.split('_saturated_')[0] if '_saturated_' in saturated_var_name else saturated_var_name

                # Extract original raw channel name
                original_channel_name = adstocked_var_name.split('_adstocked_')[0] if '_adstocked_' in adstocked_var_name else adstocked_var_name

                # Get the HL and RR
                hl_value = media_half_lives.get(adstocked_var_name)
                rr_value = (0.5)**(1/hl_value) if hl_value is not None and hl_value > 0 else None

                # Get the Gamma
                gamma_value = media_gammas_used.get(saturated_var_name)

                summary_data.append({
                    'Original Channel': original_channel_name,
                    'Transformed Variable': saturated_var_name,
                    'Half-Life (HL)': hl_value,
                    'Retention Rate (RR)': f"{rr_value:.2%}" if rr_value is not None else None,
                    'Gamma': gamma_value
                })

            summary_df_params = pd.DataFrame(summary_data)
            display(summary_df_params)

    print("-" * 30)


print("\nModel Coefficients and Parameter Summary Complete.")
print("="*60)

"""## Summary:

### Data Analysis Key Findings

*   The final Ridge model was successfully trained using the best hyperparameters identified by `GridSearchCV`.
*   Predictions were generated for both the training and holdout datasets using the trained model.
*   Residuals (difference between actual and predicted values) were calculated for both training and holdout sets.
*   Visualizations were successfully generated, showing the actual vs. predicted sales over time, residuals over time, and the distribution of residuals for both datasets.
*   Model evaluation metrics were calculated:
    *   Training R-squared was 90.97%, and Holdout R-squared was 90.65%. The small difference (0.32%) suggests the model generalizes well. The high R-squared indicates the model explains a large portion of the variance.
    *   Training MAPE was 10.56%, and Holdout MAPE was 11.13%. These moderate MAPE values suggest reasonable accuracy, with an average prediction error of approximately \$12,157 (based on the average sales value of \$109,220.52).
    *   Training Durbin-Watson statistic was 2.0348, and Holdout Durbin-Watson was 1.9607. These values are close to 2, suggesting little to no significant autocorrelation in the residuals for both sets.
*   Variance Inflation Factors (VIF) were calculated for the features. All variables showed VIF values less than 5, indicating no significant multicollinearity issues among the independent variables.
*   Model coefficients, including the intercept, were calculated and displayed, sorted by absolute value.
*   A summary of the model parameters was provided, confirming the selected model type (Ridge), the best hyperparameters used (`alpha=100`), and the Half-Life, Retention Rate, and Gamma values used for adstocking and saturation of media variables.

### Insights or Next Steps

*   The model demonstrates good performance on both training and holdout data, with high R-squared and moderate MAPE, suggesting it captures the underlying trends well.
*   The low VIF values and Durbin-Watson statistic near 2 indicate that the model's assumptions regarding multicollinearity and residual autocorrelation are reasonably met, contributing to the reliability of the coefficient interpretations.

"""

# Updated, error-hardened version of your "Generate and Plot Model-Based Response Curves" cell.
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import traceback

# -----------------------
# Helper fallbacks
# -----------------------
# If you already have an apply_saturation function in your notebook, this will NOT override it.
if 'apply_saturation' not in globals():
    def apply_saturation(x, gamma):
        """
        Return a saturation-transformed value in the same units as x.
        Uses a stable formulation: sat_fraction = x / (x + gamma),
        saturated_output = sat_fraction * x so the result remains in the same units
        and still shows saturation as x grows.
        (This choice preserves units so scaling by mean/std of the adstocked variable makes sense.)
        """
        x = np.asarray(x, dtype=float)
        gamma = float(gamma) if gamma is not None else 1.0
        sat_frac = x / (x + gamma + 1e-9)
        return sat_frac * x

# If you already have a format_number function, this will not override it.
if 'format_number' not in globals():
    def format_number(v, precision=2):
        """Nice readable number with commas and SI fallback when very large."""
        try:
            if v is None or (isinstance(v, float) and np.isnan(v)):
                return "0"
            v = float(v)
            abs_v = abs(v)
            if abs_v >= 1e12:
                return f"{v/1e12:.{precision}f}T"
            if abs_v >= 1e9:
                return f"{v/1e9:.{precision}f}B"
            if abs_v >= 1e6:
                return f"{v/1e6:.{precision}f}M"
            if abs_v >= 1e3:
                return f"{v/1e3:.{precision}f}K"
            # small value, show as int if near integer
            if abs_v == int(abs_v):
                return f"{int(v):,}"
            return f"{v:,.{precision}f}"
        except Exception:
            return str(v)

# -----------------------
# Main block (safe checks + plotting + insights)
# -----------------------
try:
    print("\n" + "="*60)
    print("GENERATING AND PLOTTING MODEL-BASED RESPONSE CURVES")
    print("="*60)

    required_vars = ['final_model', 'scaler', 'media_half_lives', 'media_gammas_used', 'X_train', 'y_train', 'data']
    missing = [v for v in required_vars if v not in globals()]
    if missing:
        raise RuntimeError(f"Error: Required variables missing from the environment: {missing}. "
                           "Please run previous steps to create them.")

    # model coefficients (map column -> coef)
    coefficients = dict(zip(X_train.columns, np.asarray(final_model.coef_).ravel()))

    # Find the transformed media variables of form "<channel>_adstocked_<...>_saturated_<...>"
    transformed_media_vars_in_model = [col for col in X_train.columns if '_adstocked_' in col.lower() and '_saturated_' in col.lower()]

    if not transformed_media_vars_in_model:
        print("\nNo adstocked + saturated media variables found in model features. Nothing to plot.")
    else:
        print("\nGenerating response curves for the following variables:")
        for var in transformed_media_vars_in_model:
            print(f" - {var}")
        print("-" * 30)

        y_mean = float(np.asarray(y_train).mean())
        y_std = float(np.asarray(y_train).std(ddof=0) if hasattr(y_train, 'std') else np.std(y_train))

        # Build adstocked scaler params dictionary using original data
        adstocked_scaler_params = {}
        for adstocked_var_name, hl_value in media_half_lives.items():
            if adstocked_var_name in data.columns:
                col = data[adstocked_var_name].astype(float)
                adstocked_scaler_params[adstocked_var_name] = {'mean': float(col.mean()), 'std': float(col.std(ddof=0))}
            else:
                print(f"Warning: Adstocked column '{adstocked_var_name}' not found in original data.")

        # For each saturated variable build curve
        for saturated_var_name in transformed_media_vars_in_model:
            print(f"\nGenerating response curve for {saturated_var_name}...")

            # Parse names
            parts = saturated_var_name.split('_adstocked_')
            if len(parts) < 2:
                print(f"Warning: Could not parse name for '{saturated_var_name}'. Skipping.")
                continue
            original_channel_name = parts[0]
            adstocked_part = parts[1]
            # reconstruct the 'before saturation' adstocked var name
            adstocked_var_name_before_sat = f"{original_channel_name}_adstocked_{adstocked_part.split('_saturated_')[0]}"

            # get HL and gamma
            hl_value = media_half_lives.get(adstocked_var_name_before_sat)
            gamma_value = media_gammas_used.get(saturated_var_name)
            if hl_value is None or gamma_value is None:
                print(f"Warning: HL or gamma missing for '{saturated_var_name}' (HL={hl_value}, gamma={gamma_value}). Skipping.")
                continue

            rr_value = (0.5)**(1/hl_value) if hl_value > 0 else 0.0
            scaled_coefficient = coefficients.get(saturated_var_name)
            if scaled_coefficient is None:
                print(f"Warning: Model coefficient not found for '{saturated_var_name}'. Skipping.")
                continue

            scaler_params = adstocked_scaler_params.get(adstocked_var_name_before_sat)
            if scaler_params is None:
                print(f"Warning: Scaler parameters for '{adstocked_var_name_before_sat}' not found. Skipping.")
                continue
            mean_at_index = float(scaler_params['mean'])
            std_at_index = float(scaler_params['std']) if scaler_params['std'] != 0 else 1.0

            # original media column (assumes same as original_channel_name)
            original_media_col = original_channel_name
            if original_media_col not in data.columns:
                print(f"Warning: Original media column '{original_media_col}' not in data. Skipping.")
                continue

            original_max = float(data[original_media_col].max())
            simulation_range = np.linspace(0.0, max(1.0, original_max * 1.5), 200)
            max_sim_input = float(simulation_range.max())

            estimated_impacts = []
            for original_input in simulation_range:
                # Adstock steady state
                steady_state_adstocked = original_input / (1 - rr_value + 1e-9)
                # Saturation (returns same unit scale as steady_state_adstocked)
                saturated_output_for_curve = apply_saturation(steady_state_adstocked, gamma_value)

                # If the saturated output is essentially constant (bad scaling), use a fallback that preserves scale:
                if np.nanstd(saturated_output_for_curve) == 0:
                    saturated_output_for_curve = steady_state_adstocked * (steady_state_adstocked / (steady_state_adstocked + gamma_value + 1e-9))

                # Scale using adstocked variable mean/std (safe divide)
                scaled_saturated_output = (saturated_output_for_curve - mean_at_index) / (std_at_index + 1e-9)

                # Estimated impact on scaled sales
                scaled_impact_on_sales = scaled_saturated_output * scaled_coefficient

                # Bring back to original sales scale (scaled -> original)
                estimated_impact_on_original_sales = scaled_impact_on_sales * y_std

                estimated_impacts.append(float(estimated_impact_on_original_sales))

            estimated_impacts = np.asarray(estimated_impacts, dtype=float)

            # Build saturation annotations (50% and 80%)
            annotations = []
            for sat_level in [0.5, 0.8]:
                # Solve input such that sat_frac = input / (input + gamma) = sat_level
                adstocked_input_at_sat_level = (sat_level * gamma_value) / (1 - sat_level + 1e-9)
                original_input_at_sat_level = adstocked_input_at_sat_level * (1 - rr_value)

                if 0.0 <= original_input_at_sat_level <= max_sim_input:
                    closest_index = int(np.abs(simulation_range - original_input_at_sat_level).argmin())
                    estimated_impact_at_sat = float(estimated_impacts[closest_index])
                    annotations.append(dict(
                        x=original_input_at_sat_level,
                        y=estimated_impact_at_sat,
                        xref='x', yref='y',
                        text=f"{int(sat_level*100)}% Saturation",
                        showarrow=True, arrowhead=2,
                        ax=20, ay=-40
                    ))

            # Plot
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=simulation_range, y=estimated_impacts, mode='lines', name='Estimated Impact on Sales'))

            # Percentile verticals
            percentiles_to_plot = {0.25: {'color': 'blue', 'name': '25th Pct'},
                                   0.50: {'color': 'green', 'name': '50th Pct (Median)'},
                                   0.75: {'color': 'red', 'name': '75th Pct'}}

            if original_media_col in data.columns:
                original_percentile_values = data[original_media_col].quantile(list(percentiles_to_plot.keys())).to_dict()
                for percentile, value in original_percentile_values.items():
                    if not (0 <= value <= max_sim_input):
                        continue
                    color = percentiles_to_plot[percentile]['color']
                    name = percentiles_to_plot[percentile]['name']

                    # Different annotation offsets to minimize overlap
                    ay_offset = -40
                    annotation_pos = "bottom left"
                    if percentile == 0.25:
                        ay_offset = -60
                        annotation_pos = "bottom left"
                    elif percentile == 0.50:
                        ay_offset = 20
                        annotation_pos = "top left"
                    elif percentile == 0.75:
                        ay_offset = -60
                        annotation_pos = "bottom right"

                    # Add vertical line with readable annotation
                    fig.add_vline(x=value, line_dash='dot', line_color=color,
                                  annotation_text=f"{name}: {format_number(value)}",
                                  annotation_position=annotation_pos,
                                  annotation=dict(ay=ay_offset))

            # horizontal baseline
            fig.add_hline(y=0, line_dash='dash', line_color='gray')

            # Axis tick formats
            max_impact_val = float(max(np.nanmax(np.abs(estimated_impacts)), float(np.nanmax(np.abs(y_train)) if hasattr(y_train, '__iter__') else abs(y_train))))
            tick_format_impact = ".2s" if max_impact_val >= 1e3 else ",d"
            tick_format_original = ".2s" if max_sim_input >= 1e3 else ",d"

            # Merge our annotations and any existing ones safely
            existing_annotations = list(fig.layout.annotations) if fig.layout.annotations else []
            fig.update_layout(title=f"Estimated Response Curve for {original_channel_name} (HL={hl_value}, Gamma={gamma_value})",
                              xaxis_title=f"Original {original_channel_name} Input",
                              yaxis_title="Estimated Impact on Sales",
                              hovermode='x unified',
                              annotations=annotations + existing_annotations,
                              xaxis=dict(tickformat=tick_format_original),
                              yaxis=dict(tickformat=tick_format_impact))

            fig.show()

            # -----------------------
            # Insights (clean, plain-language)
            # -----------------------
            print("\n--- Insights ---")

            # Percentiles used for insights
            percentiles_for_insights = [0.0, 0.25, 0.50, 0.75]
            percentile_values_for_insights = {}
            percentile_impacts_for_insights = {}

            original_percentiles = data[original_media_col].quantile(percentiles_for_insights).to_dict()

            # Baseline (0)
            if 0.0 in original_percentiles and simulation_range[0] == 0.0:
                percentile_values_for_insights[0.0] = 0.0
                percentile_impacts_for_insights[0.0] = float(estimated_impacts[np.abs(simulation_range - 0.0).argmin()])
            # Populate the rest
            for p in [0.25, 0.50, 0.75]:
                if p in original_percentiles:
                    v = float(original_percentiles[p])
                    percentile_values_for_insights[p] = v
                    percentile_impacts_for_insights[p] = float(estimated_impacts[np.abs(simulation_range - v).argmin()])

            if not percentile_impacts_for_insights:
                print("Could not compute percentile impacts. Check original media column data.")
                continue

            # Values for easy reference
            impact_at_0 = percentile_impacts_for_insights.get(0.0, float(estimated_impacts[0]))
            impact_at_25 = percentile_impacts_for_insights.get(0.25, impact_at_0)
            impact_at_50 = percentile_impacts_for_insights.get(0.50, impact_at_25)
            impact_at_75 = percentile_impacts_for_insights.get(0.75, impact_at_50)

            total_impact_range = float(np.nanmax(estimated_impacts) - np.nanmin(estimated_impacts))
            negligible_change_threshold = max(1e-9, total_impact_range * 0.01)  # 1% of total impact range

            # If total impact range is essentially zero, say so and skip deep comparisons
            if np.isclose(total_impact_range, 0.0):
                print(f"- The estimated impact is effectively flat across the simulated range (change ~ 0). This indicates saturation / no measurable incremental effect from this channel.")
                print("- Consider reducing spend on this channel and reallocating.")
                print("-" * 30)
                continue

            # Compute changes
            change_0_25 = impact_at_25 - impact_at_0
            change_25_50 = impact_at_50 - impact_at_25
            change_50_75 = impact_at_75 - impact_at_50

            # Present numeric summary
            print(f"- At 0 (baseline): impact ≈ {format_number(impact_at_0)}")
            if 0.25 in percentile_values_for_insights:
                print(f"- 25th pct (input ≈ {format_number(percentile_values_for_insights[0.25])}): impact ≈ {format_number(impact_at_25)}")
            if 0.50 in percentile_values_for_insights:
                print(f"- 50th pct (input ≈ {format_number(percentile_values_for_insights[0.50])}): impact ≈ {format_number(impact_at_50)}")
            if 0.75 in percentile_values_for_insights:
                print(f"- 75th pct (input ≈ {format_number(percentile_values_for_insights[0.75])}): impact ≈ {format_number(impact_at_75)}")

            # Interpret each band
            def interpret_change(delta, label):
                if abs(delta) < negligible_change_threshold:
                    return f"- {label}: negligible change ({format_number(delta)}) → saturated / flat in this band."
                elif delta > 0:
                    return f"- {label}: positive incremental impact ({format_number(delta)}) — growth region (returns present)."
                else:
                    return f"- {label}: negative change ({format_number(delta)}) — unexpected drop (investigate)."

            print(interpret_change(change_0_25, "0 → 25th percentile"))
            print(interpret_change(change_25_50, "25th → 50th percentile"))
            print(interpret_change(change_50_75, "50th → 75th percentile"))

            # Overall guidance
            if abs(change_0_25) < negligible_change_threshold and abs(change_25_50) < negligible_change_threshold:
                print("- Overall: Channel looks saturated even at low historical spend. Recommend pausing increases and reallocating budget to channels still in growth phase.")
            elif abs(change_50_75) < negligible_change_threshold and (change_0_25 > negligible_change_threshold):
                print("- Overall: Strong initial returns at lower spend, but diminishing returns beyond the median. Consider capping spending near the median and optimizing creatives/targeting.")
            else:
                print("- Overall: Channel shows positive incremental returns in parts of the historical range — weigh incremental ROI before increasing spend further.")

            # Position of historical 75th percentile relative to the simulated range
            pct75_val = percentile_values_for_insights.get(0.75, None)
            if pct75_val is not None:
                if pct75_val > max_sim_input * 0.8:
                    print("- Note: Historical 75th percentile spend is in the upper 20% of the simulated range → much spend is at high levels where returns likely diminish.")
                elif pct75_val < max_sim_input * 0.2:
                    print("- Note: Historical 75th percentile spend is low relative to simulated range → historical spend concentrated near the low end; there may be headroom.")

            print("-" * 30)

    print("\nModel-Based Response Curve Generation Complete.")
    print("="*60)

except Exception as e:
    print("An error occurred while generating response curves:")
    traceback.print_exc()