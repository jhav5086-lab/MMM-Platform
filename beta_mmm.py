# -*- coding: utf-8 -*-
"""Beta_MMM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14klC7g67_QAaPRaWFD-MODrI2GzTqHA8
"""

# Marketing Mix Modeling - Colab Testing Version (Fixed)
import pandas as pd
import numpy as np
# import matplotlib.pyplot as plt # Commented out as not used for main plots
# import seaborn as sns # Commented out as not used for main plots
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Add other necessary imports
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.optimize import minimize
from statsmodels.tsa.seasonal import seasonal_decompose

# Plotly imports
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots # Import make_subplots
from IPython.display import display

# Set style for plots (not needed for Plotly)
# plt.style.use('default')
# sns.set_palette("husl")


# Function to convert Indian number format to float
def convert_indian_number(value):
    """Convert Indian number format string to float"""
    if isinstance(value, str):
        # Remove commas and strip whitespace
        cleaned_value = value.replace(',', '').strip()

        # Handle special cases like ' -   ' which should be treated as NaN
        if cleaned_value in ['-', ''] or cleaned_value.isspace():
            return np.nan

        try:
            return float(cleaned_value)
        except ValueError:
            print(f"Could not convert value: '{value}'")
            return np.nan
    return value

# Load and preprocess data
def load_and_preprocess_data(file_path):
    """Load and preprocess the marketing mix data"""
    # Load the data
    data = pd.read_csv(file_path)

    # First, identify all columns that might contain Indian number format
    # These are all columns except the date column
    all_columns = data.columns.tolist()
    date_column = 'Week_Ending'

    if date_column in all_columns:
        all_columns.remove(date_column)

    # Convert all numeric columns (including Sales)
    for col in all_columns:
        # Check if the column contains string values with commas (Indian number format)
        if data[col].dtype == 'object' and data[col].str.contains(',').any():
            data[col] = data[col].apply(convert_indian_number)
        # Also convert columns that might be objects but don't have commas (like ' - ')
        elif data[col].dtype == 'object':
             data[col] = data[col].apply(convert_indian_number)


    # Handle missing values in Paid Search Impressions
    if 'Paid Search Impressions' in data.columns:
        missing_count = data['Paid Search Impressions'].isna().sum()
        if missing_count > 0:
            print(f"Found {missing_count} missing values in 'Paid Search Impressions'. Imputing with 0.")
            data['Paid Search Impressions'] = data['Paid Search Impressions'].fillna(0)

    # Convert date column
    if 'Week_Ending' in data.columns:
        data['Week_Ending'] = pd.to_datetime(data['Week_Ending'], format='%d-%m-%Y %H:%M', errors='coerce')
        data = data.sort_values('Week_Ending').reset_index(drop=True)

    return data

# Format numbers in millions (not needed for Plotly tickformat)
# def format_millions(x, pos):
#     """Format numbers in millions for plot axes"""
#     return f'{x/1e6:.1f}M'


# Perform comprehensive EDA
def perform_comprehensive_eda(data, target_var='Sales'):
    """Perform comprehensive exploratory data analysis"""
    print("="*60)
    print("COMPREHENSIVE EXPLORATORY DATA ANALYSIS")
    print("="*60)

    # 1. Basic Information
    print("\n1. BASIC DATASET INFORMATION")
    print("="*40)
    print(f"Shape: {data.shape}")
    print(f"Columns: {list(data.columns)}")
    if 'Week_Ending' in data.columns:
        print(f"Date Range: {data['Week_Ending'].min()} to {data['Week_Ending'].max()}")
    print(f"Missing Values: {data.isnull().sum().sum()}")

    # Check if target variable is numeric - should be handled in load_and_preprocess_data now
    if data[target_var].dtype == 'object':
         print(f"\nWARNING: Target variable '{target_var}' is not numeric after preprocessing.")


    # 2. Summary Statistics
    print("\n\n2. SUMMARY STATISTICS")
    print("="*40)

    # Numeric variables summary
    numeric_df = data.select_dtypes(include=[np.number])
    print("Numeric Variables Summary:")
    display(numeric_df.describe())

    # Add skewness and kurtosis
    skewness = numeric_df.skew().to_frame('Skewness')
    kurtosis = numeric_df.kurtosis().to_frame('Kurtosis')
    stats_df = pd.concat([skewness, kurtosis], axis=1)
    print("\nSkewness and Kurtosis:")
    display(stats_df)

    # 3. Univariate Analysis
    print("\n\n3. UNIVARIATE ANALYSIS")
    print("="*40)

    # Create distribution plots for all numeric variables using Plotly
    numeric_cols = numeric_df.columns.tolist()
    for col in numeric_cols:
        fig = px.histogram(data, x=col, nbins=30, title=f"Distribution of {col}")
        fig.update_layout(
            xaxis_title=col,
            yaxis_title="Frequency",
            bargap=0.1 # Add gap between bars
        )
        # Add vertical lines for mean and median
        mean_val = data[col].mean()
        median_val = data[col].median()
        fig.add_vline(x=mean_val, line_dash="dash", line_color="red", annotation_text=f"Mean: {mean_val:.2f}", annotation_position="top right")
        fig.add_vline(x=median_val, line_dash="dash", line_color="green", annotation_text=f"Median: {median_val:.2f}", annotation_position="top left")

        fig.show()


    # 4. Bivariate Analysis
    print("\n\n4. BIVARIATE ANALYSIS: RELATIONSHIP WITH TARGET VARIABLE")
    print("="*40)

    # Create scatter plots against target variable using Plotly
    if target_var in numeric_cols:
        numeric_cols_for_scatter = numeric_cols.copy()
        numeric_cols_for_scatter.remove(target_var)

    for col in numeric_cols_for_scatter:
        fig = px.scatter(data, x=col, y=target_var, title=f"{target_var} vs {col}")
        fig.update_layout(
            xaxis_title=col,
            yaxis_title=target_var
        )
        # Add correlation coefficient
        correlation = data[col].corr(data[target_var])
        fig.add_annotation(
            x=data[col].min() + (data[col].max() - data[col].min()) * 0.05,
            y=data[target_var].max() - (data[target_var].max() - data[target_var].min()) * 0.05,
            text=f"r = {correlation:.3f}",
            showarrow=False,
            bgcolor="white",
            opacity=0.8
        )
        fig.show()


    # 5. Time Series Analysis
    print("\n\n5. TIME SERIES ANALYSIS")
    print("="*40)

    if 'Week_Ending' in data.columns:
        # Plot target variable over time using Plotly
        fig = px.line(data, x='Week_Ending', y=target_var, title=f"{target_var} Over Time (in Millions)")
        fig.update_layout(
            xaxis_title="Date",
            yaxis_title="Sales (in Millions)",
            yaxis_tickformat=".1s" # Format y-axis to show values in millions
        )
        fig.show()


        # Add seasonal decomposition using Plotly
        print("Seasonal Decomposition:")
        try:
            # Ensure the data is sorted by date and set as index
            temp_df = data.set_index('Week_Ending').sort_index()
            decomposition = seasonal_decompose(temp_df[target_var], period=4, model='additive', extrapolate_trend='freq')

            fig = make_subplots(rows=4, cols=1,
                                subplot_titles=('Observed', 'Trend', 'Seasonal', 'Residuals'),
                                shared_xaxes=True)

            fig.add_trace(go.Scatter(x=temp_df.index, y=decomposition.observed, mode='lines', name='Observed'), row=1, col=1)
            fig.add_trace(go.Scatter(x=temp_df.index, y=decomposition.trend, mode='lines', name='Trend'), row=2, col=1)
            fig.add_trace(go.Scatter(x=temp_df.index, y=decomposition.seasonal, mode='lines', name='Seasonal'), row=3, col=1)
            fig.add_trace(go.Scatter(x=temp_df.index, y=decomposition.resid, mode='lines', name='Residuals'), row=4, col=1)


            fig.update_layout(height=800, title_text="Seasonal Decomposition")
            # Format y-axis to show values in millions
            fig.update_yaxes(title_text="Sales (M)", tickformat=".1s", row=1, col=1)
            fig.update_yaxes(title_text="Trend (M)", tickformat=".1s", row=2, col=1)
            fig.update_yaxes(title_text="Seasonal (M)", tickformat=".1s", row=3, col=1)
            fig.update_yaxes(title_text="Residual (M)", tickformat=".1s", row=4, col=1)
            fig.show()
        except Exception as e:
            print(f"Could not perform seasonal decomposition: {str(e)}")

    # 6. Correlation Analysis
    print("\n\n6. CORRELATION ANALYSIS")
    print("="*40)

    # Full correlation matrix using Plotly
    print("Full Correlation Matrix:")
    corr = numeric_df.corr()
    # Corrected heatmap using go.Heatmap
    fig = go.Figure(data=go.Heatmap(
                        z=corr.values,
                        x=corr.columns,
                        y=corr.index,
                        colorscale='RdBu', # Changed colorscale to a valid one
                        zmid=0, # Center the color scale at 0
                        colorbar=dict(title="Correlation"),
                        text=corr.values.round(2), # Add text labels
                        texttemplate="%{text}" # Format text labels
                      ))

    fig.update_layout(title="Correlation Matrix - All Variables")
    fig.show()

    # Media variables correlation
    media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video']
    media_cols = [col for col in numeric_df.columns if any(keyword in col.lower() for keyword in media_keywords)]

    if media_cols and target_var in numeric_df.columns:
        print("Media Variables Correlation with Target:")
        media_corr = numeric_df[media_cols + [target_var]].corr()
        # Extract only correlations with target variable
        target_corr = media_corr[target_var].drop(target_var).sort_values(ascending=False)

        # Plot correlations as vertical bars using Plotly
        fig = px.bar(x=target_corr.index, y=target_corr.values * 100, title=f"Correlation of Media Variables with {target_var} (%)")
        fig.update_layout(
            xaxis_title="Media Variables",
            yaxis_title="Correlation Coefficient (%)",
            yaxis_ticksuffix="%", # Add percentage suffix
            xaxis_tickangle=-45
        )
        fig.show()


        # Also show as a table
        print((target_corr * 100).round(2).to_frame("Correlation (%)"))

    # 7. Outlier Analysis
    print("\n\n7. OUTLIER ANALYSIS")
    print("="*40)

    # Check for outliers in target variable
    Q1 = data[target_var].quantile(0.25)
    Q3 = data[target_var].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identify outliers
    outliers = data[(data[target_var] < lower_bound) | (data[target_var] > upper_bound)]
    normal_data = data[~((data[target_var] < lower_bound) | (data[target_var] > upper_bound))]
    print(f"Number of potential outliers in {target_var}: {len(outliers)}")

    if len(outliers) > 0:
        print("Outlier values:")
        display(outliers[['Week_Ending', target_var]])

        # Plot with outliers highlighted using Plotly
        fig = go.Figure()

        # Add normal values
        fig.add_trace(go.Scattergl(
            x=normal_data['Week_Ending'],
            y=normal_data[target_var],
            mode='markers',
            name='Normal Values',
            marker=dict(color='darkgreen', opacity=0.7, size=8)
        ))

        # Add outliers
        fig.add_trace(go.Scattergl(
            x=outliers['Week_Ending'],
            y=outliers[target_var],
            mode='markers',
            name='Outliers',
            marker=dict(color='red', opacity=0.9, size=10)
        ))

        # Add bounds lines
        fig.add_shape(type="line",
            x0=data['Week_Ending'].min(), y0=upper_bound, x1=data['Week_Ending'].max(), y1=upper_bound,
            line=dict(color="red", width=2, dash="dash"),
            name='Upper Bound'
        )
        fig.add_shape(type="line",
            x0=data['Week_Ending'].min(), y0=lower_bound, x1=data['Week_Ending'].max(), y1=lower_bound,
            line=dict(color="red", width=2, dash="dash"),
            name='Lower Bound'
        )


        fig.update_layout(
            title=f"Outlier Detection in {target_var}",
            xaxis_title="Date",
            yaxis_title=target_var,
            yaxis_tickformat=".1s" # Format y-axis to show values in millions
        )
        fig.show()


    # Return numeric_df and correlation matrix for interactive use
    return numeric_df, corr


# Main execution
if __name__ == "__main__":
    # Load and preprocess your data
    file_path = "/content/Book1.csv"  # Update this path to your file location
    data = load_and_preprocess_data(file_path)

    # Define the target variable here
    target_var = 'Sales'

    # Perform comprehensive EDA and get the numeric dataframe and correlation matrix
    numeric_df, correlations = perform_comprehensive_eda(data, target_var=target_var)

import pandas as pd
import numpy as np
from statsmodels.tsa.seasonal import seasonal_decompose
from IPython.display import display

def feature_engineering_module(df):
    """
    Feature engineering module for time series data
    """
    print("\n===== FEATURE ENGG MODULE =====")

    # Check for required columns
    if 'Week_Ending' not in df.columns:
        raise ValueError("DataFrame must contain a 'Week_Ending' column")
    if 'Sales' not in df.columns:
        raise ValueError("DataFrame must contain a 'Sales' column")

    # Ensure we're working with a copy to avoid modifying original data
    df = df.copy()

    # Track original columns to identify new features later
    original_columns = set(df.columns)

    # Ensure datetime index
    df['Week_Ending'] = pd.to_datetime(df['Week_Ending'])
    df = df.set_index('Week_Ending').sort_index()

    # 1. Seasonal Index (SIndex) - Always created
    try:
        period = 52  # Default period for weekly data
        if len(df) >= 2 * period:
            decomp = seasonal_decompose(df['Sales'], period=period, model='additive', extrapolate_trend='freq')
            df['SIndex'] = decomp.seasonal
            print(f"✅ Seasonal Index created with period={period}")
        else:
            print(f"⚠️ Not enough data points ({len(df)}) for seasonality period {period}")
    except Exception as e:
        print(f"⚠️ Could not create Seasonal Index: {str(e)}")

    # 2. Holiday Dummies
    try:
        add_dummies = input("\nDo you want to add holiday dummies? (Y/N): ").strip().lower()
        if add_dummies == 'y':
            dates = input("Enter holiday dates (comma separated, format YYYY-MM-DD): ")
            if dates.strip():
                dates = [d.strip() for d in dates.split(",")]
                for i, d in enumerate(dates, start=1):
                    try:
                        holiday_date = pd.to_datetime(d)
                        col_name = f"Holiday_{i}"
                        df[col_name] = (df.index == holiday_date).astype(int)
                        print(f"✅ Added dummy: {col_name} for {d}")
                    except:
                        print(f"⚠️ Could not parse date: {d}")
    except:
        print("⚠️ Skipping holiday dummies due to input issue")

    # 3. Split Variable
    try:
        split_choice = input("\nDo you want to split a variable at a date? (Y/N): ").strip().lower()
        if split_choice == 'y':
            print("\nAvailable variables:")
            all_vars = [col for col in df.columns if col not in ['SIndex']]
            for i, var in enumerate(all_vars, 1):
                print(f"{i}. {var}")

            try:
                var_idx = int(input("Select variable number to split: ")) - 1
                if 0 <= var_idx < len(all_vars):
                    var_name = all_vars[var_idx]
                    split_date = input("Enter split date (YYYY-MM-DD): ").strip()

                    try:
                        split_dt = pd.to_datetime(split_date)
                        df[f"{var_name}_pre"] = np.where(df.index <= split_dt, df[var_name], 0)
                        df[f"{var_name}_post"] = np.where(df.index > split_dt, df[var_name], 0)

                        # Drop the original variable as requested
                        df.drop(columns=[var_name], inplace=True)
                        print(f"✅ Split {var_name} into {var_name}_pre and {var_name}_post at {split_dt.date()}")
                        print(f"✅ Dropped original variable: {var_name}")
                    except Exception as e:
                        print(f"⚠️ Error splitting variable: {str(e)}")
                else:
                    print("⚠️ Invalid selection")
            except:
                print("⚠️ Invalid input")
    except:
        print("⚠️ Skipping variable splitting due to input issue")

    # 4. Super Campaign - Enhanced with variable selection by number
    try:
        super_choice = input("\nDo you want to create a super campaign? (Y/N): ").strip().lower()
        if super_choice == 'y':
            print("\nAvailable variables:")
            all_vars = [col for col in df.columns if col not in ['SIndex', 'Sales'] and not col.endswith(('_pre', '_post'))]
            for i, var in enumerate(all_vars, 1):
                print(f"{i}. {var}")

            try:
                selected = input("Enter variable numbers to combine (comma separated): ")
                var_indices = [int(x.strip())-1 for x in selected.split(",") if x.strip().isdigit()]
                vars_to_combine = [all_vars[i] for i in var_indices if 0 <= i < len(all_vars)]

                if vars_to_combine:
                    # Ask for custom name
                    name_choice = input("Do you want to provide a custom name? (Y/N): ").strip().lower()
                    if name_choice == 'y':
                        super_col = input("Enter the custom name: ").strip()
                    else:
                        super_col = "combined_var"

                    # Create the super campaign
                    df[super_col] = df[vars_to_combine].sum(axis=1)

                    # Display verification of sums
                    print(f"\nVerification of sums for {super_col}:")
                    for var in vars_to_combine:
                        print(f"{var}: {df[var].sum():.2f}")
                    print(f"{super_col}: {df[super_col].sum():.2f}")

                    # Drop the original variables as requested
                    df.drop(columns=vars_to_combine, inplace=True)
                    print(f"✅ Created Super Campaign: {super_col} combining {vars_to_combine}")
                    print(f"✅ Dropped original variables: {vars_to_combine}")
                else:
                    print("⚠️ No valid variables selected")
            except:
                print("⚠️ Invalid input")
    except:
        print("⚠️ Skipping super campaign creation due to input issue")

    # Show extra features created
    new_features = sorted(set(df.columns) - original_columns)

    if new_features:
        print("\n📊 EXTRA FEATURES CREATED:")
        print("=" * 50)

        # Create a summary table
        feature_info = []
        for col in new_features:
            if col == "SIndex":
                feature_type = "Seasonal Index"
            elif col.startswith("Holiday_"):
                feature_type = "Holiday Dummy"
            elif col.endswith(("_pre", "_post")):
                feature_type = "Split Variable"
            elif col == "combined_var" or col == "Super_Campaign":
                feature_type = "Super Campaign"
            else:
                feature_type = "Other"

            feature_info.append({
                "Feature Name": col,
                "Type": feature_type,
                "Data Type": str(df[col].dtype),
                "Non-Zero Values": f"{(df[col] != 0).sum()} / {len(df)}"
            })

        # Display as table
        display(pd.DataFrame(feature_info))

        # Show first few rows of new features
        print("\n📋 SAMPLE OF NEW FEATURES:")
        display(df[new_features].head())
    else:
        print("\nℹ️ No additional features were created.")

    return df

def interactive_data_exploration(df_features):
    """
    Interactive data exploration function
    """
    print("\n\n===== INTERACTIVE DATA EXPLORATION =====")
    print("-" * 50)
    print("Ask questions about any variable in the dataset")
    print("Type 'N', 'NO', or 'no' to finish, 'variables' to see all available variables, or 'help' for examples")

    # Get all available variables for reference
    all_variables = list(df_features.columns)
    numeric_variables = list(df_features.select_dtypes(include=[np.number]).columns)

    # Recompute correlations with new features
    numeric_df_features = df_features.select_dtypes(include=[np.number])
    correlations = numeric_df_features.corr()

    # Simple question-answering mechanism
    while True:
        try:
            question = input("\nWhat would you like to know about your data? ").strip().lower()

            # Check for exit condition
            if question in ['n', 'no']:
                print("Exiting interactive mode...")
                break

        except:
            print("\nInput error. Exiting interactive mode.")
            break

        if question == 'variables':
            print("Available variables:")
            for var in all_variables:
                print(f"• {var}")
            continue
        elif question == 'help':
            print("Examples of questions I can answer:")
            print("• 'paid search' - Get comprehensive analysis of paid search metrics")
            print("• 'discount1' - Analyze discount1 performance and relationship with sales")
            print("• 'social media' - Explore all social media related metrics")
            print("• 'trends' - View sales trends over time")
            print("• 'correlations' - See strongest correlations with sales")
            print("• 'seasonality' - Analyze seasonal patterns in sales")
            print("• 'holidays' - Analyze holiday impacts on sales")
            print("• 'split variables' - Show information about split variables")
            print("• 'super campaign' - Show information about super campaign")
            print("• 'sales correlation with paid search' - Get correlation between sales and paid search")
            continue

        # Check for special queries about feature engineering elements
        if 'seasonal' in question or 'sindex' in question:
            if 'SIndex' in df_features.columns:
                print("\nSEASONALITY ANALYSIS:")
                print("=" * 50)

                # Calculate seasonal strength
                seasonal_strength = df_features['SIndex'].std() / df_features['Sales'].std() * 100
                print(f"Seasonal strength: {seasonal_strength:.1f}% of sales variation")

                # Find strongest seasonal periods
                seasonal_peaks = df_features['SIndex'].nlargest(5)
                seasonal_troughs = df_features['SIndex'].nsmallest(5)

                print("\nStrongest seasonal peaks:")
                for date, value in seasonal_peaks.items():
                    print(f"• {date.strftime('%Y-%m-%d')}: {value:,.0f}")

                print("\nStrongest seasonal troughs:")
                for date, value in seasonal_troughs.items():
                    print(f"• {date.strftime('%Y-%m-%d')}: {value:,.0f}")
            else:
                print("No seasonal index found. Run feature engineering to create SIndex.")
            continue

        if 'holiday' in question:
            holiday_cols = [col for col in df_features.columns if col.startswith('Holiday_')]
            if holiday_cols:
                print("\nHOLIDAY ANALYSIS:")
                print("=" * 50)

                for col in holiday_cols:
                    holiday_dates = df_features[df_features[col] == 1].index
                    if not holiday_dates.empty:
                        print(f"\n{col}:")
                        for date in holiday_dates:
                            # Get sales on holiday vs average
                            holiday_sales = df_features.loc[date, 'Sales']
                            avg_sales = df_features['Sales'].mean()
                            pct_diff = (holiday_sales - avg_sales) / avg_sales * 100
                            print(f"• {date.strftime('%Y-%m-%d')}: Sales ₹{holiday_sales:,.0f} ({pct_diff:+.1f}% vs average)")
            else:
                print("No holiday dummies found. You can add them in the feature engineering module.")
            continue

        if 'split' in question:
            split_cols = [col for col in df_features.columns if col.endswith(('_pre', '_post'))]
            if split_cols:
                print("\nSPLIT VARIABLES ANALYSIS:")
                print("=" * 50)

                # Group by root variable
                split_roots = {}
                for col in split_cols:
                    root = col.rsplit('_', 1)[0]
                    if root not in split_roots:
                        split_roots[root] = []
                    split_roots[root].append(col)

                for root, cols in split_roots.items():
                    print(f"\n{root}:")
                    for col in cols:
                        period = "before" if col.endswith('_pre') else "after"
                        avg_val = df_features[col].mean()
                        print(f"• {col}: Average value {period} split: {avg_val:,.2f}")
            else:
                print("No split variables found. You can create them in the feature engineering module.")
            continue

        if 'super campaign' in question or 'combined' in question:
            super_cols = [col for col in df_features.columns if col in ['Super_Campaign', 'combined_var'] or 'super' in col.lower()]
            if super_cols:
                for super_col in super_cols:
                    print("\nSUPER CAMPAIGN ANALYSIS:")
                    print("=" * 50)

                    # Calculate correlation with sales
                    if 'Sales' in df_features.columns:
                        corr = df_features[super_col].corr(df_features['Sales'])
                        print(f"Correlation with sales: {corr:.3f}")

                    # Summary stats
                    avg_campaign = df_features[super_col].mean()
                    max_campaign = df_features[super_col].max()
                    min_campaign = df_features[super_col].min()

                    print(f"Average {super_col} value: {avg_campaign:,.2f}")
                    print(f"Maximum {super_col} value: {max_campaign:,.2f}")
                    print(f"Minimum {super_col} value: {min_campaign:,.2f}")

                    # Show contribution of original variables if available
                    print(f"Total {super_col} spend: {df_features[super_col].sum():,.2f}")
            else:
                print("No super campaign found. You can create one in the feature engineering module.")
            continue

        # Check for correlation questions
        if 'correlation' in question or 'relationship' in question:
            # Extract variable names from question
            mentioned_vars = [var for var in all_variables if var.lower() in question]

            if 'sales' in question and len(mentioned_vars) > 1:
                # Show correlation between sales and mentioned variables
                for var in mentioned_vars:
                    if var != 'Sales' and var in numeric_variables:
                        corr = df_features['Sales'].corr(df_features[var])
                        direction = "positive" if corr > 0 else "negative"
                        strength = "strong" if abs(corr) > 0.7 else "moderate" if abs(corr) > 0.3 else "weak"
                        print(f"Correlation between Sales and {var}: {corr:.3f} ({strength} {direction} relationship)")
            else:
                # Show top correlations with sales
                if 'Sales' in correlations.index:
                    strong_corrs = correlations['Sales'].drop('Sales').abs().sort_values(ascending=False)
                    # Filter for strong correlations (e.g., abs value > 0.4) and get original correlation values
                    strong_corrs = correlations['Sales'].loc[strong_corrs[strong_corrs > 0.4].index]

                    if not strong_corrs.empty:
                        print("Strongest relationships with sales:")
                        for var, corr in strong_corrs.items():
                            direction = "positive" if corr > 0 else "negative"
                            print(f"• {var}: {corr:.3f} ({direction})")
                    else:
                        print("No strong correlations found (|r| > 0.4) with Sales.")
                else:
                    print("Target variable 'Sales' not found in correlation matrix.")
            continue

        # Check for other special queries
        if 'trend' in question:
            # Show sales trends
            trend_data = df_features[['Sales']].copy()
            overall_trend = "increasing" if trend_data['Sales'].iloc[-1] > trend_data['Sales'].iloc[0] else "decreasing"

            # Handle potential division by zero if the first value is 0
            if trend_data['Sales'].iloc[0] != 0:
                change_pct = (trend_data['Sales'].iloc[-1] - trend_data['Sales'].iloc[0]) / trend_data['Sales'].iloc[0] * 100
                print(f"Sales show an {overall_trend} trend overall ({change_pct:+.1f}% change)")
            else:
                print(f"Sales show an {overall_trend} trend overall (starting from zero)")

            # Show monthly trends if available
            if len(trend_data) >= 12:
                monthly_avg = trend_data.resample('M').mean()
                if not monthly_avg.empty:
                    best_month = monthly_avg.idxmax()[0].strftime('%B')
                    worst_month = monthly_avg.idxmin()[0].strftime('%B')
                    print(f"Best month: {best_month}, Worst month: {worst_month}")
                else:
                    print("Not enough data for monthly trend analysis.")
            continue

        # Find matching variables
        matching_vars = [var for var in all_variables if question in var.lower()]

        if not matching_vars:
            # Try to understand what the user might be asking about
            if any(word in question for word in ['sale', 'revenue', 'income']):
                print(f"Analysis for Sales:")
                print(f"Range: ₹{df_features['Sales'].min():,.0f} to ₹{df_features['Sales'].max():,.0f}")
                print(f"Average: ₹{df_features['Sales'].mean():,.0f}")
                print(f"Standard Deviation: ₹{df_features['Sales'].std():,.0f}")

                # Calculate growth rate
                first_value = df_features['Sales'].iloc[0]
                last_value = df_features['Sales'].iloc[-1]
                # Handle potential division by zero if the first value is 0
                if first_value != 0:
                    growth_pct = (last_value - first_value) / first_value * 100
                    print(f"Growth over period: {growth_pct:+.1f}%")
                else:
                    print(f"Growth over period: Infinite (starting from zero)")
            else:
                # Try fuzzy matching for similar variable names
                import difflib
                suggestions = difflib.get_close_matches(question, all_variables, n=3, cutoff=0.3)
                if suggestions:
                    print(f"Variable '{question}' not found. Did you mean: {', '.join(suggestions)}?")
                else:
                    print(f"Variable '{question}' not found. Type 'variables' to see all available variables.")
            continue

        # Process each matching variable
        for var in matching_vars:
            print(f"\n{'='*50}")
            print(f"COMPREHENSIVE ANALYSIS FOR: {var}")
            print(f"{'='*50}")

            # Basic variable information
            if var in numeric_variables:
                print(f"Type: Numeric")
                print(f"Range: {df_features[var].min():,.2f} to {df_features[var].max():,.2f}")
                print(f"Mean: {df_features[var].mean():,.2f}")
                print(f"Standard Deviation: {df_features[var].std():,.2f}")
                print(f"Missing Values: {df_features[var].isnull().sum()} ({df_features[var].isnull().sum()/len(df_features)*100:.1f}%)")

                # Distribution shape
                skewness = df_features[var].skew()
                if abs(skewness) > 1:
                    shape = "highly skewed"
                elif abs(skewness) > 0.5:
                    shape = "moderately skewed"
                else:
                    shape = "approximately symmetric"
                print(f"Distribution: {shape} (skewness: {skewness:.2f})")
            else:
                print(f"Type: Categorical")
                print(f"Unique values: {df_features[var].nunique()}")
                if df_features[var].nunique() <= 10:
                    print("Value counts:")
                    print(df_features[var].value_counts())

            # Relationship with target variable
            if var in numeric_variables and var != 'Sales' and 'Sales' in df_features.columns:
                corr = df_features[var].corr(df_features['Sales'])
                print(f"\nRELATIONSHIP WITH Sales:")
                print(f"Correlation coefficient: {corr:.3f}")

                if abs(corr) > 0.7:
                    strength = "very strong"
                elif abs(corr) > 0.5:
                    strength = "strong"
                elif abs(corr) > 0.3:
                    strength = "moderate"
                elif abs(corr) > 0.1:
                    strength = "weak"
                else:
                    strength = "very weak or no"

                direction = "positive" if corr > 0 else "negative"
                print(f"→ {strength} {direction} relationship with sales")

                # Estimate impact
                if df_features[var].std() > 0:
                    # Calculate the estimated change in Sales for a one standard deviation change in the current variable
                    estimated_sales_change_per_std = corr * df_features['Sales'].std()
                    print(f"→ A one standard deviation increase in {var} is associated with an estimated ₹{estimated_sales_change_per_std:,.0f} change in sales.")

            # Time trend analysis
            # Calculate trend
            trend_data = df_features[[var]].copy()
            if len(trend_data) > 1:
                overall_trend = "increasing" if trend_data[var].iloc[-1] > trend_data[var].iloc[0] else "decreasing"
                # Handle potential division by zero if the first value is 0
                change_pct = (trend_data[var].iloc[-1] - trend_data[var].iloc[0]) / trend_data[var].iloc[0] * 100 if trend_data[var].iloc[0] != 0 else float('inf') if trend_data[var].iloc[-1] > 0 else 0

                print(f"\nTIME TREND ANALYSIS:")
                if change_pct == float('inf'):
                    print(f"→ Overall {overall_trend} trend (infinite percentage change from zero start)")
                else:
                    print(f"→ Overall {overall_trend} trend ({change_pct:+.1f}% change)")

                # Check for seasonality
                if len(trend_data) >= 12:
                    monthly_avg = trend_data.resample('M').mean()
                    if len(monthly_avg) > 3 and monthly_avg[var].mean() != 0:
                        seasonal_variation = (monthly_avg[var].max() - monthly_avg[var].min()) / monthly_avg[var].mean() * 100
                        print(f"→ Seasonal variation: {seasonal_variation:.1f}%")
                    elif monthly_avg[var].mean() == 0:
                        print(f"→ Seasonal variation: Infinite (average is zero)")
                    else:
                        print("→ Not enough data points for meaningful seasonal variation analysis.")

            # Compare with other variables in the same category
            if any(kw in var.lower() for kw in ['discount', 'promo']):
                discount_vars = [v for v in numeric_variables if any(kw in v.lower() for kw in ['discount', 'promo'])]
                if len(discount_vars) > 1:
                    print(f"\nCOMPARISON WITH OTHER DISCOUNT VARIABLES:")
                    for d_var in discount_vars:
                        if d_var != var:
                            d_corr = df_features[d_var].corr(df_features['Sales'])
                            print(f"→ {d_var}: r = {d_corr:.3f}")

            if any(kw in var.lower() for kw in ['search', 'impression', 'click', 'social', 'email', 'video']):
                media_vars = [v for v in numeric_variables if any(kw in v.lower() for kw in ['search', 'impression', 'click', 'social', 'email', 'video'])]
                if len(media_vars) > 1:
                    print(f"\nCOMPARISON WITH OTHER MEDIA VARIABLES:")
                    for m_var in media_vars:
                        if m_var != var:
                            m_corr = df_features[m_var].corr(df_features['Sales'])
                            print(f"→ {m_var}: r = {m_corr:.3f}")

            print(f"\nRECOMMENDATION FOR {var}:")
            if var in numeric_variables and var != 'Sales' and 'Sales' in df_features.columns:
                corr = df_features[var].corr(df_features['Sales'])
                if corr > 0.4:
                    print("→ Consider increasing investment in this area as it strongly correlates with sales")
                elif corr > 0.2:
                    print("→ This area shows positive correlation with sales, consider testing increased investment")
                elif corr < -0.2:
                    print("→ This area shows negative correlation with sales, consider investigating further")
                else:
                    print("→ No strong relationship with sales detected")

    # Show finalized list of variables by bucket after interactive session
    print("\n\n===== FINALIZED VARIABLES BY BUCKET =====")
    print("-" * 50)

    # Categorize variables
    base_vars = [col for col in df_features.columns if not col.startswith(('Holiday_', 'SIndex')) and not col.endswith(('_pre', '_post')) and col != 'Sales' and col not in ['combined_var', 'Super_Campaign']]
    holiday_vars = [col for col in df_features.columns if col.startswith('Holiday_')]
    split_vars = [col for col in df_features.columns if col.endswith(('_pre', '_post'))]
    seasonal_vars = [col for col in df_features.columns if col == 'SIndex']
    super_vars = [col for col in df_features.columns if col in ['combined_var', 'Super_Campaign']]

    print("TARGET VARIABLE:")
    print(f"• Sales")

    print("\nINDEPENDENT VARIABLES:")
    print("\nBase Variables:")
    for var in base_vars:
        print(f"• {var}")

    print("\nHoliday Dummies:")
    for var in holiday_vars:
        print(f"• {var}")

    print("\nSplit Variables:")
    for var in split_vars:
        print(f"• {var}")

    print("\nSeasonal Index:")
    for var in seasonal_vars:
        print(f"• {var}")

    print("\nSuper Campaign:")
    for var in super_vars:
        print(f"• {var}")

# Run feature engineering
df_features = feature_engineering_module(data)

# Run interactive data exploration
interactive_data_exploration(df_features)

# ================================================
# MODELING ENGINE - VARIABLE BUCKETING
# ================================================
def variable_bucketing(df):
    """
    Categorizes variables into different buckets for modeling

    Parameters:
    df (DataFrame): Input dataframe with all variables

    Returns:
    dict: Dictionary with variables categorized into buckets
    """
    print("\n===== VARIABLE BUCKETING =====")

    # Get all column names
    all_columns = df.columns.tolist()

    # Define buckets
    buckets = {
        'Media Variables': [],
        'Base Variables': [],
        'Promo Variables': [],
        'Extra Features': [],
        'Other Variables': []
    }

    # Media variables (keywords to identify media variables)
    media_keywords = ['impression', 'click', 'social', 'search', 'email', 'video', 'media', 'campaign', 'ad', 'spend']
    for col in all_columns:
        if any(keyword in col.lower() for keyword in media_keywords):
            buckets['Media Variables'].append(col)

    # Promo variables (discount-related)
    promo_keywords = ['discount', 'promo', 'promotion', 'offer']
    for col in all_columns:
        if any(keyword in col.lower() for keyword in promo_keywords):
            buckets['Promo Variables'].append(col)

    # Extra features (created during feature engineering)
    extra_keywords = ['sindex', 'holiday', '_pre', '_post', 'super_campaign']
    for col in all_columns:
        if any(keyword in col.lower() for keyword in extra_keywords):
            buckets['Extra Features'].append(col)

    # Base variables (everything else excluding Sales and date index)
    base_exclusions = ['sales', 'week_ending'] + buckets['Media Variables'] + buckets['Promo Variables'] + buckets['Extra Features']
    for col in all_columns:
        if col.lower() not in [excl.lower() for excl in base_exclusions]:
            buckets['Base Variables'].append(col)

    # Remove duplicates (if any variable was categorized in multiple buckets)
    for bucket_name in buckets:
        buckets[bucket_name] = list(set(buckets[bucket_name]))

    # Display the bucketing results
    print("📊 VARIABLE BUCKETS:")
    print("=" * 50)

    for bucket_name, variables in buckets.items():
        print(f"\n{bucket_name.upper()} ({len(variables)} variables):")
        if variables:
            for var in sorted(variables):
                print(f"  - {var}")
        else:
            print("  (None)")

    return buckets

# Apply variable bucketing to your data
variable_buckets = variable_bucketing(df_features)

# ================================================
# FINAL VARIABLE SELECTION FOR MODELING
# ================================================
def create_final_variable_list(variable_buckets):
    """
    Creates final list of variables for modeling by handling splits and super campaigns

    Parameters:
    variable_buckets (dict): Dictionary with variables categorized into buckets

    Returns:
    list: Final list of variables to use in modeling
    """
    print("\n===== FINAL VARIABLE SELECTION =====")

    # Create a copy of the buckets to work with
    final_vars = {}
    for bucket, vars_list in variable_buckets.items():
        final_vars[bucket] = vars_list.copy()

    # Identify split variables and their roots
    split_vars = []
    split_roots = set()

    for var in final_vars['Extra Features']:
        if var.endswith('_pre') or var.endswith('_post'):
            split_vars.append(var)
            # Extract the root variable name (remove _pre or _post)
            root_var = var.rsplit('_', 1)[0]
            split_roots.add(root_var)

    # Remove original variables that were split
    for bucket in ['Media Variables', 'Base Variables', 'Promo Variables']:
        for root_var in split_roots:
            if root_var in final_vars[bucket]:
                print(f"🗑️  Removing original variable '{root_var}' (replaced with split versions)")
                final_vars[bucket].remove(root_var)

    # Check if super campaign was created
    super_campaign_vars = []
    for var in final_vars['Extra Features']:
        if 'Super_Campaign' in var:
            super_campaign_vars.append(var)

    # If super campaign was created, remove the individual media variables that were combined
    if super_campaign_vars:
        print("🔍 Super campaign detected, identifying components...")

        # For each super campaign, we need to know which variables were combined
        # This would typically be stored during feature engineering, but we'll infer from names
        for super_var in super_campaign_vars:
            # Extract the variables that were combined (this is a simplification)
            # In a real scenario, you'd have stored this information during feature engineering
            if 'Super_Campaign' in super_var:
                # This is a simple approach - in practice you'd need to know exactly which variables were combined
                print(f"⚠️  Need to manually specify which variables were combined into '{super_var}'")
                print("Available media variables:", final_vars['Media Variables'])

                # Ask user to specify which variables were combined
                combined_vars_input = input(
                    f"Enter the media variables that were combined into '{super_var}' (comma-separated): "
                )

                if combined_vars_input.strip():
                    combined_vars = [v.strip() for v in combined_vars_input.split(',')]
                    for var_to_remove in combined_vars:
                        if var_to_remove in final_vars['Media Variables']:
                            print(f"🗑️  Removing '{var_to_remove}' (included in super campaign)")
                            final_vars['Media Variables'].remove(var_to_remove)

    # Create the final list of all variables
    all_final_vars = []
    for bucket_vars in final_vars.values():
        all_final_vars.extend(bucket_vars)

    # Remove duplicates
    all_final_vars = list(set(all_final_vars))

    # Display the final variable list
    print("\n📋 FINAL VARIABLES FOR MODELING:")
    print("=" * 50)

    for var in sorted(all_final_vars):
        # Categorize each variable
        if var in final_vars['Media Variables']:
            category = "Media"
        elif var in final_vars['Base Variables']:
            category = "Base"
        elif var in final_vars['Promo Variables']:
            category = "Promo"
        elif var in final_vars['Extra Features']:
            category = "Extra"
        else:
            category = "Other"

        print(f"{var} ({category})")

    print(f"\nTotal variables: {len(all_final_vars)}")

    return all_final_vars

# Create the final variable list
final_variables = create_final_variable_list(variable_buckets)

def preview_final_variables(df, final_variables, target_var='Sales'):
    """
    Display a preview of the final variables and target variable
    """
    print("\n===== FINAL VARIABLES PREVIEW =====")

    # Ensure target variable is included in the preview
    preview_columns = final_variables.copy()
    if target_var not in preview_columns and target_var in df.columns:
        preview_columns.append(target_var)

    # Display the first 10 rows
    print(f"Preview of first 10 rows ({len(preview_columns)} variables):")
    print("=" * 80)

    # Create a preview dataframe
    preview_df = df[preview_columns].head(10).copy()

    # Format numbers to avoid scientific notation
    pd.set_option('display.float_format', '{:.2f}'.format)

    # Display the preview
    display(preview_df)

    # Reset the display option
    pd.reset_option('display.float_format')

    # Show summary statistics
    print("\nSummary Statistics:")
    print("=" * 80)

    # Get summary stats for numeric columns only
    numeric_cols = df[preview_columns].select_dtypes(include=[np.number]).columns.tolist()
    if numeric_cols:
        stats_df = df[numeric_cols].describe().T
        stats_df = stats_df[['count', 'mean', 'std', 'min', 'max']]
        display(stats_df)
    else:
        print("No numeric variables to display statistics for.")

# Preview the final variables
preview_final_variables(df_features, final_variables)

# ================================================
# MODEL PREPROCESSING MODULE
# ================================================
from sklearn.preprocessing import StandardScaler

def model_preprocessing(df, feature_columns, target_column='Sales', test_size=0.2):
    """
    Preprocess data for modeling: scaling and time-based train-test split

    Parameters:
    df (DataFrame): Input dataframe with all variables
    feature_columns (list): List of independent variables
    target_column (str): Name of the target variable (default: 'Sales')
    test_size (float): Proportion of data to use for testing (default: 0.2)

    Returns:
    tuple: X_train_scaled, X_test_scaled, y_train, y_test, scaler
    """
    print("\n===== MODEL PREPROCESSING =====")

    # Separate features and target
    X = df[feature_columns].copy()
    y = df[target_column].copy()

    print(f"Original feature shapes: X={X.shape}, y={y.shape}")

    # Time-based train-test split (not random for time series)
    split_idx = int(len(X) * (1 - test_size))

    X_train = X.iloc[:split_idx]
    X_test = X.iloc[split_idx:]
    y_train = y.iloc[:split_idx]
    y_test = y.iloc[split_idx:]

    print(f"After time-based split:")
    print(f"X_train: {X_train.shape}, X_test: {X_test.shape}")
    print(f"y_train: {y_train.shape}, y_test: {y_test.shape}")

    # Scale the features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Convert back to DataFrames with original column names
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns, index=X_train.index)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns, index=X_test.index)

    print("✅ Features scaled using StandardScaler")
    print("✅ Time-based train-test split completed")

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler

# Apply preprocessing to your data
X_train_scaled, X_test_scaled, y_train, y_test, scaler = model_preprocessing(
    df_features,
    final_variables,
    target_column='Sales',
    test_size=0.2
)

# Display preview of scaled training data
print("\n📊 PREVIEW OF SCALED TRAINING DATA (first 5 rows):")
display(X_train_scaled.head())

print("\n📊 PREVIEW OF TARGET VARIABLE (first 5 rows):")
display(y_train.head())

# ================================================
# COMPLETE MARKETING MIX MODELING PIPELINE
# ================================================
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from statsmodels.stats.outliers_influence import variance_inflation_factor
from IPython.display import display

# First, let's make sure we have the data
try:
    # Check if data is available
    data
except NameError:
    print("❌ ERROR: 'data' is not defined. Please run your data loading code first.")
    # You need to run your data loading code here
    # For example: data = load_and_preprocess_data(file_path)
    raise

# If you've already run feature engineering, use df_features
# Otherwise, we need to run feature engineering first
if 'df_features' not in globals():
    print("⚠️  df_features not found. Running feature engineering...")

    # Run your feature engineering code here
    # This should be the feature_engineering_module function you created earlier
    df_features = feature_engineering_module(data)
else:
    print("✅ df_features found. Proceeding with modeling preparation.")

# Make sure final_variables is defined
if 'final_variables' not in globals():
    print("⚠️  final_variables not found. Creating from df_features columns...")
    # Create final_variables by excluding non-feature columns
    exclude_cols = ['Week_Ending', 'Sales']  # Add any other columns to exclude
    final_variables = [col for col in df_features.columns if col not in exclude_cols]
    print(f"Created final_variables: {final_variables}")

# Now run the complete model preparation
def complete_model_preparation(df, feature_columns, target_column='Sales', test_size=0.2):
    """
    Complete model preparation with preprocessing and multicollinearity analysis
    """
    print("\n===== COMPLETE MODEL PREPARATION =====")

    # 1. Preprocessing
    print("\n1. DATA PREPROCESSING")
    print("=" * 30)

    # Separate features and target
    X = df[feature_columns].copy()
    y = df[target_column].copy()

    # Time-based train-test split
    split_idx = int(len(X) * (1 - test_size))
    X_train = X.iloc[:split_idx]
    X_test = X.iloc[split_idx:]
    y_train = y.iloc[:split_idx]
    y_test = y.iloc[split_idx:]

    # Scale the features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Convert back to DataFrames
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns, index=X_train.index)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns, index=X_test.index)

    print("✅ Preprocessing completed")
    print(f"Training set: {X_train_scaled.shape}")
    print(f"Testing set: {X_test_scaled.shape}")

    # 2. Multicollinearity Analysis
    print("\n2. MULTICOLLINEARITY ANALYSIS")
    print("=" * 30)

    # Calculate VIF for each feature
    vif_data = pd.DataFrame()
    vif_data["Feature"] = X_train_scaled.columns
    vif_data["VIF"] = [variance_inflation_factor(X_train_scaled.values, i)
                       for i in range(len(X_train_scaled.columns))]
    vif_data = vif_data.sort_values("VIF", ascending=False)

    # Display VIF results
    print("VIF Results:")
    display(vif_data)

    # 3. Analyze multicollinearity pattern
    print("\n3. MULTICOLLINEARITY PATTERN ANALYSIS")
    print("=" * 30)

    high_vif_features = vif_data[vif_data["VIF"] > 10]["Feature"].tolist()
    if high_vif_features:
        print(f"High VIF features: {high_vif_features}")
        corr_matrix = X_train_scaled[high_vif_features].corr()
        print("Correlation between high VIF variables:")
        display(corr_matrix)

    # 4. Business context evaluation
    print("\n4. BUSINESS CONTEXT EVALUATION")
    print("=" * 30)
    print("""
The high VIF between split variables is EXPECTED because they are derived from
the same original variable. This is a modeling choice rather than a statistical problem.

Recommended approach for marketing mix modeling:
1. Keep the split variables to capture different effects over time
2. Use regularized regression (ElasticNet) to handle multicollinearity
3. Validate model stability with cross-validation
""")

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, vif_data

# Run the complete preparation process
X_train_scaled, X_test_scaled, y_train, y_test, scaler, vif_results = complete_model_preparation(
    df_features,
    final_variables,
    target_column='Sales',
    test_size=0.2
)

print("\n✅ MODEL PREPARATION COMPLETE")
print("You can now proceed with regularized model building (ElasticNet recommended)")

# ================================================
# MODEL SELECTION (INTERACTIVE) — NO TRAINING HERE
# ================================================
# What this cell does:
# 1) Lets the user choose a model from a dropdown: Ridge, Lasso, ElasticNet
# 2) Asks "why?" via a multi-select checklist + a free-text box
# 3) On confirm, saves a config dict -> MODEL_CHOICE (with default param grids)
# 4) Prints what will be used later. DOES NOT TRAIN.

from datetime import datetime

# Default hyperparameter grids you can tweak later (NOT used here)
_DEFAULT_GRIDS = {
    "ridge":      {"alpha": [0.01, 0.1, 1.0, 10.0, 100.0]},
    "lasso":      {"alpha": [0.0001, 0.001, 0.01, 0.1, 1.0], "max_iter": [10000]},
    "elasticnet": {"alpha": [0.0001, 0.001, 0.01, 0.1, 1.0], "l1_ratio": [0.2, 0.5, 0.8], "max_iter": [10000]},
}

# Short guidance text used in the UI
_GUIDE = {
    "ridge": (
        "Ridge (L2): good when many predictors are correlated and you want to keep them; "
        "shrinks coefficients but rarely to zero."
    ),
    "lasso": (
        "Lasso (L1): useful for feature selection (drives some coefficients to zero); "
        "can be unstable with strong multicollinearity."
    ),
    "elasticnet": (
        "ElasticNet (L1+L2): balances feature selection and stability; "
        "often a safe default for correlated media channels."
    ),
}

# Will be populated after confirmation
MODEL_CHOICE = None   # dict with keys: model, reasons, notes, grid, timestamp

# Try to use ipywidgets UI; if unavailable, fall back to CLI prompts.
try:
    import ipywidgets as widgets
    from IPython.display import display, clear_output

    # --- Widgets ---
    model_dd = widgets.Dropdown(
        options=[("ElasticNet", "elasticnet"), ("Ridge", "ridge"), ("Lasso", "lasso")],
        value="elasticnet",
        description="Model:",
        disabled=False,
    )

    reasons_ms = widgets.SelectMultiple(
        options=[
            "High multicollinearity",
            "Need feature selection",
            "Balance selection & stability",
            "Small dataset",
            "Interpretability matters",
            "Sparse true drivers expected",
            "Reduce overfitting risk",
        ],
        value=("Balance selection & stability",),
        description="Why?",
        rows=6,
        disabled=False,
        layout=widgets.Layout(width="50%"),
    )

    notes_txt = widgets.Textarea(
        placeholder="Write your hypothesis/business reasoning (e.g., 'channels are correlated; want stability + some selection').",
        description="Notes:",
        layout=widgets.Layout(width="90%", height="80px"),
    )

    explain_btn = widgets.Button(description="Explain choice", icon="info")
    confirm_btn = widgets.Button(description="Confirm selection", button_style="success", icon="check")
    out = widgets.Output()

    def _explain_choice(_btn=None):
        with out:
            clear_output(wait=True)
            m = model_dd.value
            print(f"Model selected: {m.title()}")
            print("-" * 60)
            print(_GUIDE[m])
            print("\nDefault grid (editable later):", _DEFAULT_GRIDS[m])

    def _confirm_choice(_btn=None):
        global MODEL_CHOICE
        m = model_dd.value
        MODEL_CHOICE = {
            "model": m,
            "reasons": list(reasons_ms.value),
            "notes": (notes_txt.value or "").strip(),
            "grid": _DEFAULT_GRIDS[m].copy(),
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "ready_to_train": False,   # enforce no training yet
        }
        with out:
            clear_output(wait=True)
            print("✅ Selection stored in MODEL_CHOICE (no model has been fit).")
            print("Summary:")
            print(f"  • Model: {MODEL_CHOICE['model'].title()}")
            print(f"  • Reasons: {MODEL_CHOICE['reasons'] or '(none)'}")
            print(f"  • Notes: {MODEL_CHOICE['notes'] or '(none)'}")
            print(f"  • Param grid: {MODEL_CHOICE['grid']}")
            print("\nNext step suggestion (when you’re ready):")
            print("  → Review/adjust the grid above, then pass MODEL_CHOICE to your training cell.")

    explain_btn.on_click(_explain_choice)
    confirm_btn.on_click(_confirm_choice)

    header = widgets.HTML("<h3>Model Selection — confirm before any training</h3>")
    ui = widgets.VBox([
        header,
        model_dd,
        reasons_ms,
        notes_txt,
        widgets.HBox([explain_btn, confirm_btn]),
        out
    ])
    display(ui)
    _explain_choice()  # show guidance initially

except Exception as _e:
    # ---- CLI Fallback (no widgets) ----
    print("Widgets not available; using simple prompts (no training will run).")
    print("Choose model: [1] ElasticNet  [2] Ridge  [3] Lasso")
    choice = input("Enter 1/2/3: ").strip()
    mapping = {"1": "elasticnet", "2": "ridge", "3": "lasso"}
    m = mapping.get(choice, "elasticnet")
    print("\nSelect reasons (comma-separated numbers):")
    opts = [
        "High multicollinearity",
        "Need feature selection",
        "Balance selection & stability",
        "Small dataset",
        "Interpretability matters",
        "Sparse true drivers expected",
        "Reduce overfitting risk",
    ]
    for i, o in enumerate(opts, 1):
        print(f"{i}. {o}")
    sel = input("Your choices: ").strip()
    picked = []
    if sel:
        for s in sel.split(","):
            s = s.strip()
            if s.isdigit() and 1 <= int(s) <= len(opts):
                picked.append(opts[int(s) - 1])
    notes = input("Notes / hypothesis (optional): ").strip()

    MODEL_CHOICE = {
        "model": m,
        "reasons": picked,
        "notes": notes,
        "grid": _DEFAULT_GRIDS[m].copy(),
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "ready_to_train": False,
    }

    print("\n✅ Selection stored in MODEL_CHOICE (no model has been fit).")
    print("Summary:")
    print(f"  • Model: {MODEL_CHOICE['model'].title()}")
    print(f"  • Reasons: {MODEL_CHOICE['reasons'] or '(none)'}")
    print(f"  • Notes: {MODEL_CHOICE['notes'] or '(none)'}")
    print(f"  • Param grid: {MODEL_CHOICE['grid']}")
    print("\nWhen ready, hand MODEL_CHOICE to your training cell.")

import numpy as np
import pandas as pd
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from statsmodels.stats.stattools import durbin_watson
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

try:
    model_name = MODEL_CHOICE['model'].strip().lower()
    print(f"✅ Model selected: {model_name.title()}")
except Exception:
    raise ValueError("⚠️ Please run the Model Selection cell first!")

dep_var = "Sales"
if 'final_variables' in globals() and final_variables:
    indep_vars = final_variables
    print("Using final_variables for modeling.")
else:
    indep_vars = [
        'Discount1', 'Discount2', 'Total SKU', 'Gasoline Price',
        'Average Price', 'Email Clicks', 'Organic Search Impressions',
        'Modular Video Impressions', 'Paid Social Impressions',
        'Paid Search Impressions'
    ]
    for col in df_features.columns:
        if col not in indep_vars and col not in ['Week_Ending', 'Sales']:
            indep_vars.append(col)
    print("Using columns from df_features (excluding Week_Ending, Sales) as indep_vars.")

indep_vars = [col for col in indep_vars if col in df_features.columns]
if not indep_vars:
    raise ValueError("No independent variables found for modeling.")

X = df_features[indep_vars].fillna(0)
y = df_features[dep_var]
dates = df_features.index

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

split_idx = int(len(X_scaled) * 0.8)
X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]
y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
dates_test = dates[split_idx:]
dates_train = dates[:split_idx]

if model_name == "ridge":
    model = Ridge(alpha=1.0, random_state=42)
elif model_name == "lasso":
    model = Lasso(alpha=0.01, random_state=42)
elif model_name == "elasticnet":
    model = ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42)
else:
    raise ValueError("⚠️ Invalid model choice. Please pick Ridge, Lasso, or ElasticNet.")

model.fit(X_train, y_train)
X_full_scaled = scaler.transform(X)
y_pred_full = model.predict(X_full_scaled)
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

def mape(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true[y_true != 0])) * 100

metrics = {
    "Dataset": ["Train", "Test"],
    "R² (%)": [
        r2_score(y_train, y_train_pred) * 100,
        r2_score(y_test, y_test_pred) * 100
    ],
    "RMSE": [
        f"{np.sqrt(mean_squared_error(y_train, y_train_pred)) :,.0f}",
        f"{np.sqrt(mean_squared_error(y_test, y_test_pred)) :,.0f}"
    ],
    "MAPE (%)": [
        mape(y_train, y_train_pred),
        mape(y_test, y_test_pred)
    ],
    "DW (Residuals)": [
        durbin_watson(y_train - y_train_pred),
        durbin_watson(y_test - y_test_pred)
    ]
}

metrics_df = pd.DataFrame(metrics)
metrics_df[["R² (%)", "MAPE (%)", "DW (Residuals)"]] = metrics_df[["R² (%)", "MAPE (%)", "DW (Residuals)"]].applymap(lambda x: f"{x:.2f}" if isinstance(x, (float, int)) else x)

print("\n📊 MODEL PERFORMANCE METRICS")
print("="*50)
display(metrics_df)

coefs = pd.DataFrame({
    "Variable": indep_vars,
    "Coefficient": model.coef_
}).sort_values(by="Coefficient", key=abs, ascending=False)

print("\n📊 MODEL COEFFICIENTS")
print("="*50)
display(coefs)

fig_coef = px.bar(coefs, x="Coefficient", y="Variable", orientation='h',
                  color="Coefficient", color_continuous_scale="RdBu",
                  title=f"{model_name.title()} Coefficients (Impact on Sales)")
fig_coef.add_vline(x=0, line_dash="dash", line_color="black")
fig_coef.update_layout(yaxis={'categoryorder':'total ascending'})
fig_coef.show()

# Calculate contributions by bucket - FIXED APPROACH
# First, let's check if we have any coefficients that are not zero
print(f"\n🔍 DEBUG: Model coefficients range: {model.coef_.min():.6f} to {model.coef_.max():.6f}")
print(f"🔍 DEBUG: Scaled data range: {X_full_scaled.min():.6f} to {X_full_scaled.max():.6f}")

# Calculate contributions (feature value * coefficient)
contributions_full = pd.DataFrame(X_full_scaled * model.coef_,
                                 columns=indep_vars,
                                 index=dates)

# Map variables to buckets
bucket_mapping = {}
for var in indep_vars:
    var_lower = var.lower()
    if any(kw in var_lower for kw in ['impression', 'click', 'social', 'search', 'email', 'video', 'media', 'campaign', 'ad', 'spend']):
        bucket_mapping[var] = 'media'
    elif any(kw in var_lower for kw in ['discount', 'promo', 'promotion', 'offer']):
        bucket_mapping[var] = 'promo'
    else:
        bucket_mapping[var] = 'base'

print("\n🔍 BUCKET ASSIGNMENTS:")
for var, bucket in bucket_mapping.items():
    print(f"  {var} -> {bucket}")

# Calculate bucket contributions
bucket_contributions = pd.DataFrame(index=dates)
for bucket in ['base', 'media', 'promo']:
    bucket_vars = [var for var in indep_vars if bucket_mapping.get(var) == bucket]
    if bucket_vars:
        bucket_contributions[bucket] = contributions_full[bucket_vars].sum(axis=1)
    else:
        bucket_contributions[bucket] = 0

print(f"\n🔍 BUCKET CONTRIBUTIONS RANGE:")
for bucket in bucket_contributions.columns:
    print(f"  {bucket}: {bucket_contributions[bucket].min():.6f} to {bucket_contributions[bucket].max():.6f}")

# Plot bucket contributions
if not bucket_contributions.empty and not (bucket_contributions == 0).all().all():
    # Convert to millions for better visualization
    bucket_contributions_millions = bucket_contributions / 1e6

    fig_bucket = go.Figure()
    for bucket in bucket_contributions_millions.columns:
        fig_bucket.add_trace(go.Scatter(
            x=bucket_contributions_millions.index,
            y=bucket_contributions_millions[bucket],
            mode='lines',
            name=bucket.capitalize(),
            stackgroup='one'  # This creates the area chart effect
        ))

    fig_bucket.update_layout(
        title="Contribution to Sales by Main Buckets Over Time (in Millions)",
        xaxis_title="Date",
        yaxis_title="Contribution to Sales (Millions)",
        hovermode="x unified"
    )
    fig_bucket.show()

    # Add pie chart for average absolute contribution by bucket
    avg_abs_contribution = bucket_contributions.abs().mean()
    total_avg_abs = avg_abs_contribution.sum()

    if total_avg_abs > 0:
        pie_data = pd.DataFrame({
            'Bucket': avg_abs_contribution.index,
            'Avg_Abs_Contribution': avg_abs_contribution.values,
            'Percentage': (avg_abs_contribution.values / total_avg_abs * 100).round(2)
        })

        fig_pie = px.pie(pie_data,
                         values='Avg_Abs_Contribution',
                         names='Bucket',
                         title='Average Absolute Contribution by Bucket',
                         hover_data=['Percentage'])

        fig_pie.update_traces(textinfo='percent+label')
        fig_pie.show()

        print("\n📊 AVERAGE ABSOLUTE CONTRIBUTION BY BUCKET:")
        for bucket in avg_abs_contribution.index:
            pct = avg_abs_contribution[bucket] / total_avg_abs * 100
            print(f"  {bucket.capitalize()}: {avg_abs_contribution[bucket]:.2f} ({pct:.2f}%)")

else:
    print("⚠️ No meaningful bucket contributions to plot (all zeros)")

# Continue with the rest of the code...
total_abs_contribution = contributions_full.abs().sum().sum()
if total_abs_contribution > 0:
    contrib_summary_full = (
        contributions_full.abs().sum() / total_abs_contribution * 100
    ).sort_values(ascending=False)

    print("\n📊 RELATIVE CONTRIBUTIONS TO SALES (% of Total Absolute Effect) - FULL DATA")
    print("="*70)
    display(contrib_summary_full.apply(lambda x: f"{x:.2f}%"))

    fig_contrib = px.bar(x=contrib_summary_full.index, y=contrib_summary_full.values,
                         title="Relative Contribution by Variable (% of Total Absolute Effect) - Full Data")
    fig_contrib.update_layout(
        xaxis_title="Variable",
        yaxis_title="Contribution (% of Total Absolute Effect)",
        yaxis_ticksuffix="%",
        xaxis_tickangle=-45
    )
    fig_contrib.show()
else:
    print("⚠️ Total absolute contribution is zero. Cannot calculate relative contributions.")

residuals_full = y - y_pred_full
fig_ts = make_subplots(rows=2, cols=1,
                       subplot_titles=("Actual vs Predicted Sales", "Residuals"),
                       shared_xaxes=True,
                       vertical_spacing=0.1)

fig_ts.add_trace(go.Scatter(x=dates, y=y/1e6, mode='lines+markers', name='Actual Sales (Millions)', marker=dict(size=5)),
                 row=1, col=1)
fig_ts.add_trace(go.Scatter(x=dates, y=y_pred_full/1e6, mode='lines+markers', name='Predicted Sales (Millions)', marker=dict(size=5, symbol='x')),
                 row=1, col=1)
fig_ts.add_trace(go.Bar(x=dates, y=residuals_full, name='Residuals', marker=dict(color='grey', opacity=0.7)),
                 row=2, col=1)
fig_ts.add_hline(y=0, line_dash="dash", line_color="red", row=2, col=1)

fig_ts.update_layout(height=700, title_text="Actual vs Predicted Sales with Residuals (Full Data)")
fig_ts.update_yaxes(title_text="Sales (Millions)", tickformat=".1f", row=1, col=1)
fig_ts.update_yaxes(title_text="Residuals", row=2, col=1)
fig_ts.update_xaxes(tickformat="%Y-%m-%d", row=1, col=1)
fig_ts.update_xaxes(tickformat="%Y-%m-%d", row=2, col=1)
fig_ts.show()

print("\n✅ MODEL TRAINING AND OUTPUT GENERATION COMPLETE")

