# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uGSUqWZDUmpoDFXgDwkUBM-SRP0Ny1nf
"""

# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.optimize import minimize
from statsmodels.tsa.seasonal import seasonal_decompose
import io

import matplotlib.pyplot as plt
import seaborn as sns

# Set page configuration
st.set_page_config(
    page_title="Beta MMM App",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state variables
if 'data' not in st.session_state:
    st.session_state.data = None
if 'original_data' not in st.session_state:
    st.session_state.original_data = None
if 'target_var' not in st.session_state:
    st.session_state.target_var = 'Sales'
if 'initial_base_vars' not in st.session_state:
    st.session_state.initial_base_vars = []
if 'initial_media_vars' not in st.session_state:
    st.session_state.initial_media_vars = []
if 'initial_promo_vars' not in st.session_state:
    st.session_state.initial_promo_vars = []
if 'initial_other_vars' not in st.session_state:
    st.session_state.initial_other_vars = []
if 'initial_independent_vars' not in st.session_state:
    st.session_state.initial_independent_vars = []
if 'correlations' not in st.session_state:
    st.session_state.correlations = None
if 'adstocked_media_vars' not in st.session_state:
    st.session_state.adstocked_media_vars = []
if 'media_half_lives' not in st.session_state:
    st.session_state.media_half_lives = {}
if 'saturated_media_vars_created' not in st.session_state:
    st.session_state.saturated_media_vars_created = []
if 'media_gammas_used' not in st.session_state:
    st.session_state.media_gammas_used = {}
if 'media_kappas_used' not in st.session_state:
    st.session_state.media_kappas_used = {}

# --- Helper Functions ---

def format_number(x):
    if pd.isna(x): return ""
    if isinstance(x, (int, float, np.number)):
        if abs(x) >= 1e9: return f'{x/1e9:.2f}B'
        elif abs(x) >= 1e6: return f'{x/1e6:.2f}M'
        elif abs(x) >= 1e3: return f'{x/1e3:.2f}K'
        else: return f'{x:.2f}'
    return str(x)

def convert_indian_number(value):
    if isinstance(value, str):
        cleaned_value = value.replace(',', '').strip()
        if cleaned_value in ['-', ''] or cleaned_value.isspace(): return np.nan
        try: return float(cleaned_value)
        except ValueError: return np.nan
    if isinstance(value, (int, float, np.number)): return value
    return np.nan

def load_and_preprocess_data(uploaded_file):
    if not uploaded_file:
        st.error("No file uploaded.")
        return None

    try:
        if uploaded_file.name.endswith('.csv'):
            data = pd.read_csv(uploaded_file)
        elif uploaded_file.name.endswith(('.xlsx', '.xls')):
            data = pd.read_excel(uploaded_file)
        else:
            st.error("Unsupported file format. Please upload a CSV or Excel file.")
            return None

        if 'Week_Ending' not in data.columns:
            st.error("The uploaded file must contain a 'Week_Ending' column.")
            return None

        try:
            data['Week_Ending'] = pd.to_datetime(data['Week_Ending'])
        except:
            try:
                data['Week_Ending'] = pd.to_datetime(data['Week_Ending'], format='%d-%m-%Y %H:%M')
            except:
                try:
                    data['Week_Ending'] = pd.to_datetime(data['Week_Ending'], format='%m/%d/%Y')
                except:
                    st.error("Could not parse 'Week_Ending' column. Please ensure it's in a recognizable date format.")
                    return None

        object_cols = data.select_dtypes(include=['object']).columns
        for col in object_cols:
            if col != 'Week_Ending':
                data[col] = data[col].apply(convert_indian_number)
                data[col] = pd.to_numeric(data[col], errors='coerce')

        # Store original data before any potential FE
        st.session_state.original_data = data.copy()

        return data
    except Exception as e:
        st.error(f"Error loading file: {str(e)}")
        return None

def get_transformation_recommendations(numeric_df):
    numeric_df = numeric_df.select_dtypes(include=[np.number])
    if numeric_df.empty: return pd.DataFrame(columns=['Variable', 'Skewness', 'Kurtosis', 'Recommendation', 'Color'])

    skewness = numeric_df.skew()
    kurtosis = numeric_df.kurtosis()

    recommendations = []
    for col in numeric_df.columns:
        skew_val = skewness[col]
        kurt_val = kurtosis[col]

        if abs(skew_val) > 1.5:
            recommendation = "Log transformation (high right skew)" if skew_val > 1.5 else "Square root or Box-Cox transformation (high left skew)"
            color = "#ffcccc" # Lighter red
        elif abs(skew_val) > 0.75:
            recommendation = "Consider mild transformation (moderate skew)"
            color = "#ffebcc" # Lighter orange
        else:
            recommendation = "No transformation needed (approximately symmetric)"
            color = "#ccffcc" # Lighter green

        if abs(kurt_val) > 3:
            recommendation += " | Potential outliers detected (high kurtosis)"
            if color == "#ccffcc": # If not already red or orange, make it light orange
                 color = "#ffebcc"

        recommendations.append({
            'Variable': col, 'Skewness': skew_val, 'Kurtosis': kurt_val,
            'Recommendation': recommendation, 'Color': color
        })

    return pd.DataFrame(recommendations)

def perform_comprehensive_eda(data, target_var='Sales'):
    st.subheader("COMPREHENSIVE EXPLORATORY DATA ANALYSIS")

    st.write("#### 1. BASIC DATASET INFORMATION")
    st.write(f"Shape: {data.shape}")
    st.write(f"Columns: {list(data.columns)}")
    if 'Week_Ending' in data.columns:
        st.write(f"Date Range: {data['Week_Ending'].min()} to {data['Week_Ending'].max()}")
    st.write(f"Total Missing Values: {data.isnull().sum().sum()}")

    st.write("#### 1.1 MISSING VALUES")
    missing_values = data.isnull().sum()
    missing_values = missing_values[missing_values > 0].sort_values(ascending=False)
    if missing_values.empty:
        st.write("No missing values found in the dataset.")
    else:
        missing_values_df = missing_values.to_frame(name='Missing Count')
        missing_values_df['Missing Percentage (%)'] = (missing_values_df['Missing Count'] / len(data)) * 100
        st.write("Missing values per column:")
        missing_values_df['Missing Percentage (%)'] = missing_values_df['Missing Percentage (%)'].map('{:.2f}%'.format)
        st.dataframe(missing_values_df)

    numeric_df = data.select_dtypes(include=[np.number])

    st.write("#### 2. SUMMARY STATISTICS")
    formatted_stats = numeric_df.describe().copy()
    st.dataframe(formatted_stats)

    transformation_df = get_transformation_recommendations(numeric_df)
    st.write("#### Transformation Recommendations:")

    # Create a styled dataframe for transformation recommendations
    def color_cells(val):
        if 'high right skew' in val or 'high left skew' in val:
            return 'background-color: #ffcccc'
        elif 'moderate skew' in val:
            return 'background-color: #ffebcc'
        elif 'symmetric' in val:
            return 'background-color: #ccffcc'
        return ''

    styled_df = transformation_df.style.applymap(color_cells, subset=['Recommendation'])
    st.dataframe(styled_df)

    st.write("#### 3. UNIVARIATE ANALYSIS")
    for col in numeric_df.columns:
        if not pd.api.types.is_numeric_dtype(data[col]): continue

        mean_val = data[col].mean()
        median_val = data[col].median()

        fig, ax = plt.subplots(figsize=(10, 6))
        sns.histplot(data[col], kde=True, ax=ax)
        ax.set_title(f"Distribution of {col}")
        ax.set_xlabel(col)
        ax.set_ylabel("Frequency")

        if data[col].max() > 1000:
            ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: format_number(x)))

        ax.axvline(mean_val, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {format_number(mean_val)}')
        ax.axvline(median_val, color='green', linestyle='dashed', linewidth=1, label=f'Median: {format_number(median_val)}')

        ax.legend()
        st.pyplot(fig)

    st.write("#### 4. BIVARIATE ANALYSIS: RELATIONSHIP WITH TARGET VARIABLE")
    if target_var in numeric_df.columns:
        numeric_cols_for_scatter = [col for col in numeric_df.columns if col != target_var]

        for col in numeric_cols_for_scatter:
            if not pd.api.types.is_numeric_dtype(data[col]) or not pd.api.types.is_numeric_dtype(data[target_var]): continue

            correlation = data[col].corr(data[target_var])

            fig, ax = plt.subplots(figsize=(10, 6))
            ax.scatter(data[col], data[target_var])
            ax.set_title(f"{target_var} vs {col} (r = {correlation:.3f})")
            ax.set_xlabel(col)
            ax.set_ylabel(target_var)
            ax.grid(True)

            if data[col].max() > 1000:
                ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: format_number(x)))
            if data[target_var].max() > 1000:
                ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: format_number(y)))

            st.pyplot(fig)

    st.write("#### 5. TIME SERIES ANALYSIS")
    if 'Week_Ending' in data.columns and target_var in data.columns:
        if pd.api.types.is_numeric_dtype(data[target_var]):
            fig, ax = plt.subplots(figsize=(12, 6))
            ax.plot(data['Week_Ending'], data[target_var])
            ax.set_title(f"{target_var} Over Time")
            ax.set_xlabel("Week Ending")
            ax.set_ylabel(target_var)
            ax.grid(True)
            if data[target_var].max() > 1000:
                ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: format_number(y)))
            st.pyplot(fig)

            st.write("Seasonal Decomposition:")
            try:
                temp_df = data.set_index('Week_Ending').sort_index()
                if len(temp_df) >= 2 * 52:
                    decomposition = seasonal_decompose(temp_df[target_var], period=52, model='multiplicative', extrapolate_trend='freq')
                    fig, axes = plt.subplots(4, 1, figsize=(12, 10))
                    axes[0].plot(decomposition.observed)
                    axes[0].set_ylabel("Observed")
                    axes[1].plot(decomposition.trend)
                    axes[1].set_ylabel("Trend")
                    axes[2].plot(decomposition.seasonal)
                    axes[2].set_ylabel("Seasonal")
                    axes[3].plot(decomposition.resid)
                    axes[3].set_ylabel("Residual")
                    fig.suptitle("Seasonal Decomposition (Period 52)", y=0.98)

                    for ax in axes:
                         if ax.get_ylim()[1] > 1000:
                              ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: format_number(y)))

                    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
                    st.pyplot(fig)
                else:
                    st.write(f"Not enough data points ({len(temp_df)}) to perform seasonal decomposition with period 52. Need at least {2*52}.")
            except Exception as e:
                st.write(f"Could not perform seasonal decomposition: {str(e)}")

    st.write("#### 6. CORRELATION ANALYSIS")
    numeric_df_for_corr = data.select_dtypes(include=[np.number])
    if not numeric_df_for_corr.empty:
        corr = numeric_df_for_corr.corr()
        st.session_state.correlations = corr.copy()

        fig, ax = plt.subplots(figsize=(12, 10))
        sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm', ax=ax)
        ax.set_title("Correlation Matrix - All Variables")
        st.pyplot(fig)

        media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video']
        media_cols = [col for col in numeric_df_for_corr.columns if any(keyword in col.lower() for keyword in media_keywords)]

        if media_cols and target_var in numeric_df_for_corr.columns:
            media_corr = numeric_df_for_corr[media_cols + [target_var]].corr()
            target_corr = media_corr[target_var].drop(target_var).sort_values(ascending=False)
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.barplot(x=target_corr.index, y=target_corr.values, palette='viridis', ax=ax)
            ax.set_title(f"Correlation of Media Variables with {target_var}")
            ax.set_xlabel("Media Variables")
            ax.set_ylabel("Correlation Coefficient")
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            st.pyplot(fig)
            st.dataframe(target_corr.round(3).to_frame("Correlation"))

            st.write("#### 6.1 MEDIA EXECUTION SHARE")
            media_totals = numeric_df_for_corr[media_cols].sum()
            total_media = media_totals.sum()
            if total_media > 0:
                media_share = (media_totals / total_media) * 100
                media_share = media_share.sort_values(ascending=False)
                fig, ax = plt.subplots(figsize=(10, 6))
                sns.barplot(x=media_share.index, y=media_share.values, palette='magma', ax=ax)
                ax.set_title("Media Execution Share by Channel")
                ax.set_xlabel("Channel")
                ax.set_ylabel("Share (%)")
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                st.pyplot(fig)

                st.write("Media Execution Share Percentage:")
                media_share_df = media_share.round(2).to_frame("Share (%)")
                media_share_df['Share (%)'] = media_share_df['Share (%)'].apply(lambda x: f'{x:.2f}%')
                st.dataframe(media_share_df)

    st.write("#### 7. OUTLIER ANALYSIS")
    if target_var in numeric_df.columns and pd.api.types.is_numeric_dtype(data[target_var]):
        Q1 = data[target_var].quantile(0.25)
        Q3 = data[target_var].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = data[(data[target_var] < lower_bound) | (data[target_var] > upper_bound)]
        normal_data = data[~((data[target_var] < lower_bound) | (data[target_var] > upper_bound))].copy()
        st.write(f"Number of potential outliers in {target_var}: {len(outliers)}")

        if len(outliers) > 0:
            st.write("Outlier values:")
            st.dataframe(outliers[['Week_Ending', target_var]])
            fig, ax = plt.subplots(figsize=(12, 6))
            ax.plot(normal_data['Week_Ending'], normal_data[target_var], marker='o', linestyle='-', markersize=5, label='Normal Values')
            ax.plot(outliers['Week_Ending'], outliers[target_var], marker='o', linestyle='', color='red', markersize=10, label='Outliers')
            ax.axhline(y=upper_bound, color='red', linestyle='dashed', label=f'Upper Bound: {format_number(upper_bound)}')
            ax.axhline(y=lower_bound, color='red', linestyle='dashed', label=f'Lower Bound: {format_number(lower_bound)}')
            ax.set_title(f"Outlier Detection in {target_var}")
            ax.set_xlabel("Date")
            ax.set_ylabel(target_var)
            ax.legend()
            ax.grid(True)
            if data[target_var].max() > 1000:
                ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: format_number(y)))
            st.pyplot(fig)

    st.success("EDA Complete.")

    return data.copy(), numeric_df.copy()

def bucket_variables(data):
    target_var = 'Sales'
    all_cols = data.columns.tolist()
    independent_vars = [col for col in all_cols if col != target_var and col != 'Week_Ending']

    media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video', 'spend', 'cost', 'budget']
    promo_keywords = ['discount', 'promo']
    base_keywords = ['price', 'macro']

    base_vars = [col for col in independent_vars if any(keyword in col.lower() for keyword in base_keywords)]
    media_vars = [col for col in independent_vars if any(keyword in col.lower() for keyword in media_keywords)]
    promo_vars = [col for col in independent_vars if any(keyword in col.lower() for keyword in promo_keywords)]
    other_vars = [col for col in independent_vars if col not in base_vars + media_vars + promo_vars]

    return target_var, base_vars, media_vars, promo_vars, other_vars, independent_vars

def prepare_data_for_modeling_and_bucket(data):
    st.subheader("DATA PREPARATION FOR MODELING (INITIAL BUCKETING)")

    target_var = 'Sales'
    dependent_var, current_base_vars, current_media_vars, current_promo_vars, current_other_vars, current_independent_vars = bucket_variables(data)

    st.write("--- Variables for Modeling (Tabular - Initial Bucketing) ---")
    variable_list_data = []

    if dependent_var in data.columns:
        variable_list_data.append({'Category': 'Dependent Variable', 'Variable Name': dependent_var})

    for var in current_base_vars:
        variable_list_data.append({'Category': 'Independent Variables (Base & Non-Marketing)', 'Variable Name': var})
    for var in current_media_vars:
        variable_list_data.append({'Category': 'Independent Variables (Media - Original)', 'Variable Name': var})
    for var in current_promo_vars:
        variable_list_data.append({'Category': 'Independent Variables (Promo)', 'Variable Name': var})
    if current_other_vars:
        for var in current_other_vars:
             variable_list_data.append({'Category': 'Independent Variables (Other)', 'Variable Name': var})

    variable_list_df = pd.DataFrame(variable_list_data)
    st.dataframe(variable_list_df)

    st.success("Initial Data Preparation for Modeling Complete.")

    return data.copy(), current_base_vars, current_media_vars, current_promo_vars, current_other_vars, current_independent_vars

def run_analysis_and_eda(uploaded_file):
    if not uploaded_file:
        st.error("No file uploaded. Please upload a file to continue.")
        return

    try:
        # Clear any previous data stored in session state before a new run
        for key in ['data', 'original_data', 'initial_base_vars', 'initial_media_vars',
                   'initial_promo_vars', 'initial_other_vars', 'correlations']:
            if key in st.session_state:
                del st.session_state[key]

        data = load_and_preprocess_data(uploaded_file)
        if data is None:
            return # Exit if no file uploaded

        st.session_state.target_var = 'Sales'
        target_var = st.session_state.target_var

        st.success("--- Data Loading and Preprocessing Complete ---")
        st.write(f"Data shape: {data.shape}")
        st.write(f"Columns: {list(data.columns)}")

        # Perform EDA
        data, numeric_df = perform_comprehensive_eda(data, target_var=target_var)
        st.session_state.data = data

        # Perform initial bucketing with original variables after EDA
        data_for_initial_bucket = data.copy()
        data_for_initial_bucket, initial_base_vars, initial_media_vars, initial_promo_vars, initial_other_vars, initial_independent_vars = prepare_data_for_modeling_and_bucket(data_for_initial_bucket)

        st.session_state.initial_base_vars = initial_base_vars
        st.session_state.initial_promo_vars = initial_promo_vars
        st.session_state.initial_other_vars = initial_other_vars
        st.session_state.initial_media_vars = initial_media_vars
        st.session_state.initial_independent_vars = initial_independent_vars

        st.success("--- Initial Data Preparation for Feature Engineering and Modeling Complete ---")
        st.info("Please proceed to the Feature Engineering section.")

    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        st.error("Please check your file format and try again.")

def generate_seasonality_index(data, date_col, period=52):
    if date_col not in data.columns:
        st.error(f"Date column '{date_col}' not found in data.")
        return data

    temp_df = data.copy().sort_values(date_col)
    if 'Sales' not in temp_df.columns or not pd.api.types.is_numeric_dtype(temp_df['Sales']):
        st.warning(f"Warning: 'Sales' column is not numeric. Cannot generate seasonality index.")
        return data

    seasonality_col = f'seasonality_index_{period}'
    if seasonality_col in temp_df.columns:
        st.info(f"Column '{seasonality_col}' already exists. Skipping creation.")
        return data

    temp_df[seasonality_col] = temp_df['Sales'].rolling(window=period, min_periods=1).mean()
    temp_df[seasonality_col] = temp_df.apply(lambda row: row['Sales'] / row[seasonality_col] if row[seasonality_col] != 0 and pd.notna(row['Sales']) else 1, axis=1)
    temp_df[seasonality_col] = temp_df[seasonality_col].fillna(1)

    st.success(f"Generated seasonality index with period {period}")
    return temp_df

def add_specific_date_dummies(data, date_col, specific_dates):
    if date_col not in data.columns:
        st.error(f"Date column '{date_col}' not found in data.")
        return data

    modified_data = data.copy()

    for date_str in specific_dates:
        try:
            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
            dummy_col = f'dummy_{date_str.replace("-", "_")}'
            if dummy_col in modified_data.columns:
                st.info(f"Column '{dummy_col}' already exists. Skipping creation.")
                continue

            modified_data[dummy_col] = (modified_data[date_col] == date_obj).astype(int)
            st.success(f"Added dummy variable for {date_str}")
        except ValueError:
            st.error(f"Invalid date format: {date_str}. Use YYYY-MM-DD format.")

    return modified_data

def split_media_variable(data, date_col, media_var, split_date_str):
    if date_col not in data.columns or media_var not in data.columns:
        st.error(f"Date column '{date_col}' or media variable '{media_var}' not found in data.")
        return data, None, None, False

    modified_data = data.copy()

    try:
        split_date = datetime.strptime(split_date_str, '%Y-%m-%d')
        pre_col = f"{media_var}_pre_{split_date.strftime('%Y_%m_%d')}"
        post_col = f"{media_var}_post_{split_date.strftime('%Y_%m_%d')}"

        if pre_col in modified_data.columns and post_col in modified_data.columns:
            st.info(f"Split columns '{pre_col}' and '{post_col}' already exist for '{media_var}' at '{split_date_str}'. Skipping creation.")
            return data, pre_col, post_col, False

        if not pd.api.types.is_numeric_dtype(modified_data[media_var]):
            st.warning(f"Warning: Media variable '{media_var}' is not numeric. Cannot split.")
            return data, None, None, False

        modified_data[pre_col] = modified_data.apply(lambda row: row[media_var] if row[date_col] < split_date else 0, axis=1)
        modified_data[post_col] = modified_data.apply(lambda row: row[media_var] if row[date_col] >= split_date else 0, axis=1)

        st.success(f"Split {media_var} into {pre_col} and {post_col} at {split_date_str}")
        return modified_data, pre_col, post_col, True

    except ValueError:
        st.error(f"Invalid date format: {split_date_str}. Use YYYY-MM-DD format.")
        return data, None, None, False

def create_super_campaign_variable(data, component_vars, super_campaign_name):
    missing_vars = [var for var in component_vars if var not in data.columns]
    if missing_vars:
        st.error(f"Component variables not found in data: {missing_vars}")
        return data, None, False

    modified_data = data.copy()

    if super_campaign_name in modified_data.columns:
        st.info(f"Super campaign column '{super_campaign_name}' already exists. Skipping creation.")
        return data, super_campaign_name, False

    non_numeric_components = [var for var in component_vars if not pd.api.types.is_numeric_dtype(modified_data[var])]
    if non_numeric_components:
        st.warning(f"Warning: Non-numeric component variables found: {non_numeric_components}. Cannot create super campaign.")
        return data, None, False

    modified_data[super_campaign_name] = modified_data[component_vars].sum(axis=1)
    st.success(f"Created super campaign variable '{super_campaign_name}' from {component_vars}")
    return modified_data, super_campaign_name, True

def perform_feature_engineering():
    st.subheader("FEATURE ENGINEERING")

    if st.session_state.data is None:
        st.error("No data available. Please run Data Upload and EDA first.")
        return

    # Use session state to hold the data
    fe_data = st.session_state.data.copy()

    # Identify potential media variables for splitting and super campaigns
    media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video', 'spend', 'cost', 'budget']
    potential_media_vars = [col for col in fe_data.columns if any(keyword in col.lower() for keyword in media_keywords) and pd.api.types.is_numeric_dtype(fe_data[col])]
    potential_media_vars.sort()

    # Create feature engineering UI
    st.write("#### Feature Engineering Options")

    # Seasonality Index Creation
    st.write("##### Seasonality Index Creation")
    seasonality_input = st.text_input("Periods (e.g., 52, 26):", key="seasonality_input")

    # Dummy Variable Creation
    st.write("##### Dummy Variable Creation")
    dummy_dates_input = st.text_input("Dates (YYYY-MM-DD, comma separated):", key="dummy_dates_input")

    # Variable Splitting
    st.write("##### Variable Splitting")
    if potential_media_vars:
        media_var_options = {f"{i+1}. {var}": var for i, var in enumerate(potential_media_vars)}
        media_var_choice = st.selectbox("Select variable to split:", list(media_var_options.keys()), key="media_var_split")
        media_var_to_split = media_var_options[media_var_choice] if media_var_choice else None
    else:
        st.info("No media variables available for splitting.")
        media_var_to_split = None

    split_date_input = st.text_input("Split Date (YYYY-MM-DD):", key="split_date_input")

    # Super Campaign Creation
    st.write("##### Super Campaign Creation")
    if potential_media_vars:
        component_vars_selected = st.multiselect("Select component variables:", potential_media_vars, key="component_vars")
    else:
        st.info("No media variables available for super campaign creation.")
        component_vars_selected = []

    super_campaign_name = st.text_input("Super Campaign Name:", key="super_campaign_name")

    # Buttons for actions
    col1, col2 = st.columns(2)
    with col1:
        create_features_btn = st.button("Create Features", type="primary")
    with col2:
        undo_fe_btn = st.button("Undo Feature Engineering", type="secondary")

    if create_features_btn:
        # Process Seasonality
        if seasonality_input:
            try:
                seasonality_periods = [int(p.strip()) for p in seasonality_input.split(',') if p.strip()]
                if all(p > 0 for p in seasonality_periods):
                    for period in seasonality_periods:
                        fe_data = generate_seasonality_index(fe_data, 'Week_Ending', period=period)
                else:
                    st.error("All seasonality periods must be positive integers.")
            except ValueError:
                st.error("Invalid seasonality input. Please enter comma-separated integers.")

        # Process Dummy Dates
        if dummy_dates_input:
            specific_dates = [d.strip() for d in dummy_dates_input.split(',') if d.strip()]
            valid_dates = []
            invalid_dates = []
            for date_str in specific_dates:
                try:
                    datetime.strptime(date_str, '%Y-%m-%d')
                    valid_dates.append(date_str)
                except ValueError:
                    invalid_dates.append(date_str)

            if invalid_dates:
                st.error(f"Invalid date format for: {invalid_dates}. Please use YYYY-MM-DD format.")
            else:
                fe_data = add_specific_date_dummies(fe_data, 'Week_Ending', valid_dates)

        # Process Variable Splitting
        if media_var_to_split and split_date_input:
            fe_data, pre_col, post_col, success = split_media_variable(fe_data, 'Week_Ending', media_var_to_split, split_date_input)

        # Process Super Campaign Creation
        if component_vars_selected and super_campaign_name:
            fe_data, super_campaign_col, success = create_super_campaign_variable(fe_data, component_vars_selected, super_campaign_name)

        # Update the session state data with the modified fe_data
        st.session_state.data = fe_data.copy()

        st.success("Feature Engineering applied successfully!")
        st.write("#### Data Head After Feature Engineering")
        st.dataframe(fe_data.head())
        st.write("#### Columns After Feature Engineering")
        st.write(list(fe_data.columns))

    if undo_fe_btn:
        if 'original_data' in st.session_state and st.session_state.original_data is not None:
            st.session_state.data = st.session_state.original_data.copy()
            st.success("Feature Engineering reverted. Data restored to state before Feature Engineering.")
            st.write("#### Data Head After Undo (Original Data)")
            st.dataframe(st.session_state.data.head())
            st.write("#### Columns After Undo (Original Data)")
            st.write(list(st.session_state.data.columns))
        else:
            st.error("Original data before Feature Engineering not found. Cannot undo.")

def apply_adstock(data_series, decay_rate):
    """Applies geometric adstock transformation."""
    adstocked_series = data_series.copy()
    if not pd.api.types.is_numeric_dtype(adstocked_series):
        st.warning(f"Warning: Data series '{data_series.name}' is not numeric. Cannot apply adstock.")
        return data_series

    adstocked_series = adstocked_series.fillna(0)
    for i in range(1, len(adstocked_series)):
        adstocked_series.iloc[i] = adstocked_series.iloc[i] + adstocked_series.iloc[i-1] * decay_rate
    return adstocked_series

def apply_saturation(data_series, gamma, kappa):
    """Applies Hill saturation function: x^gamma / (x^gamma + kappa^gamma)"""
    if not pd.api.types.is_numeric_dtype(data_series):
        st.warning(f"Warning: Data series is not numeric. Cannot apply saturation.")
        return data_series

    data_series = data_series.replace(0, 1e-9)
    return data_series**gamma / (data_series**gamma + kappa**gamma)

def perform_adstock_transformation():
    st.subheader("ADSTOCK TRANSFORMATION (PAID MEDIA - ADSTOCK ONLY)")

    if st.session_state.data is None:
        st.error("No data available. Please run Data Upload and EDA first.")
        return

    adstock_data = st.session_state.data.copy()

    # Dynamically identify potential media variables from current DataFrame columns
    engineered_media_keywords_in_cols = ['_pre_', '_post_', 'super']
    all_numeric_cols = adstock_data.select_dtypes(include=[np.number]).columns.tolist()

    # Identify engineered media variables present in the current data
    engineered_media_vars = [col for col in all_numeric_cols if any(keyword in col.lower() for keyword in engineered_media_keywords_in_cols)]

    # Identify original media variables present in the current data
    media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video', 'spend', 'cost', 'budget']
    original_media_vars = [col for col in all_numeric_cols if any(keyword in col.lower() for keyword in media_keywords) and col not in engineered_media_vars]

    # Determine which original media variables should be excluded because an engineered version exists
    original_to_exclude = set()
    for orig_var in original_media_vars:
        normalized_orig = orig_var.lower().replace(" ", "")
        if any(normalized_orig in eng_var.lower().replace(" ", "") for eng_var in engineered_media_vars):
             original_to_exclude.add(orig_var)

    # Exclude original media variables that are components of existing super campaign variables
    super_campaign_vars_in_data = [col for col in engineered_media_vars if 'super' in col.lower()]
    component_vars_of_super = set()
    if 'super media' in super_campaign_vars_in_data:
        if 'Modular Video Impressions' in original_media_vars:
             component_vars_of_super.add('Modular Video Impressions')
        if 'Paid Social Impressions' in original_media_vars:
             component_vars_of_super.add('Paid Social Impressions')

    # The final list of media variables for adstock consideration
    media_vars_for_adstock = sorted(list(engineered_media_vars + [var for var in original_media_vars if var not in original_to_exclude and var not in component_vars_of_super]))

    # Filter to ensure these variables exist in the data, are numeric, and have no missing values
    adstock_vars_no_nan = [
        var for var in media_vars_for_adstock
        if var in adstock_data.columns and adstock_data[var].isnull().sum() == 0 and pd.api.types.is_numeric_dtype(adstock_data[var])
    ]

    if not adstock_vars_no_nan:
        st.info("No numeric media variables found without missing values for adstock transformation.")
        return

    st.write("#### Adstock Transformation Setup")
    st.write("Enter a Half-Life value (periods, e.g., 8) for variables you want to adstock. Leave as 0 to skip.")

    # Create input widgets for each variable
    hl_values = {}
    for var in adstock_vars_no_nan:
        hl_values[var] = st.number_input(f'Half-Life for {var}:', min_value=0.0, value=0.0, step=0.5, key=f"hl_{var}")

    col1, col2 = st.columns(2)
    with col1:
        apply_adstock_btn = st.button("Apply Adstock", type="primary")
    with col2:
        undo_adstock_btn = st.button("Undo Adstock", type="secondary")

    if apply_adstock_btn:
        media_half_lives = {}
        adstocked_vars_created_list = []

        # Keep track of variables that were originally in the data before this adstock application
        original_cols_before_adstock = set(adstock_data.columns)

        # Identify columns to potentially drop (existing adstocked columns)
        existing_adstocked_cols = [col for col in adstock_data.columns if '_adstocked_HL' in col]

        # Create a temporary copy of the data to work with for this application round
        temp_adstock_data = adstock_data.copy()

        # Iterate through the variables to apply adstock
        for media_var in adstock_vars_no_nan:
            hl = hl_values[media_var]

            if hl > 0:  # Apply adstock if Half-Life is positive
                # Calculate Retention Rate (RR)
                rr = (0.5)**(1/hl)

                # Create new column name
                hl_name_part = str(hl).replace('.', '').replace('-', 'minus')
                adstocked_col_name = f'{media_var}_adstocked_HL{hl_name_part}'

                # Ensure the media variable exists and is numeric before applying adstock
                if media_var in temp_adstock_data.columns and pd.api.types.is_numeric_dtype(temp_adstock_data[media_var]):
                    # Apply adstock to the variable in the temporary dataframe
                    temp_adstock_data[adstocked_col_name] = apply_adstock(temp_adstock_data[media_var], rr)
                    media_half_lives[adstocked_col_name] = hl
                    adstocked_vars_created_list.append(adstocked_col_name)
                    st.success(f"Applied Adstock to '{media_var}' with HL={hl} -> '{adstocked_col_name}'")

        # Now, merge the temporary dataframe back into the main adstock_data
        # Drop original adstocked columns from the main dataframe before merging new ones
        cols_to_drop_from_main = [col for col in existing_adstocked_cols if col not in adstocked_vars_created_list]
        if cols_to_drop_from_main:
            adstock_data = adstock_data.drop(columns=cols_to_drop_from_main)
            st.info(f"Dropped old adstocked columns: {cols_to_drop_from_main}")

        # Identify new columns created in this run
        newly_created_cols = [col for col in temp_adstock_data.columns if col not in adstock_data.columns]

        # Add newly created adstocked columns from temp_adstock_data to the main adstock_data
        for new_col in newly_created_cols:
            adstock_data[new_col] = temp_adstock_data[new_col]

        # Update the session state data with the modified adstock_data
        st.session_state.data = adstock_data.copy()
        st.session_state.media_half_lives = media_half_lives
        st.session_state.adstocked_media_vars = [col for col in adstock_data.columns if '_adstocked_HL' in col]

        st.success("Adstock Transformation Complete!")
        st.write("#### Data Head After Adstock Transformation")
        st.dataframe(adstock_data.head())

        st.write("#### Adstocked Variables Currently in Data")
        st.write(st.session_state.adstocked_media_vars)

        # Generate Adstock Decay Curves
        st.write("#### Adstock Decay Curves")
        adstocked_vars_for_plotting = st.session_state.adstocked_media_vars

        if not adstocked_vars_for_plotting:
            st.info("No adstocked variables found in the data to plot decay curves.")
        else:
            for adstocked_var_name in adstocked_vars_for_plotting:
                # Try to get the HL from the latest dictionary first
                hl_value = st.session_state.media_half_lives.get(adstocked_var_name)

                # If not found there, try to extract it from the column name as a fallback
                if hl_value is None or hl_value <= 0:
                    try:
                        # Extract HL from column name like 'VarName_adstocked_HL80'
                        hl_str_part = adstocked_var_name.split('_adstocked_HL')[1]
                        # Remove any trailing parts (like _saturated_...) and replace 'minus'
                        hl_str = hl_str_part.split('_saturated_')[0].replace('minus', '-')
                        hl_value = float(hl_str)
                        # Store this extracted value in the dictionary for future plots in this run
                        st.session_state.media_half_lives[adstocked_var_name] = hl_value
                    except (IndexError, ValueError):
                        st.warning(f"Could not extract Half-Life from variable name '{adstocked_var_name}'. Cannot plot decay curve.")
                        continue

                if hl_value is None or hl_value <= 0:
                    st.warning(f"Could not retrieve or extract a valid Half-Life ({hl_value}) for variable '{adstocked_var_name}'. Cannot plot decay curve.")
                    continue

                # Calculate the Retention Rate (RR) from the extracted HL
                rr_for_curve = (0.5)**(1/hl_value)

                # Generate points for the decay curve
                periods = np.arange(0, int(hl_value * 2) + 5)
                decay_values = [rr_for_curve**p for p in periods]

                fig, ax = plt.subplots(figsize=(8, 5))
                ax.plot(periods, decay_values, marker='o', linestyle='-', label='Adstock Decay')

                # Add reference points
                if rr_for_curve > 0 and rr_for_curve < 1:
                    estimated_hl_from_rr = hl_value

                    # Plot 50% effect point if within plot range
                    if estimated_hl_from_rr >= periods.min() and estimated_hl_from_rr <= periods.max():
                        ax.plot(estimated_hl_from_rr, 0.5, marker='o', color='green', markersize=10, label=f'50% Effect (HL)')
                        ax.axvline(x=estimated_hl_from_rr, color='green', linestyle='dashed', label=f"HL ({estimated_hl_from_rr:.2f} periods)")

                # Extract original raw name from adstocked variable name for title
                original_raw_name = adstocked_var_name.split('_adstocked_HL')[0] if '_adstocked_HL' in adstocked_var_name else adstocked_var_name.split('_saturated_')[0]

                ax.set_title(f"Adstock Decay Curve for {original_raw_name} (HL={hl_value:.2f}, RR={rr_for_curve:.2%})")
                ax.set_xlabel("Periods (Weeks) Since Impulse")
                ax.set_ylabel("Decay Factor (Effect Remaining)")
                ax.set_ylim(0, 1.1)
                ax.grid(True)
                ax.legend()
                st.pyplot(fig)

        st.success("Adstock Decay Curve Generation Complete.")

    if undo_adstock_btn:
        if st.session_state.data is not None:
            # Find all adstocked columns currently in the data
            adstocked_cols_to_drop = [col for col in st.session_state.data.columns if '_adstocked_HL' in col]
            if adstocked_cols_to_drop:
                st.session_state.data = st.session_state.data.drop(columns=adstocked_cols_to_drop)
                st.session_state.adstocked_media_vars = []
                st.session_state.media_half_lives = {}
                st.success("Adstock Transformation reverted. Adstocked columns removed.")
                st.write("#### Data Head After Undo Adstock")
                st.dataframe(st.session_state.data.head())
            else:
                st.info("No adstocked columns found to undo.")
        else:
            st.error("Data before Adstock not found. Cannot undo.")

def perform_saturation_transformation():
    st.subheader("APPLYING SATURATION & PLOTTING SATURATION CURVES")

    if 'adstocked_media_vars' not in st.session_state or not st.session_state.adstocked_media_vars:
        st.info("No adstocked variables found from the previous Adstock Transformation step.")
        st.info("Saturation transformation and plotting skipped.")
        st.session_state.saturated_media_vars_created = []
        st.session_state.media_gammas_used = {}
        st.session_state.media_kappas_used = {}
        return

    saturation_data = st.session_state.data.copy() if st.session_state.data is not None else None

    if saturation_data is None:
        st.error("No data available. Please run Data Upload, EDA, Feature Engineering, Adstock, and Saturation steps first.")
        return

    adstocked_vars_for_saturation = [var for var in st.session_state.adstocked_media_vars if var in saturation_data.columns and pd.api.types.is_numeric_dtype(saturation_data[var])]

    if not adstocked_vars_for_saturation:
        st.info("No numeric adstocked variables found in the DataFrame for saturation.")
        st.info("Saturation transformation and plotting skipped.")
        st.session_state.saturated_media_vars_created = []
        st.session_state.media_gammas_used = {}
        st.session_state.media_kappas_used = {}
        return

    st.write("#### Found the following adstocked variables for saturation:")
    for var in adstocked_vars_for_saturation:
        st.write(f"- {var}")

    st.write("#### Saturation Transformation Setup")
    st.write("Enter Gamma and Kappa values for variables you want to saturate. Leave default values to use guidance or modify as needed.")
    st.write("##### Guidance for gamma values:")
    st.write("- High gamma (2.0-5.0): C-Shaped curve - Good for direct response channels")
    st.write("- Medium gamma (1.0-2.0): Moderate S-Shape - Good for social/short-consideration")
    st.write("- Low gamma (0.1-1.0): S-Shaped curve - Good for brand building channels")
    st.write("##### Guidance for kappa values:")
    st.write("- Kappa influences the inflection point of the curve.")
    st.write("- A good starting point for kappa is the average value of the adstocked variable.")

    # Create input widgets for each variable
    gamma_values = {}
    kappa_values = {}

    for var in adstocked_vars_for_saturation:
        channel_name = var.split('_adstocked_')[0]

        default_gamma = 1.0
        if any(x in channel_name.lower() for x in ['search', 'click', 'direct']):
            default_gamma = 3.0
        elif any(x in channel_name.lower() for x in ['social', 'email', 'digital']):
            default_gamma = 1.5
        elif any(x in channel_name.lower() for x in ['video', 'tv', 'brand', 'display']):
            default_gamma = 0.7

        # Set the specified default kappa value
        default_kappa = 2080261.732665588

        col1, col2 = st.columns(2)
        with col1:
            gamma_values[var] = st.number_input(f'Gamma for {var}:', min_value=0.1, value=default_gamma, step=0.1, key=f"gamma_{var}")
        with col2:
            kappa_values[var] = st.number_input(f'Kappa for {var}:', min_value=0.0, value=default_kappa, step=1000.0, key=f"kappa_{var}")

    col1, col2 = st.columns(2)
    with col1:
        apply_saturation_btn = st.button("Apply Saturation & Plot", type="primary")
    with col2:
        undo_saturation_btn = st.button("Undo Saturation", type="secondary")

    if apply_saturation_btn:
        media_gammas_used = {}
        media_kappas_used = {}
        saturated_media_vars_created_local = []

        for adstocked_var in adstocked_vars_for_saturation:
            gamma = gamma_values[adstocked_var]
            kappa = kappa_values[adstocked_var]

            if gamma > 0 and kappa >= 0:
                gamma_name_part = str(gamma).replace('.', '').replace('-', 'minus')
                kappa_name_part = str(kappa).replace('.', '').replace('-', 'minus')
                saturated_col_name = f'{adstocked_var}_saturated_gamma{gamma_name_part}_kappa{kappa_name_part}'

                if adstocked_var in saturation_data.columns and pd.api.types.is_numeric_dtype(saturation_data[adstocked_var]):
                    if saturated_col_name in saturation_data.columns:
                        saturation_data = saturation_data.drop(columns=[saturated_col_name])

                    saturation_data[saturated_col_name] = apply_saturation(saturation_data[adstocked_var], gamma, kappa)
                    saturated_media_vars_created_local.append(saturated_col_name)
                    media_gammas_used[saturated_col_name] = gamma
                    media_kappas_used[saturated_col_name] = kappa
                    st.success(f"Applied Saturation to '{adstocked_var}' with gamma={gamma}, kappa={kappa} -> '{saturated_col_name}'")

        st.session_state.data = saturation_data.copy()
        st.session_state.saturated_media_vars_created = saturated_media_vars_created_local
        st.session_state.media_gammas_used = media_gammas_used
        st.session_state.media_kappas_used = media_kappas_used

        st.success("Saturation Transformation Complete!")
        st.write("#### Data Head After Saturation Transformation")
        st.dataframe(saturation_data.head())

        st.write("#### Saturated Variables Created")
        display_names = [f"{var.split('_adstocked_')[0] if '_adstocked_' in var else var} (saturated)" for var in saturated_media_vars_created_local]
        st.write(display_names)

        # Plot saturation curves
        st.write("#### Saturation Curves")
        if not saturated_media_vars_created_local:
            st.info("No saturated variables found in the data to plot saturation curves.")
        else:
            for saturated_var_name in saturated_media_vars_created_local:
                try:
                    gamma = st.session_state.media_gammas_used.get(saturated_var_name)
                    kappa = st.session_state.media_kappas_used.get(saturated_var_name)

                    if gamma is None or kappa is None or gamma <= 0 or kappa < 0:
                        st.warning(f"Could not retrieve valid gamma ({gamma}) or kappa ({kappa}) for variable '{saturated_var_name}'. Cannot plot saturation curve.")
                        continue

                    if gamma > 0:
                        kappa_plot = max(kappa, 1e-9)
                        try:
                            x_at_99_saturation = (0.99 / (1 - 0.99))**(1/gamma) * kappa_plot
                            if np.isinf(x_at_99_saturation) or x_at_99_saturation > 1e15:
                                x_at_99_saturation = kappa_plot * 100
                        except (OverflowError, ZeroDivisionError):
                            x_at_99_saturation = 5 * kappa_plot + 100

                        adstocked_var_original_name = saturated_var_name.split('_saturated_')[0]
                        max_actual_adstocked_value = saturation_data[adstocked_var_original_name].max() if adstocked_var_original_name in saturation_data.columns else x_at_99_saturation * 2
                        max_input_range = min(max(x_at_99_saturation * 1.5, kappa_plot * 5), max_actual_adstocked_value * 2)
                    else:
                        adstocked_var_original_name = saturated_var_name.split('_saturated_')[0]
                        max_input_range = saturation_data[adstocked_var_original_name].max() * 1.2 if adstocked_var_original_name in saturation_data.columns else 100

                    max_input_range = max(max_input_range, 1.0)
                    input_values_range = np.linspace(0, max_input_range, 200)
                    kappa_plot = max(kappa, 1e-9)
                    saturated_values_range = apply_saturation(pd.Series(input_values_range), gamma, kappa)

                    fig, ax = plt.subplots(figsize=(8, 5))
                    ax.plot(input_values_range, saturated_values_range, label='Saturation Curve')

                    if gamma > 0 and kappa >= 0:
                        x_50 = kappa
                        if x_50 >= 0 and x_50 <= max_input_range:
                            y_50 = apply_saturation(pd.Series([x_50]), gamma, kappa).iloc[0]
                            ax.plot(x_50, y_50, marker='o', color='green', markersize=10, label='50% Saturation (x=Kappa)')
                            ax.axvline(x=x_50, color='green', linestyle='dashed', label=f"50% Saturation (Input: {format_number(x_50)})")

                    if gamma > 0 and kappa >= 0:
                        try:
                            x_80 = (4**(1/gamma)) * kappa
                            if np.isinf(x_80) or x_80 > 1e15:
                                x_80 = kappa_plot * 50
                        except OverflowError:
                            x_80 = kappa_plot * 50

                        if x_80 >= 0 and x_80 <= max_input_range:
                            y_80 = apply_saturation(pd.Series([x_80]), gamma, kappa).iloc[0]
                            ax.plot(x_80, y_80, marker='o', color='red', markersize=10, label='80% Saturation')
                            ax.axvline(x=x_80, color='red', linestyle='dashed', label=f"80% Saturation (Input: {format_number(x_80)})")

                    parts = saturated_var_name.split('_adstocked_HL')
                    original_raw_name = parts[0] if parts else saturated_var_name.split('_saturated_')[0]

                    ax.set_title(f"Saturation Curve for {original_raw_name} (Gamma={gamma}, Kappa={kappa})")
                    ax.set_xlabel("Adstocked Input Value")
                    ax.set_ylabel("Saturated Output Value")
                    ax.set_ylim(0, 1.1)
                    ax.grid(True)
                    ax.legend()
                    st.pyplot(fig)

                except Exception as e:
                    st.error(f"Error generating plot for {saturated_var_name}: {e}")

        st.info("Please proceed to the next step: Prepare Data for Modeling")

    if undo_saturation_btn:
        if st.session_state.data is not None:
            saturated_cols_to_drop = [col for col in st.session_state.data.columns if '_saturated_gamma' in col and '_kappa' in col]
            if saturated_cols_to_drop:
                st.session_state.data = st.session_state.data.drop(columns=saturated_cols_to_drop)
                st.session_state.saturated_media_vars_created = []
                st.session_state.media_gammas_used = {}
                st.session_state.media_kappas_used = {}
                st.success("Saturation Transformation reverted. Saturated columns removed.")
                st.write("#### Data Head After Undo Saturation")
                st.dataframe(st.session_state.data.head())
            else:
                st.info("No saturated columns found to undo.")
        else:
            st.error("Data before Saturation not found. Cannot undo.")

# Main app
def main():
    st.title("Beta Marketing Mix Modeling (MMM) App")
    st.sidebar.title("Navigation")

    # Create navigation
    pages = {
        "Data Upload & EDA": page_data_upload_eda,
        "Feature Engineering": page_feature_engineering,
        "Adstock Transformation": page_adstock_transformation,
        "Saturation Transformation": page_saturation_transformation
    }

    selection = st.sidebar.radio("Go to", list(pages.keys()))
    page = pages[selection]
    page()

def page_data_upload_eda():
    st.header("Data Upload and Exploratory Data Analysis")
    st.write("Please upload your data file (CSV or Excel format):")

    uploaded_file = st.file_uploader("Choose a file", type=['csv', 'xlsx', 'xls'])

    if st.button("Run EDA", type="primary"):
        if uploaded_file is not None:
            run_analysis_and_eda(uploaded_file)
        else:
            st.error("Please upload a file first.")

def page_feature_engineering():
    st.header("Feature Engineering")
    st.write("Configure and apply feature engineering transformations.")

    if st.button("Setup Feature Engineering", type="primary"):
        perform_feature_engineering()

def page_adstock_transformation():
    st.header("Adstock Transformation")
    perform_adstock_transformation()

def page_saturation_transformation():
    st.header("Saturation Transformation")
    perform_saturation_transformation()

if __name__ == "__main__":
    main()