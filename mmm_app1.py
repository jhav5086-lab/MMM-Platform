# -*- coding: utf-8 -*-
"""mmm_app1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JXtjSTLnGAzWSJUplvugjFZGjZZGL6iV
"""

# mmm_app.py
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Modeling & stats
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Plotting
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Helper functions (ported / adapted from beta_mmm.py)
def format_number(x):
    if pd.isna(x): return ""
    try:
        x = float(x)
    except:
        return str(x)
    if abs(x) >= 1e9: return f'{x/1e9:.2f}B'
    elif abs(x) >= 1e6: return f'{x/1e6:.2f}M'
    elif abs(x) >= 1e3: return f'{x/1e3:.2f}K'
    else: return f'{x:.2f}'

def convert_indian_number(value):
    if isinstance(value, str):
        cleaned_value = value.replace(',', '').strip()
        if cleaned_value in ['-', ''] or cleaned_value.isspace(): return np.nan
        try: return float(cleaned_value)
        except ValueError: return np.nan
    if isinstance(value, (int, float, np.number)): return value
    return np.nan

def load_and_preprocess_data(file):
    # Accepts a file-like object (uploaded)
    df = pd.read_csv(file)
    # Attempt robust date parsing for Week_Ending
    if 'Week_Ending' in df.columns:
        try:
            df['Week_Ending'] = pd.to_datetime(df['Week_Ending'], errors='coerce', dayfirst=True)
        except:
            df['Week_Ending'] = pd.to_datetime(df['Week_Ending'], errors='coerce')
    # Convert object columns to numeric where possible using convert_indian_number
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].apply(convert_indian_number)
        df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

def apply_adstock(series, alpha):
    # alpha is RR (retention rate between 0 and 1)
    adstocked = series.copy().astype(float)
    for i in range(1, len(series)):
        adstocked.iloc[i] = series.iloc[i] + alpha * adstocked.iloc[i-1]
    return adstocked

def apply_saturation(series, gamma):
    # stable hill-like function in [0,1] * preserve units variant = sat_frac * x
    x = np.asarray(series, dtype=float)
    gamma = float(gamma) if gamma is not None else 1.0
    sat_frac = x / (x + gamma + 1e-9)
    return sat_frac * x

def get_transformation_recommendations(numeric_df):
    numeric_df = numeric_df.select_dtypes(include=[np.number])
    if numeric_df.empty: return pd.DataFrame(columns=['Variable', 'Skewness', 'Kurtosis', 'Recommendation', 'Color'])
    skewness = numeric_df.skew()
    kurtosis = numeric_df.kurtosis()
    recs = []
    for col in numeric_df.columns:
        s = skewness[col]; k = kurtosis[col]
        if abs(s) > 1.5:
            recommendation = "Log transformation (high right skew)" if s > 1.5 else "Square root or Box-Cox (left skew)"
            color = "red"
        elif abs(s) > 0.75:
            recommendation = "Consider mild transformation (moderate skew)"
            color = "orange"
        else:
            recommendation = "No transformation needed"
            color = "green"
        if abs(k) > 3:
            recommendation += " | Potential outliers (high kurtosis)"
            if color != "red": color = "orange"
        recs.append({'Variable': col, 'Skewness': round(s,3), 'Kurtosis': round(k,3), 'Recommendation': recommendation, 'Color': color})
    return pd.DataFrame(recs)

def generate_seasonality_index(df, date_col='Week_Ending', period=52):
    temp = df.copy().sort_values(date_col)
    if 'Sales' not in temp.columns or not pd.api.types.is_numeric_dtype(temp['Sales']):
        st.warning("Sales column missing or non-numeric; cannot create seasonality index.")
        return df
    colname = f'seasonality_index_{period}'
    temp[colname] = temp['Sales'].rolling(window=period, min_periods=1).mean()
    temp[colname] = temp.apply(lambda r: r['Sales']/r[colname] if r[colname] != 0 and pd.notna(r['Sales']) else 1, axis=1)
    temp[colname] = temp[colname].fillna(1)
    st.success(f"Created {colname}")
    return temp

def add_specific_date_dummies(df, date_col='Week_Ending', dates_list=[]):
    for d in dates_list:
        try:
            date_obj = pd.to_datetime(d)
            col = f'dummy_{date_obj.strftime("%Y_%m_%d")}'
            df[col] = (df[date_col] == date_obj).astype(int)
        except Exception as e:
            st.warning(f"Could not parse date {d}: {e}")
    return df

def split_media_variable(data, date_col, media_var, split_date_str):
    if date_col not in data.columns or media_var not in data.columns:
        st.warning(f"Date column or media variable not found: {date_col}, {media_var}")
        return data, None, None, False
    try:
        split_date = pd.to_datetime(split_date_str)
    except:
        st.warning("Invalid split date format; use YYYY-MM-DD")
        return data, None, None, False
    pre_col = f"{media_var}_pre_{split_date.strftime('%Y_%m_%d')}"
    post_col = f"{media_var}_post_{split_date.strftime('%Y_%m_%d')}"
    if not pd.api.types.is_numeric_dtype(data[media_var]):
        st.warning(f"Media var {media_var} not numeric; cannot split")
        return data, None, None, False
    data[pre_col] = data.apply(lambda r: r[media_var] if r[date_col] < split_date else 0, axis=1)
    data[post_col] = data.apply(lambda r: r[media_var] if r[date_col] >= split_date else 0, axis=1)
    st.success(f"Split {media_var} into {pre_col} and {post_col}")
    return data, pre_col, post_col, True

def create_super_campaign_variable(data, component_vars, super_campaign_name):
    missing = [v for v in component_vars if v not in data.columns]
    if missing:
        st.warning(f"Missing components: {missing}")
        return data, None, False
    non_numeric = [v for v in component_vars if not pd.api.types.is_numeric_dtype(data[v])]
    if non_numeric:
        st.warning(f"Non-numeric components: {non_numeric}")
        return data, None, False
    data[super_campaign_name] = data[component_vars].sum(axis=1)
    st.success(f"Created super campaign {super_campaign_name}")
    return data, super_campaign_name, True

def calculate_estimated_weekly_spend(data, media_costs_dict, fy_ranges_dict):
    estimated_weekly_spend = {}
    spend_calculation_vars = [col for col in data.columns if any(keyword in col.lower() for keyword in ['email clicks', 'paid social impressions', 'paid search impressions', 'modular video impressions', 'organic search impressions', '_pre_', '_post_', 'super_campaign'])]
    for channel in spend_calculation_vars:
        weekly_spend = []
        original_channel_for_cost = channel
        for idx, row in data.iterrows():
            week_ending = row.get('Week_Ending', None)
            activity = row.get(channel, 0)
            current_fy = None
            for fy, date_range in fy_ranges_dict.items():
                try:
                    start = pd.to_datetime(date_range[0]); end = pd.to_datetime(date_range[1])
                    if week_ending is not pd.NaT and start <= week_ending <= end:
                        current_fy = fy
                        break
                except: pass
            # find cost info
            cost_type, cost_value = None, 0
            # try direct key
            if current_fy and channel in media_costs_dict and current_fy in media_costs_dict[channel]:
                info = media_costs_dict[channel][current_fy]
                cost_type, cost_value = info['type'], info['value']
            else:
                # try to match original channel keys ignoring _pre_/post_ suffixes
                key_found = None
                for key in media_costs_dict.keys():
                    if key.lower() in channel.lower():
                        key_found = key
                        break
                if key_found and current_fy and current_fy in media_costs_dict[key_found]:
                    info = media_costs_dict[key_found][current_fy]
                    cost_type, cost_value = info['type'], info['value']
                elif 'organic search impressions' in channel.lower():
                    cost_type, cost_value = 'CPM', 0
            if cost_type == 'CPM':
                spend = (activity/1000) * cost_value if cost_value else 0
            elif cost_type == 'CPC':
                spend = activity * cost_value if cost_value else 0
            else:
                spend = 0
            weekly_spend.append(spend)
        estimated_weekly_spend[channel] = weekly_spend
    # handle super-campaign spend as sum of components if present
    super_campaign_cols = [c for c in estimated_weekly_spend.keys() if 'super_campaign' in c.lower()]
    if super_campaign_cols:
        comp_cols = [c for c in estimated_weekly_spend.keys() if 'super_campaign' not in c.lower()]
        for s in super_campaign_cols:
            total = [0]*len(data)
            for comp in comp_cols:
                total = [a+b for a,b in zip(total, estimated_weekly_spend[comp])]
            estimated_weekly_spend[s] = total
    return estimated_weekly_spend

def create_media_spend_report(data, estimated_weekly_spend_dict, fy_ranges_dict):
    # sum spend per FY and produce YoY changes, execution sums
    estimated_fy_spend = {}
    fy_execution_data = {}
    for channel, weekly in estimated_weekly_spend_dict.items():
        temp = pd.DataFrame({'Week_Ending': data['Week_Ending'], 'Spend': weekly})
        estimated_fy_spend[channel] = {}
        for fy, rng in fy_ranges_dict.items():
            try:
                start = pd.to_datetime(rng[0]); end = pd.to_datetime(rng[1])
                s = temp[(temp['Week_Ending'] >= start) & (temp['Week_Ending'] <= end)]['Spend'].sum()
            except:
                s = temp['Spend'].sum()
            estimated_fy_spend[channel][fy] = s
    # execution
    media_cols = [col for col in data.columns if any(k in col.lower() for k in ['impressions','clicks','social','search','email','video','super_campaign'])]
    for channel in media_cols:
        temp = pd.DataFrame({'Week_Ending': data['Week_Ending'], 'Execution': data[channel] if pd.api.types.is_numeric_dtype(data[channel]) else 0})
        fy_execution_data[channel] = {}
        for fy, rng in fy_ranges_dict.items():
            try:
                start = pd.to_datetime(rng[0]); end = pd.to_datetime(rng[1])
                v = temp[(temp['Week_Ending'] >= start) & (temp['Week_Ending'] <= end)]['Execution'].sum()
            except:
                v = temp['Execution'].sum()
            fy_execution_data[channel][fy] = v
    # Build report DataFrame
    rows = []
    fys = list(fy_ranges_dict.keys())
    for channel in sorted(set(list(estimated_fy_spend.keys()) + list(fy_execution_data.keys()))):
        row = {'Channel': channel}
        for fy in fys:
            row[f'{fy} Spend (Est)'] = estimated_fy_spend.get(channel, {}).get(fy, 0)
            row[f'{fy} Execution'] = fy_execution_data.get(channel, {}).get(fy, 0)
        # YoY spend change for each adjacent period
        for i in range(1,len(fys)):
            p = f'{fys[i-1]} to {fys[i]}'
            prev = row[f'{fys[i-1]} Spend (Est)']; cur = row[f'{fys[i]} Spend (Est)']
            row[f'{p} YoY Spend Change (%)'] = ((cur - prev)/prev*100) if prev!=0 else (float('inf') if cur>0 else 0)
            # Execution YoY
            prev_exec = row[f'{fys[i-1]} Execution']; cur_exec = row[f'{fys[i]} Execution']
            row[f'{p} YoY Execution Change (%)'] = ((cur_exec - prev_exec)/prev_exec*100) if prev_exec!=0 else (float('inf') if cur_exec>0 else 0)
        rows.append(row)
    df_report = pd.DataFrame(rows)
    # format numbers in display
    display_df = df_report.copy()
    for c in display_df.columns:
        if 'Spend (Est)' in c or 'Execution' in c:
            display_df[c] = display_df[c].apply(format_number)
        if 'YoY' in c:
            display_df[c] = display_df[c].apply(lambda x: "Infinite" if x==float('inf') else (f"{x:.2f}%" if pd.notna(x) else x))
    return display_df, df_report

def bucket_variables(df):
    promo_keywords = ['discount','promo']
    price_keywords = ['price']
    distribution_keywords = ['sku']
    media_keywords = ['impressions','clicks','social','search','email','video']
    # transformed media (adstocked + saturated)
    media_vars_transformed = [col for col in df.columns if '_adstocked_' in col.lower() and '_saturated_' in col.lower()]
    excluded = ['Sales','Week_Ending'] + media_vars_transformed
    promo_vars=[]; price_vars=[]; distribution=[]; seasonality=[]; other=[]
    for col in df.columns:
        if col in excluded: continue
        lower=col.lower()
        if any(k in lower for k in promo_keywords): promo_vars.append(col)
        elif any(k in lower for k in price_keywords): price_vars.append(col)
        elif any(k in lower for k in distribution_keywords): distribution.append(col)
        elif 'seasonality_index' in lower or 'dummy_' in lower: seasonality.append(col)
        elif any(k in lower for k in media_keywords):
            # these may be original media columns
            other.append(col)
        else:
            other.append(col)
    dependent_var='Sales'
    independent = seasonality + promo_vars + media_vars_transformed + other
    return dependent_var, seasonality, media_vars_transformed, promo_vars, other, independent

# ---------------------------
# STREAMLIT App layout
# ---------------------------
st.set_page_config(layout="wide", page_title="MMM App (Streamlit port of Beta_MMM)")

st.title("Marketing Mix Modeling - Interactive App (Port of Beta_MMM)")
st.markdown("This app replicates the notebook workflow: load data → EDA → feature engineering → adstock & saturation → spend calculations → modeling & response curves.")

# Sidebar: Upload
st.sidebar.header("1) Upload Data")
uploaded_file = st.sidebar.file_uploader("Upload CSV file (Book1.csv or similar)", type=['csv'])
if uploaded_file is None:
    st.info("Please upload a CSV file to begin. Example file expected to have 'Week_Ending' and 'Sales' columns.")
    st.stop()

# Load data
data = load_and_preprocess_data(uploaded_file)
st.sidebar.success("File loaded")
st.subheader("Data sample")
st.dataframe(data.head())

# Basic EDA panel
st.header("Exploratory Data Analysis (EDA)")

col1, col2 = st.columns([1,1])
with col1:
    st.subheader("Summary Statistics")
    numeric_df = data.select_dtypes(include=[np.number])
    if numeric_df.empty:
        st.warning("No numeric columns found.")
    else:
        desc = numeric_df.describe().T
        st.dataframe(desc)

with col2:
    st.subheader("Missing Values")
    missing = data.isnull().sum()
    missing = missing[missing>0].sort_values(ascending=False)
    if missing.empty:
        st.write("No missing values.")
    else:
        st.dataframe(missing.to_frame("Missing Count"))

# Distribution recommendations
st.subheader("Transformation Recommendations")
trans_recs = get_transformation_recommendations(numeric_df)
st.dataframe(trans_recs)

# Time series plot for Sales
if 'Week_Ending' in data.columns and 'Sales' in data.columns:
    st.subheader("Sales Over Time")
    fig = px.line(data.sort_values('Week_Ending'), x='Week_Ending', y='Sales', title='Sales Over Time')
    st.plotly_chart(fig, use_container_width=True)
else:
    st.write("Week_Ending or Sales missing; skipping time series plot.")

# Feature Engineering section
st.header("Feature Engineering")
with st.expander("Seasonality Index"):
    periods_input = st.text_input("Enter comma-separated seasonality periods (e.g., 52,26) or leave blank", value="52")
    if st.button("Generate Seasonality Index"):
        try:
            periods = [int(p.strip()) for p in periods_input.split(',') if p.strip()]
            for p in periods:
                data = generate_seasonality_index(data, 'Week_Ending', period=p)
            st.success("Seasonality index(es) created.")
        except Exception as e:
            st.error(f"Error: {e}")

with st.expander("Add Date Dummies"):
    dates_str = st.text_input("Enter comma-separated dates (YYYY-MM-DD) to create dummy variables", "")
    if st.button("Add date dummies"):
        dates = [d.strip() for d in dates_str.split(',') if d.strip()]
        data = add_specific_date_dummies(data, 'Week_Ending', dates)
        st.success("Dummies added (if valid).")

with st.expander("Split Media Variable"):
    media_var_to_split = st.selectbox("Select media variable to split", options=[c for c in data.columns if c not in ['Week_Ending','Sales']], index=0)
    split_date = st.text_input("Split date (YYYY-MM-DD)", "")
    if st.button("Split variable"):
        if split_date:
            data, pre, post, ok = split_media_variable(data, 'Week_Ending', media_var_to_split, split_date)
            if ok:
                st.success(f"Split done: {pre}, {post}")
        else:
            st.warning("Enter split date.")

with st.expander("Create Super Campaign"):
    comps = st.multiselect("Select component variables (numeric)", options=[c for c in data.columns if pd.api.types.is_numeric_dtype(data[c])])
    super_name = st.text_input("Super campaign name", "Super_Campaign")
    if st.button("Create Super Campaign"):
        if len(comps)>=1 and super_name:
            data, sc_name, ok = create_super_campaign_variable(data, comps, super_name)
            if ok:
                st.success(f"Created {sc_name}")
        else:
            st.warning("Select components and provide name")

st.subheader("Updated data (head)")
st.dataframe(data.head())

# Identify paid media channels (excl organic)
media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video']
paid_media_channels = [c for c in data.columns if any(k in c.lower() for k in media_keywords) and 'organic' not in c.lower()]
st.sidebar.header("2) Media Cost Input & FY Ranges")
st.sidebar.write("Detected paid media channels:")
st.sidebar.write(paid_media_channels)

# FY Ranges input
st.sidebar.subheader("Fiscal Year Ranges")
fy_range_default = {
    'FY22': ['2022-01-01','2022-12-31'],
    'FY23': ['2023-01-01','2023-12-31'],
    'FY24': ['2024-01-01','2024-12-31']
}
fy_ranges = {}
for fy in fy_range_default:
    s = st.sidebar.text_input(f"{fy} start", fy_range_default[fy][0], key=f"{fy}_s")
    e = st.sidebar.text_input(f"{fy} end", fy_range_default[fy][1], key=f"{fy}_e")
    fy_ranges[fy] = [s,e]

# Interactive media cost table
st.sidebar.subheader("Media Costs (CPM/CPC)")
media_costs = {}
for channel in paid_media_channels:
    st.sidebar.markdown(f"**{channel}**")
    ctype = st.sidebar.selectbox(f"Cost type for {channel}", options=['CPM','CPC'], key=f"{channel}_type")
    media_costs[channel] = {}
    for fy in fy_ranges.keys():
        val = st.sidebar.number_input(f"{channel} cost {fy} ({ctype})", min_value=0.0, value=0.0, key=f"{channel}_{fy}_cost")
        media_costs[channel][fy] = {'type': ctype, 'value': val}

if st.sidebar.button("Calculate estimated weekly spend"):
    est_weekly_spend = calculate_estimated_weekly_spend(data, media_costs, fy_ranges)
    st.success("Estimated weekly spend computed.")
    st.session_state['est_weekly_spend'] = est_weekly_spend
else:
    est_weekly_spend = st.session_state.get('est_weekly_spend', None)

if est_weekly_spend is not None:
    st.header("Estimated Weekly Spend & Media Execution")
    report_display, report_raw = create_media_spend_report(data, est_weekly_spend, fy_ranges)
    st.dataframe(report_display)
    st.download_button("Download media report CSV", report_raw.to_csv(index=False).encode('utf-8'), file_name='media_spend_report.csv')

# Adstock section
st.header("Adstock Transformation")
# Choose variables to adstock
available_media = [c for c in data.columns if any(k in c.lower() for k in media_keywords) and 'organic' not in c.lower()]
selected_for_adstock = st.multiselect("Select media variables to apply adstock", options=available_media, default=available_media)
hl_inputs = {}
if selected_for_adstock:
    st.write("Select Half-Life (HL) for each selected variable (typical values 0.3,0.6,0.8 or integer weeks). HL > 0.")
    for m in selected_for_adstock:
        v = st.number_input(f"HL for {m}", min_value=0.01, value=0.6, key=f"hl_{m}")
        hl_inputs[m] = v
    if st.button("Apply Adstock"):
        adstocked_vars = []
        media_half_lives = {}
        for m, hl in hl_inputs.items():
            rr = (0.5)**(1/hl)
            colname = f"{m}_adstocked_HL{str(hl).replace('.','')}"
            if pd.api.types.is_numeric_dtype(data[m]):
                data[colname] = apply_adstock(data[m], rr)
                adstocked_vars.append(colname)
                media_half_lives[colname] = hl
            else:
                st.warning(f"{m} not numeric; skipping adstock.")
        st.success(f"Created adstocked vars: {adstocked_vars}")
        st.session_state['adstocked_vars'] = adstocked_vars
        st.session_state['media_half_lives'] = media_half_lives

# Show adstock decay curves
if 'adstocked_vars' in st.session_state and st.session_state['adstocked_vars']:
    st.subheader("Adstock Decay Curves")
    for av in st.session_state['adstocked_vars']:
        hlv = st.session_state['media_half_lives'].get(av, None)
        if hlv is None: continue
        rr = (0.5)**(1/hlv)
        periods = np.arange(0,11)
        decay = [rr**p for p in periods]
        fig = go.Figure(go.Scatter(x=periods, y=decay, mode='lines+markers'))
        fig.update_layout(title=f"Adstock Decay for {av} (HL={hlv}, RR={rr:.3f})", xaxis_title='Periods (weeks)', yaxis_title='Effect Remaining')
        st.plotly_chart(fig, use_container_width=True)

# Saturation step
st.header("Saturation (Hill) Application")
if 'adstocked_vars' in st.session_state and st.session_state['adstocked_vars']:
    sat_gamma_inputs = {}
    st.write("Choose gamma for each adstocked variable (defaults suggested)")
    for av in st.session_state['adstocked_vars']:
        # guess default gamma by channel name heuristics
        name = av.split('_adstocked_')[0].lower() if '_adstocked_' in av else av.lower()
        if any(x in name for x in ['search','click','direct']): default_gamma = 3.0
        elif any(x in name for x in ['social','email','digital']): default_gamma = 1.5
        elif any(x in name for x in ['video','tv','brand','display']): default_gamma = 0.7
        else: default_gamma = 1.0
        g = st.number_input(f"Gamma for {av}", min_value=0.0001, value=float(default_gamma), key=f"gamma_{av}")
        sat_gamma_inputs[av] = g
    if st.button("Apply Saturation to selected adstocked vars"):
        media_gammas_used = {}
        saturated_created = []
        for av, g in sat_gamma_inputs.items():
            colname = f"{av}_saturated_gamma{g}"
            if pd.api.types.is_numeric_dtype(data[av]):
                data[colname] = apply_saturation(data[av], g)
                saturated_created.append(colname)
                media_gammas_used[colname]=g
            else:
                st.warning(f"{av} not numeric; skipping saturation.")
        st.success(f"Saturation applied. Created: {saturated_created}")
        st.session_state['media_gammas_used'] = media_gammas_used
        st.session_state['saturated_vars'] = saturated_created

# Show saturation curves (theoretical)
if 'media_gammas_used' in st.session_state and st.session_state['media_gammas_used']:
    st.subheader("Saturation Curves (theoretical shape)")
    for av, g in st.session_state['media_gammas_used'].items():
        x = np.linspace(0, max(g*10, 1), 200)
        y = x / (x + g + 1e-9)
        fig = go.Figure(go.Scatter(x=x, y=y, mode='lines'))
        fig.add_vline(x=g, line_dash="dash", line_color="green", annotation_text="50% sat ~ gamma")
        fig.update_layout(title=f"Saturation curve for {av} (gamma={g})", xaxis_title="Adstocked Input", yaxis_title="Saturation fraction")
        st.plotly_chart(fig, use_container_width=True)

# Final variable bucketing
st.header("Final variable bucketing & modeling preparation")
if st.button("Bucket variables"):
    dependent_var, final_base_vars, final_media_vars_transformed, final_promo_vars, final_other_vars, final_independent_vars = prepare_for_modeling(data=None) if False else bucket_variables(data)
    st.session_state['final_base_vars'] = final_base_vars
    st.session_state['final_media_vars_transformed'] = final_media_vars_transformed
    st.session_state['final_promo_vars'] = final_promo_vars
    st.session_state['final_other_vars'] = final_other_vars
    st.success("Variables bucketed (see below)")
    st.write("Media transformed:", final_media_vars_transformed)
    st.write("Promo vars:", final_promo_vars)
    st.write("Other vars:", final_other_vars)

# MODEL BUILDING
st.header("Model building & selection")
model_choice = st.selectbox("Select model", ['Ridge','Lasso','ElasticNet'])
alpha = st.number_input("Alpha / regularization (alpha parameter)", min_value=0.0001, value=1.0)
if st.button("Run model (train/test split last 10%)"):
    # prepare independent variables similar to notebook
    independent_vars = []
    if 'final_base_vars' in st.session_state:
        independent_vars += st.session_state['final_base_vars']
    # include saturated transformed media variables from session or data columns
    transformed_cols = [c for c in data.columns if '_adstocked_' in c.lower() and '_saturated_' in c.lower()]
    independent_vars += transformed_cols
    if 'final_promo_vars' in st.session_state:
        independent_vars += st.session_state['final_promo_vars']
    # filter existant
    independent_vars = [c for c in independent_vars if c in data.columns]
    if not independent_vars:
        st.error("No independent variables found for modeling. Make sure adstock + saturation created and bucketing done.")
    else:
        # prepare data
        X = data[independent_vars].fillna(0)
        y = data['Sales'].fillna(0)
        holdout_pct = 0.10
        holdout_size = int(len(X) * holdout_pct)
        split_index = len(X) - holdout_size
        X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
        y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]
        scaler = StandardScaler()
        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)
        if model_choice == 'Ridge':
            model = Ridge(alpha=alpha)
        elif model_choice == 'Lasso':
            model = Lasso(alpha=alpha, max_iter=10000)
        else:
            model = ElasticNet(alpha=alpha, l1_ratio=0.5, max_iter=10000)
        model.fit(X_train_scaled, y_train)
        y_pred_train = model.predict(X_train_scaled)
        y_pred_test = model.predict(X_test_scaled)
        st.write("Train R2:", r2_score(y_train, y_pred_train))
        st.write("Test R2:", r2_score(y_test, y_pred_test))
        st.write("Train RMSE:", mean_squared_error(y_train, y_pred_train, squared=False))
        st.write("Test RMSE:", mean_squared_error(y_test, y_pred_test, squared=False))
        # Coefs
        coefs = pd.DataFrame({'feature': X_train_scaled.columns, 'coef': model.coef_}).sort_values(by='coef', key=lambda s: s.abs(), ascending=False)
        st.subheader("Model coefficients (sorted by absolute value)")
        st.dataframe(coefs)
        st.session_state['final_model'] = model
        st.session_state['scaler'] = scaler
        st.session_state['X_train_scaled'] = X_train_scaled
        st.session_state['X_test_scaled'] = X_test_scaled
        st.session_state['y_train'] = y_train
        st.session_state['y_test'] = y_test
        st.success("Model training complete and saved in session_state")

# VIF calculation (optional)
if st.button("Calculate VIF (on X_train_scaled)"):
    if 'X_train_scaled' not in st.session_state:
        st.warning("Run model first to get X_train_scaled.")
    else:
        Xc = st.session_state['X_train_scaled']
        Xc_sm = sm.add_constant(Xc)
        vif_data = pd.DataFrame({'feature': Xc_sm.columns,
                                 'VIF': [variance_inflation_factor(Xc_sm.values, i) for i in range(Xc_sm.shape[1])]})
        vif_data = vif_data[vif_data['feature']!='const'].sort_values('VIF', ascending=False)
        st.dataframe(vif_data)

# Response curves + automated insights
st.header("Response curves & insights")
if st.button("Generate response curve for all saturated media vars"):
    if 'final_model' not in st.session_state or 'scaler' not in st.session_state or 'y_train' not in st.session_state:
        st.warning("Train a model first.")
    else:
        model = st.session_state['final_model']
        scaler = st.session_state['scaler']
        y_train = st.session_state['y_train']
        coefficients = dict(zip(st.session_state['X_train_scaled'].columns, model.coef_))
        # find saturated vars in data
        saturated_vars = [c for c in data.columns if '_adstocked_' in c.lower() and '_saturated_' in c.lower()]
        # Prepare adstock scaler params using original adstocked columns means/stds
        adstock_means = {}
        adstock_stds = {}
        # try to map names
        for var in data.columns:
            if '_adstocked_' in var.lower() and '_saturated_' not in var.lower():
                adstock_means[var] = data[var].mean()
                adstock_stds[var] = data[var].std() if data[var].std()!=0 else 1.0
        for sat in saturated_vars:
            st.subheader(f"Response curve: {sat}")
            # parse original channel
            parts = sat.split('_adstocked_')
            if len(parts) < 2:
                st.warning("Could not parse sat var")
                continue
            original = parts[0]
            # reconstruct adstock pre-saturation name
            adstock_before = f"{original}_adstocked_{parts[1].split('_saturated_')[0]}"
            hl = None
            rr_val = 0.0
            # try find HL from session_state if available
            if 'media_half_lives' in st.session_state and adstock_before in st.session_state['media_half_lives']:
                hl = st.session_state['media_half_lives'][adstock_before]
                rr_val = (0.5)**(1/hl) if hl>0 else 0
            # gamma from session
            gamma = None
            if 'media_gammas_used' in st.session_state and sat in st.session_state['media_gammas_used']:
                gamma = st.session_state['media_gammas_used'][sat]
            # fallback to a default
            if gamma is None:
                gamma = 1.0
            # coefficient (scaled)
            scaled_coef = coefficients.get(sat, None)
            if scaled_coef is None:
                st.warning(f"Coef for {sat} not found in model.")
                continue
            # find mean/std of adstocked var before saturation
            mean_at_index = adstock_means.get(adstock_before, data[adstock_before].mean() if adstock_before in data.columns else 0)
            std_at_index = adstock_stds.get(adstock_before, data[adstock_before].std() if adstock_before in data.columns else 1)
            # simulation range in original variable units: use original channel distribution
            if original in data.columns:
                orig_max = data[original].max()
                sim_range = np.linspace(0, orig_max*1.5 if orig_max>0 else 10, 200)
            else:
                # small range fallback
                sim_range = np.linspace(0, 10, 200)
            estimated_impacts = []
            for orig_input in sim_range:
                # steady state adstocked
                steady_state = orig_input / (1 - rr_val + 1e-9)
                sat_out = apply_saturation(steady_state, gamma)
                scaled_sat = (sat_out - mean_at_index) / (std_at_index + 1e-9)
                scaled_impact_on_sales = scaled_sat * scaled_coef
                estimated_original_sales_impact = scaled_impact_on_sales * y_train.std()
                estimated_impacts.append(estimated_original_sales_impact)
            # plot
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=sim_range, y=estimated_impacts, mode='lines', name='Estimated Impact on Sales'))
            # add percentile vertical lines (25,50,75) of original input
            if original in data.columns:
                pct_values = data[original].quantile([0.25,0.5,0.75]).to_dict()
                colors = {0.25:'blue',0.5:'green',0.75:'red'}
                labels = {0.25:'25th',0.5:'50th (median)',0.75:'75th'}
                for p,v in pct_values.items():
                    if 0<=v<=sim_range.max():
                        fig.add_vline(x=v, line_dash='dot', line_color=colors[p], annotation_text=f"{labels[p]}: {format_number(v)}", annotation_position='bottom right')
            fig.update_layout(title=f"Estimated Response Curve for {original} (HL={hl}, Gamma={gamma})", xaxis_title=f"Original {original} Input", yaxis_title="Estimated Impact on Sales", hovermode='x unified')
            st.plotly_chart(fig, use_container_width=True)
            # Insights: compute impact deltas at 0, 25, 50, 75 percentiles
            if original in data.columns:
                percentiles = [0,0.25,0.5,0.75]
                pvals = data[original].quantile(percentiles).to_dict()
                impacts = {}
                for p,v in pvals.items():
                    idx = np.abs(sim_range - v).argmin()
                    impacts[p] = estimated_impacts[idx]
                # baseline
                i0 = impacts.get(0, estimated_impacts[0])
                i25 = impacts.get(0.25, i0)
                i50 = impacts.get(0.5, i25)
                i75 = impacts.get(0.75, i50)
                st.write(f"Impact at 0: {format_number(i0)}")
                st.write(f"Impact at 25th ({format_number(pvals[0.25])}): {format_number(i25)}")
                st.write(f"Impact at 50th ({format_number(pvals[0.5])}): {format_number(i50)}")
                st.write(f"Impact at 75th ({format_number(pvals[0.75])}): {format_number(i75)}")
                st.write("Incremental changes:")
                st.write(f"0 → 25th: {format_number(i25 - i0)}")
                st.write(f"25th → 50th: {format_number(i50 - i25)}")
                st.write(f"50th → 75th: {format_number(i75 - i50)}")
        st.success("Generated response curves & insights")

st.write("---")
st.write("This app is a close port of the Beta_MMM notebook. If you need specific extra notebook flows (e.g., GridSearchCV parameter grids, advanced model diagnostics, or exporting specific plots), we can add those next.")