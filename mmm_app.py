# -*- coding: utf-8 -*-
"""mmm_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mFlFUzuEFUZAWVSi4LKnxnoY_bhoM8Bc
"""

# mmm_app.py
"""
Streamlit frontend for your MMM notebook logic (beta_mmm.py).
Place this file in the same folder as beta_mmm.py and run:
    streamlit run mmm_app.py
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import io
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
import seaborn as sns
import plotly.graph_objects as go
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error
from scipy.optimize import nnls
import statsmodels.api as sm
import warnings
warnings.filterwarnings("ignore")

st.set_page_config(page_title="MMM Platform", layout="wide")
sns.set_palette("husl")

# Try importing notebook functions (preferred)
USE_BETA_MODULE = False
try:
    import beta_mmm
    if hasattr(beta_mmm, "load_and_preprocess_data") and hasattr(beta_mmm, "perform_comprehensive_eda"):
        USE_BETA_MODULE = True
except Exception:
    USE_BETA_MODULE = False

# -------------------- Helper functions --------------------

def convert_indian_number(value):
    if isinstance(value, str):
        cleaned_value = value.replace(',', '').strip()
        if cleaned_value in ['', '-', None] or cleaned_value.isspace():
            return np.nan
        try:
            return float(cleaned_value)
        except Exception:
            return np.nan
    return value

def fallback_load(csv_file):
    df = pd.read_csv(csv_file)
    # find a date-like column
    possible_dates = [c for c in df.columns if c.lower() in ['week_ending', 'week ending', 'date', 'week']]
    if possible_dates:
        date_col = possible_dates[0]
    else:
        date_col = None
    # convert object columns that look numeric
    for col in df.columns:
        if df[col].dtype == object:
            s = df[col].dropna().astype(str)
            if s.shape[0] > 0 and (s.str.contains(',').any() or s.str.match(r'^[\d\.\,]+$').all()):
                df[col] = df[col].apply(convert_indian_number)
    if date_col:
        try:
            df[date_col] = pd.to_datetime(df[date_col], errors='coerce', infer_datetime_format=True)
            df = df.sort_values(date_col).reset_index(drop=True)
            df = df.rename(columns={date_col: 'Week_Ending'})
        except Exception:
            pass
    return df

def show_timeseries(df, target):
    fig, ax = plt.subplots(figsize=(10,4))
    if 'Week_Ending' in df.columns:
        x = pd.to_datetime(df['Week_Ending'])
    else:
        x = range(len(df))
    ax.plot(x, df[target], marker='o', linewidth=1)
    ax.set_title(f"{target} over time")
    ax.yaxis.set_major_formatter(FuncFormatter(lambda val, p: f"{val/1e6:.1f}M"))
    st.pyplot(fig)

def correlation_heatmap(numeric_df):
    fig, ax = plt.subplots(figsize=(10,8))
    sns.heatmap(numeric_df.corr(), annot=True, fmt=".2f", cmap="coolwarm", ax=ax)
    st.pyplot(fig)

# Minimal constrained linear regression using NNLS for media variables
class ConstrainedLinearRegression:
    def __init__(self, media_var_names=None):
        self.media_var_names = media_var_names or []
        self.coef_ = None
        self.intercept_ = None
        self.feature_names_ = None

    def fit(self, X_df, y):
        X = X_df.copy()
        self.feature_names_ = list(X.columns)
        X_arr = X.values
        media_indices = [i for i, c in enumerate(self.feature_names_) if c in self.media_var_names]
        non_media_indices = [i for i in range(len(self.feature_names_)) if i not in media_indices]

        ones = np.ones((X_arr.shape[0], 1))
        X_with_intercept = np.hstack([ones, X_arr])

        if non_media_indices:
            use_cols = [0] + [i+1 for i in non_media_indices]
            coef_non_media, *_ = np.linalg.lstsq(X_with_intercept[:, use_cols], y, rcond=None)
            y_resid = y - X_with_intercept[:, use_cols] @ coef_non_media
            media_cols = [i+1 for i in media_indices] if media_indices else []
            if media_cols:
                coef_media, _ = nnls(X_with_intercept[:, media_cols], y_resid)
            else:
                coef_media = np.array([])
            self.intercept_ = coef_non_media[0]
            coefs = np.zeros(X_arr.shape[1])
            for j, idx in enumerate(non_media_indices):
                coefs[idx] = coef_non_media[j+1]
            for j, idx in enumerate(media_indices):
                coefs[idx] = coef_media[j]
            self.coef_ = coefs
        else:
            coef_media, _ = nnls(X_arr, y)
            self.intercept_ = 0
            self.coef_ = coef_media

    def predict(self, X_df):
        return X_df.values.dot(self.coef_) + self.intercept_

def compute_contributions(coefs, X_df, intercept=0):
    contrib = pd.DataFrame(index=X_df.index)
    for c in X_df.columns:
        contrib[c] = coefs.get(c, 0) * X_df[c]
    contrib['base'] = intercept
    contrib['total_pred'] = contrib.sum(axis=1)
    return contrib

def train_model_pipeline(df_model, modeling_buckets, model_choice, holdout_weeks=12, constrain_media=False, random_state=42):
    dfm = df_model.copy()
    if 'Week_Ending' in dfm.columns:
        dfm['Week_Ending'] = pd.to_datetime(dfm['Week_Ending'], errors='coerce')
        dfm = dfm.set_index('Week_Ending')
    target = modeling_buckets.get('target_var', 'Sales')
    dfm = dfm.dropna(subset=[target])
    if holdout_weeks >= len(dfm):
        holdout_weeks = int(max(1, len(dfm)/4))
    train_df = dfm.iloc[:-holdout_weeks, :] if holdout_weeks>0 else dfm.copy()
    test_df = dfm.iloc[-holdout_weeks:, :] if holdout_weeks>0 else dfm.copy()

    X_train = train_df.drop(columns=[target])
    y_train = train_df[target].values
    X_test = test_df.drop(columns=[target])
    y_test = test_df[target].values

    scaler = StandardScaler()
    X_train_num = scaler.fit_transform(X_train.values)
    X_test_num = scaler.transform(X_test.values)
    X_train_df = pd.DataFrame(X_train_num, columns=X_train.columns, index=X_train.index)
    X_test_df = pd.DataFrame(X_test_num, columns=X_test.columns, index=X_test.index)

    model_obj = None
    if constrain_media:
        media_vars = modeling_buckets.get('media_vars', [])
        clr = ConstrainedLinearRegression(media_var_names=media_vars)
        clr.fit(X_train_df, y_train)
        y_pred_train = clr.predict(X_train_df)
        y_pred_test = clr.predict(X_test_df)
        model_obj = clr
    else:
        from sklearn.linear_model import Ridge, Lasso, ElasticNet
        if model_choice == 'ridge':
            model = Ridge(alpha=1.0, random_state=random_state)
        elif model_choice == 'lasso':
            model = Lasso(alpha=0.001, max_iter=10000, random_state=random_state)
        else:
            model = ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000, random_state=random_state)
        model.fit(X_train_df, y_train)
        y_pred_train = model.predict(X_train_df)
        y_pred_test = model.predict(X_test_df)
        model_obj = {'sklearn_model': model, 'scaler': scaler}

    r2_train = r2_score(y_train, y_pred_train) if len(y_train)>0 else np.nan
    r2_test = r2_score(y_test, y_pred_test) if len(y_test)>0 else np.nan
    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train)) if len(y_train)>0 else np.nan
    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test)) if len(y_test)>0 else np.nan
    mape_train = mean_absolute_percentage_error(y_train, y_pred_train)*100 if len(y_train)>0 else np.nan
    mape_test = mean_absolute_percentage_error(y_test, y_pred_test)*100 if len(y_test)>0 else np.nan

    if constrain_media:
        coefs = pd.Series(model_obj.coef_, index=X_train_df.columns)
        intercept = model_obj.intercept_
    else:
        coefs = pd.Series(model_obj['sklearn_model'].coef_, index=X_train_df.columns)
        intercept = model_obj['sklearn_model'].intercept_

    results = {
        'model_obj': model_obj,
        'y_train': y_train, 'y_test': y_test,
        'y_pred_train': y_pred_train, 'y_pred_test': y_pred_test,
        'r2_train': r2_train, 'r2_test': r2_test,
        'rmse_train': rmse_train, 'rmse_test': rmse_test,
        'mape_train': mape_train, 'mape_test': mape_test,
        'coefs': coefs, 'intercept': intercept,
        'X_train_df': X_train_df, 'X_test_df': X_test_df
    }
    return results

# -------------------- Streamlit UI --------------------

st.title("ðŸ“Š Marketing Mix Modeling â€” Streamlit App")

# Sidebar - upload and global settings
with st.sidebar:
    st.header("Upload & Global settings")
    uploaded_file = st.file_uploader("Upload MMM CSV", type=["csv"])
    st.markdown("---")
    default_holdout = st.number_input("Default holdout weeks", min_value=1, max_value=52, value=12)
    default_constrain_media = st.checkbox("Default: Constrain media coefficients (NNLS)", value=False)
    st.markdown("---")
    st.info("Put beta_mmm.py in same folder to reuse exact notebook functions (recommended).")

if not uploaded_file:
    st.info("Upload your MMM CSV in the sidebar to get started (left).")
    st.stop()

# Load data (prefer beta_mmm loader)
if USE_BETA_MODULE:
    try:
        data = beta_mmm.load_and_preprocess_data(uploaded_file)
        st.success("Loaded data using beta_mmm.load_and_preprocess_data()")
    except Exception:
        data = fallback_load(uploaded_file)
        st.warning("beta_mmm.load_and_preprocess_data failed â€” used fallback loader.")
else:
    data = fallback_load(uploaded_file)

st.session_state['data'] = data

st.subheader("Data preview")
st.dataframe(data.head())

# Workflow tabs
tab_eda, tab_fe, tab_model, tab_results = st.tabs(["Exploration", "Feature Engineering", "Modeling", "Results"])

# ---------------- Exploration Tab ----------------
with tab_eda:
    st.header("Exploration & EDA")
    target_col = st.selectbox("Select target variable", options=list(data.columns), index=list(data.columns).index('Sales') if 'Sales' in data.columns else 0)
    if st.button("Run EDA", key="run_eda"):
        if USE_BETA_MODULE:
            try:
                # beta_mmm.perform_comprehensive_eda prints/plots - run it to produce same outputs
                _ = beta_mmm.perform_comprehensive_eda(data.copy(), target_var=target_col)
                st.success("Ran perform_comprehensive_eda from beta_mmm.py (outputs printed to console).")
            except Exception:
                st.warning("beta_mmm.perform_comprehensive_eda failed â€” running EDA fallback.")
                numeric_df = data.select_dtypes(include=[np.number])
                st.write("Numeric summary:")
                st.dataframe(numeric_df.describe())
                correlation_heatmap(numeric_df)
        else:
            numeric_df = data.select_dtypes(include=[np.number])
            st.write("Numeric summary:")
            st.dataframe(numeric_df.describe())
            correlation_heatmap(numeric_df)
        if 'Week_Ending' in data.columns and target_col in data.columns:
            try:
                show_timeseries(data, target_col)
            except Exception as e:
                st.write("Time series plot error:", e)

# ---------------- Feature Engineering Tab ----------------
with tab_fe:
    st.header("Feature Engineering & Media Reporting")
    st.markdown("All interactive steps from your notebook are represented here: spend estimation (CPM/CPC), seasonal index (SIndex), custom dummies, splits, super-campaigns, and fiscal-year reporting.")

    if st.button("Start Feature Engineering", key="start_fe"):
        # We'll implement feature engineering in-streamlit using widget inputs
        df = data.copy()
        if 'Week_Ending' not in df.columns:
            st.error("Your dataset must contain 'Week_Ending' column (detected earlier).")
            st.stop()
        df['Week_Ending'] = pd.to_datetime(df['Week_Ending'], errors='coerce')
        df = df.sort_values('Week_Ending').reset_index(drop=True)
        df_fe = df.copy()
        df_fe = df_fe.set_index('Week_Ending')
        df_fe.index.name = 'Week_Ending'

        # Fiscal year
        if 'Fiscal_Year' not in df_fe.columns:
            df_fe['Fiscal_Year'] = df_fe.index.year

        TARGET = st.text_input("Target variable (for FE)", value="Sales")
        if TARGET not in df_fe.columns:
            TARGET = st.selectbox("Pick target variable", options=list(df_fe.columns), index=0)

        # Detect candidate media columns
        media_keywords = ['impressions','impr','clicks','click','social','search','video','tv','display','email','paid']
        candidate_media = [c for c in df_fe.columns if any(k in c.lower() for k in media_keywords) and c!=TARGET]
        st.write("Detected media-like columns:", candidate_media)

        # Spend estimation
        with st.expander("Estimate spend (set CPM/CPC rates per FY)"):
            selected_channels = st.multiselect("Select channels to estimate spend for", options=candidate_media, default=candidate_media[:3])
            years = sorted(df_fe['Fiscal_Year'].unique().tolist())
            channel_rates = {}
            for ch in selected_channels:
                st.markdown(f"**{ch}**")
                col_type = 'impressions' if any(k in ch.lower() for k in ['impr','impressions','video','tv','display']) else ('clicks' if 'click' in ch.lower() else 'volume')
                cols = st.columns(len(years))
                rates = {}
                for i,y in enumerate(years):
                    with cols[i]:
                        rates[y] = st.number_input(f"Rate FY{y}", min_value=0.0, value=0.0, step=0.1, key=f"rate_{ch}_{y}")
                channel_rates[ch] = {'type': col_type, 'rates': rates}
            # compute spend columns
            estimated_spend_cols = []
            for ch, info in channel_rates.items():
                if ch not in df_fe.columns:
                    st.warning(f"{ch} not in data; cannot compute spend.")
                    continue
                rate_series = df_fe['Fiscal_Year'].map(pd.Series(info['rates'])).fillna(0)
                col_data = pd.to_numeric(df_fe[ch], errors='coerce').fillna(0)
                spend_col = f"{ch}_Spend"
                if info['type'] == 'impressions':
                    df_fe[spend_col] = (col_data / 1000) * rate_series
                else:
                    df_fe[spend_col] = col_data * rate_series
                if (df_fe[spend_col] > 0).any():
                    estimated_spend_cols.append(spend_col)
            if estimated_spend_cols:
                st.success(f"Created estimated spend columns: {estimated_spend_cols}")

        # Seasonal decomposition
        with st.expander("Seasonal decomposition (create SIndex)"):
            season_choice = st.selectbox("Seasonal period", options=[4,13,26,52], index=3, key="season_period")
            try:
                series = df_fe[TARGET].copy()
                if series.isna().any():
                    series = series.fillna(method='ffill').fillna(method='bfill')
                decomposition = sm.tsa.seasonal_decompose(series, period=season_choice, model='additive', extrapolate_trend='freq')
                df_fe['SIndex'] = decomposition.seasonal
                st.success("SIndex created.")
                st.line_chart(df_fe['SIndex'].reset_index().set_index('Week_Ending'))
            except Exception as e:
                st.error(f"SIndex creation failed: {e}")

        # Custom dummies
        with st.expander("Create custom date dummies"):
            add_dummies = st.checkbox("Add custom dummies?")
            custom_dummy_cols = []
            if add_dummies:
                dummy_dates_input = st.text_input("Comma-separated dates (YYYY-MM-DD)", value="")
                if dummy_dates_input.strip():
                    date_strings = [d.strip() for d in dummy_dates_input.split(",") if d.strip()]
                    parsed = pd.to_datetime(date_strings, errors='coerce')
                    parsed = [d for d in parsed if not pd.isna(d)]
                    for i,date_obj in enumerate(parsed, 1):
                        name = f"dummy{i}_{date_obj.strftime('%Y-%m-%d')}"
                        df_fe[name] = (df_fe.index.normalize() == date_obj.normalize()).astype(int)
                        custom_dummy_cols.append(name)
                    st.write("Created dummies:", custom_dummy_cols)

        # Split variable
        with st.expander("Split a variable into pre/post at a chosen date"):
            do_split = st.checkbox("Do a split?")
            if do_split:
                exclude = ['SIndex','Fiscal_Year', TARGET] + custom_dummy_cols
                split_candidates = [c for c in df_fe.columns if c not in exclude and '_Spend' not in c]
                if split_candidates:
                    var_to_split = st.selectbox("Variable to split", split_candidates)
                    split_date = st.date_input("Split date", value=df_fe.index[int(len(df_fe)/2)].date())
                    if st.button("Apply split", key="apply_split_fe"):
                        split_dt = pd.to_datetime(split_date)
                        pre_mask = df_fe.index <= split_dt
                        df_fe[f"{var_to_split}_pre"] = df_fe[var_to_split].where(pre_mask, 0)
                        df_fe[f"{var_to_split}_post"] = df_fe[var_to_split].where(~pre_mask, 0)
                        spend_cp = f"{var_to_split}_Spend"
                        if spend_cp in df_fe.columns:
                            df_fe[f"{spend_cp}_pre"] = df_fe[spend_cp].where(pre_mask, 0)
                            df_fe[f"{spend_cp}_post"] = df_fe[spend_cp].where(~pre_mask, 0)
                        df_fe.drop(columns=[var_to_split], inplace=True, errors='ignore')
                        if spend_cp in df_fe.columns:
                            df_fe.drop(columns=[spend_cp], inplace=True, errors='ignore')
                        st.success("Split applied and original variable dropped.")
                else:
                    st.info("No variables available to split.")

        # Super campaign
        with st.expander("Create a Super Campaign by summing variables"):
            do_super = st.checkbox("Create super campaign?")
            if do_super:
                available_for_super = [c for c in df_fe.columns if c not in ['SIndex','Fiscal_Year', TARGET] and not c.endswith(('_pre','_post')) and '_Spend' not in c]
                selection = st.multiselect("Variables to combine", options=available_for_super)
                super_name = st.text_input("Super campaign base name", value="Super_Campaign")
                if st.button("Create super campaign", key="create_super_fe"):
                    if selection:
                        super_vol = f"{super_name}_Volume"
                        df_fe[super_vol] = df_fe[selection].sum(axis=1)
                        spend_cols = [f"{s}_Spend" for s in selection if f"{s}_Spend" in df_fe.columns]
                        if spend_cols:
                            df_fe[f"{super_name}_Spend"] = df_fe[spend_cols].sum(axis=1)
                        df_fe.drop(columns=selection, inplace=True, errors='ignore')
                        st.success(f"Created {super_vol} (and spend if spend cols existed).")
                    else:
                        st.warning("Select variables to combine.")

        # Fiscal-year aggregation & YoY
        with st.expander("Fiscal year aggregation & YoY"):
            media_volume_reporting = [c for c in df_fe.columns if any(k in c.lower() for k in ['impr','impressions','click','social','search','video','tv','display','email','paid','super','_volume']) and '_Spend' not in c and c!=TARGET]
            spend_media_reporting = [c for c in df_fe.columns if c.endswith('_Spend')]
            report_candidates = list(set(media_volume_reporting + spend_media_reporting))
            if report_candidates:
                yearly_agg = df_fe.groupby('Fiscal_Year')[report_candidates].sum()
                st.write("Yearly aggregated sample:")
                st.dataframe(yearly_agg)
                if len(yearly_agg.index) >= 2:
                    yoy = yearly_agg.pct_change().iloc[1:] * 100
                    st.write("YoY % changes:")
                    st.dataframe(yoy.round(2))
            else:
                st.info("No media execution or spend columns found to aggregate.")

        # Final modeling df
        with st.expander("Finalize modeling dataset and see buckets"):
            all_cols_final = df_fe.columns.tolist()
            base_vars_candidates = ['Average Price','Gasoline Price','SIndex','Total SKU'] + [c for c in all_cols_final if c.startswith('Holiday_') or c.startswith('dummy')]
            base_vars = [c for c in base_vars_candidates if c in df_fe.columns]
            promo_vars = [c for c in all_cols_final if 'discount' in c.lower()]
            media_vars = [c for c in all_cols_final if (any(k in c.lower() for k in ['impr','impressions','click','social','search','video','tv','display','email','paid','super','combined_var','_volume','_pre','_post'])) and '_Spend' not in c and c not in base_vars and c not in promo_vars and c!=TARGET]
            modeling_vars = base_vars + promo_vars + media_vars
            if TARGET in df_fe.columns:
                modeling_vars = modeling_vars + [TARGET]
            df_model = df_fe[modeling_vars].reset_index()
            st.write("Modeling buckets:")
            st.write({"base_vars": base_vars, "promo_vars": promo_vars, "media_vars": media_vars, "target_var": TARGET})
            st.dataframe(df_model.head())
            # store in session
            st.session_state['df_features'] = df_fe.reset_index()
            st.session_state['df_model'] = df_model
            st.session_state['modeling_buckets'] = {'base_vars': base_vars, 'promo_vars': promo_vars, 'media_vars': media_vars, 'target_var': TARGET}
            st.success("Feature engineering outputs stored in session_state.")

# ---------------- Modeling Tab ----------------
with tab_model:
    st.header("Model selection & training (use df_model from Feature Engineering)")
    if 'df_model' not in st.session_state:
        st.warning("Run Feature Engineering first (Feature Engineering tab).")
    else:
        modeling_buckets = st.session_state['modeling_buckets']
        st.write("Modeling buckets:", modeling_buckets)
        model_choice = st.selectbox("Choose model", options=['elasticnet','ridge','lasso'], index=0)
        holdout_weeks = st.number_input("Holdout weeks", min_value=1, max_value=52, value=int(default_holdout), step=1)
        constrain_media = st.checkbox("Constrain media coefficients (non-negative)", value=default_constrain_media)
        if st.button("Train model", key="train_model"):
            st.info("Training model â€” please wait.")
            df_model = st.session_state['df_model'].copy()
            # ensure Week_Ending index exists
            if 'Week_Ending' in df_model.columns:
                df_model['Week_Ending'] = pd.to_datetime(df_model['Week_Ending'], errors='coerce')
            results = train_model_pipeline(df_model, modeling_buckets, model_choice, holdout_weeks, constrain_media)
            st.session_state['model_results'] = results
            st.success("Training complete and saved in session_state.")
            st.metric("RÂ² (train)", f"{results['r2_train']:.3f}")
            if not np.isnan(results['r2_test']):
                st.metric("RÂ² (test)", f"{results['r2_test']:.3f}")
            st.write("Train RMSE:", f"{results['rmse_train']:.2f}")
            st.write("Train MAPE %:", f"{results['mape_train']:.2f}")
            if not np.isnan(results['rmse_test']):
                st.write("Test RMSE:", f"{results['rmse_test']:.2f}")
                st.write("Test MAPE %:", f"{results['mape_test']:.2f}")
            st.subheader("Model coefficients")
            st.dataframe(results['coefs'].sort_values(ascending=False).to_frame("coef"))

# ---------------- Results Tab ----------------
with tab_results:
    st.header("Model results, contributions & response curves")
    if 'model_results' not in st.session_state:
        st.info("Train a model in the Modeling tab first.")
    else:
        res = st.session_state['model_results']
        # Actual vs Predicted (holdout)
        if len(res['y_test']) > 0:
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=res['X_test_df'].index, y=res['y_test'], mode='lines+markers', name='Actual'))
            fig.add_trace(go.Scatter(x=res['X_test_df'].index, y=res['y_pred_test'], mode='lines+markers', name='Predicted'))
            fig.update_layout(title="Actual vs Predicted (holdout)", xaxis_title="Date", yaxis_title="Sales")
            st.plotly_chart(fig, use_container_width=True)
        # contributions (train)
        coefs = res['coefs']
        X_train_df = res['X_train_df']
        intercept = res['intercept']
        contrib_train = compute_contributions(coefs, X_train_df, intercept=intercept)
        agg = contrib_train.sum().drop('total_pred', errors='ignore').sort_values(ascending=False)
        st.subheader("Aggregate contributions (train period)")
        df_contrib = agg.to_frame("Contribution").assign(Share=lambda d: (d['Contribution']/d['Contribution'].sum()*100).round(2))
        st.dataframe(df_contrib)
        # response curves
        st.subheader("Approximate response curves (in scaled units)")
        media_vars = st.session_state['modeling_buckets'].get('media_vars', [])
        if media_vars:
            rc_var = st.selectbox("Pick media variable to plot response curve", options=media_vars)
            X_sample = X_train_df.copy()
            var_vals = np.linspace(X_sample[rc_var].min(), X_sample[rc_var].max(), 50)
            preds = []
            for v in var_vals:
                X_tmp = X_sample.median().to_frame().T
                X_tmp.iloc[0] = X_sample.median()
                X_tmp[rc_var] = v
                pred = X_tmp.values.dot(coefs.values) + intercept
                preds.append(pred[0])
            fig2 = go.Figure()
            fig2.add_trace(go.Scatter(x=var_vals, y=preds, mode='lines'))
            fig2.update_layout(title=f"Response curve for {rc_var} (scaled units)", xaxis_title="Scaled value", yaxis_title="Predicted Sales")
            st.plotly_chart(fig2, use_container_width=True)
        else:
            st.info("No media vars found to show response curves.")
        # simple scenario analysis
        st.subheader("Simple scenario analysis")
        if media_vars:
            scen_var = st.selectbox("Scenario media variable", media_vars, key="scenario_var")
            pct_change = st.slider("Change applied (%)", -100, 500, 10)
            last_row = res['X_test_df'].iloc[-1] if len(res['X_test_df'])>0 else res['X_train_df'].iloc[-1]
            modified = last_row.copy()
            modified[scen_var] = modified[scen_var] * (1 + pct_change/100.0)
            new_pred = modified.values.dot(coefs.values) + intercept
            base_pred = last_row.values.dot(coefs.values) + intercept
            st.write(f"Base predicted sales (last period): {base_pred:.2f}")
            st.write(f"Scenario predicted sales: {new_pred:.2f} ({(new_pred-base_pred)/base_pred*100:.2f}% change)")
        # download coefficients
        if st.button("Download coefficients CSV"):
            buf = io.StringIO()
            res['coefs'].to_csv(buf, header=True)
            b = buf.getvalue().encode()
            st.download_button("Download coefficients", data=b, file_name="mmm_coefficients.csv", mime="text/csv")