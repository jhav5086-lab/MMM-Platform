# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mFlFUzuEFUZAWVSi4LKnxnoY_bhoM8Bc
"""

# -*- coding: utf-8 -*-
"""Beta_MMM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14klC7g67_QAaPRaWFD-MODrI2GzTqHA8
"""

# Marketing Mix Modeling - Colab Testing Version (Optimized)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Add other necessary imports
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.optimize import minimize
from statsmodels.tsa.seasonal import seasonal_decompose
import matplotlib.ticker as mticker
from IPython.display import display

# Set style for plots
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Function to convert Indian number format to float
def convert_indian_number(value):
    """Convert Indian number format string to float"""
    if isinstance(value, str):
        # Remove commas and strip whitespace
        cleaned_value = value.replace(',', '').strip()

        # Handle special cases like ' -   ' which should be treated as NaN
        if cleaned_value in ['-', ''] or cleaned_value.isspace():
            return np.nan

        try:
            return float(cleaned_value)
        except ValueError:
            print(f"Could not convert value: '{value}'")
            return np.nan
    return value

# Load and preprocess data
def load_and_preprocess_data(file_path):
    """Load and preprocess the marketing mix data"""
    # Load the data
    data = pd.read_csv(file_path)

    # First, identify all columns that might contain Indian number format
    # These are all columns except the date column
    all_columns = data.columns.tolist()
    date_column = 'Week_Ending'

    if date_column in all_columns:
        all_columns.remove(date_column)

    # Convert all numeric columns (including Sales) - Optimized conversion
    # Apply conversion only to object columns that are likely numbers
    for col in all_columns:
        if data[col].dtype == 'object':
            # Use a more robust check for potential numbers before applying conversion
            try:
                 # Attempt to convert a sample to detect if it's numeric strings
                 data[col].sample(min(10, len(data))).astype(str).str.replace(',', '').astype(float)
                 # If the sample conversion works, apply to the whole column
                 data[col] = data[col].apply(convert_indian_number)
            except (ValueError, AttributeError):
                 # If sample conversion fails, it's likely not a numeric column with commas
                 pass

    # Handle missing values in Paid Search Impressions
    if 'Paid Search Impressions' in data.columns:
        missing_count = data['Paid Search Impressions'].isna().sum()
        if missing_count > 0:
            print(f"Found {missing_count} missing values in 'Paid Search Impressions'. Imputing with 0.")
            data['Paid Search Impressions'] = data['Paid Search Impressions'].fillna(0)

    # Convert date column
    if 'Week_Ending' in data.columns:
        # Use infer_datetime_format=True and cache=True for potentially faster conversion
        data['Week_Ending'] = pd.to_datetime(data['Week_Ending'], format='%d-%m-%Y %H:%M', errors='coerce', infer_datetime_format=True, cache=True)
        data = data.sort_values('Week_Ending').reset_index(drop=True)

    return data

# Perform comprehensive EDA
def perform_comprehensive_eda(data, target_var='Sales'):
    """Perform comprehensive exploratory data analysis"""
    print("="*60)
    print("COMPREHENSIVE EXPLORATORY DATA ANALYSIS")
    print("="*60)

    # 1. Basic Information
    print("\n1. BASIC DATASET INFORMATION")
    print("="*40)
    print(f"Shape: {data.shape}")
    print(f"Columns: {list(data.columns)}")
    if 'Week_Ending' in data.columns:
        print(f"Date Range: {data['Week_Ending'].min()} to {data['Week_Ending'].max()}")
    print(f"Missing Values: {data.isnull().sum().sum()}")

    # Check if target variable is numeric
    if data[target_var].dtype == 'object':
         print(f"\nWARNING: Target variable '{target_var}' is not numeric after preprocessing.")

    # 2. Summary Statistics
    print("\n\n2. SUMMARY STATISTICS")
    print("="*40)

    # Numeric variables summary
    numeric_df = data.select_dtypes(include=[np.number])
    print("Numeric Variables Summary:")
    display(numeric_df.describe())

    # Add skewness and kurtosis
    skewness = numeric_df.skew().to_frame('Skewness')
    kurtosis = numeric_df.kurtosis().to_frame('Kurtosis')
    stats_df = pd.concat([skewness, kurtosis], axis=1)
    print("\nSkewness and Kurtosis:")
    display(stats_df)

    # 3. Univariate Analysis (Using Matplotlib/Seaborn)
    print("\n\n3. UNIVARIATE ANALYSIS")
    print("="*40)

    # Create distribution plots for all numeric variables
    numeric_cols = numeric_df.columns.tolist()
    for col in numeric_cols:
        plt.figure(figsize=(10, 6))
        sns.histplot(data=data, x=col, kde=True, bins=30)
        plt.title(f"Distribution of {col}")
        plt.ylabel("Frequency")

        # Add vertical lines for mean and median with improved formatting
        mean_val = data[col].mean()
        median_val = data[col].median()

        # Conditional formatting for labels and axis ticks based on column name
        if 'impressions' in col.lower() or 'clicks' in col.lower():
            plt.xlabel(f"{col} (Millions)") # Indicate units in label
            mean_label = f'Mean: {mean_val/1e6:.2f}M' # Show in millions for labels
            median_label = f'Median: {median_val/1e6:.2f}M'
            plt.ticklabel_format(style='plain', axis='x') # Turn off scientific notation for x-axis
            # Manually format x-axis ticks for millions
            plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))

        elif 'discount' in col.lower():
             plt.xlabel(f"{col} (%)") # Indicate units in label
             mean_label = f'Mean: {mean_val*100:.2f}%' # Show as percentage for labels
             median_label = f'Median: {median_val*100:.2f}%'
             # Format x-axis ticks as percentages
             plt.gca().xaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0)) # xmax=1.0 because data is 0-1 range

        elif col == target_var:
             plt.xlabel(col)
             mean_label = f'Mean: {mean_val/1e6:.2f}M' # Show in millions for labels
             median_label = f'Median: {median_val/1e6:.2f}M'
             plt.ticklabel_format(style='plain', axis='x') # Turn off scientific notation for x-axis
             plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # Format ticks in millions
        else:
            plt.xlabel(col)
            mean_label = f'Mean: {mean_val:,.2f}' # General formatting with commas
            median_label = f'Median: {median_val:,.2f}'
            plt.ticklabel_format(style='plain', axis='x') # Turn off scientific notation for x-axis


        plt.axvline(mean_val, color='red', linestyle='dashed', linewidth=1, label=mean_label)
        plt.axvline(median_val, color='green', linestyle='dashed', linewidth=1, label=median_label)
        plt.legend()
        plt.show()


    # 4. Bivariate Analysis (Using Matplotlib/Seaborn)
    print("\n\n4. BIVARIATE ANALYSIS: RELATIONSHIP WITH TARGET VARIABLE")
    print("="*40)

    # Create scatter plots against target variable
    if target_var in numeric_cols:
        numeric_cols_for_scatter = numeric_cols.copy()
        numeric_cols_for_scatter.remove(target_var)

    for col in numeric_cols_for_scatter:
        plt.figure(figsize=(10, 6))
        sns.scatterplot(data=data, x=col, y=target_var)
        plt.title(f"{target_var} vs {col}")

        # Conditional formatting for x-axis label and potentially ticks
        if 'impressions' in col.lower() or 'clicks' in col.lower():
            plt.xlabel(f"{col} (Millions)") # Indicate units in label
            plt.ticklabel_format(style='plain', axis='x')
            plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # Format ticks in millions
        elif 'discount' in col.lower():
             plt.xlabel(f"{col} (%)") # Indicate units in label
             plt.gca().xaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0)) # Format ticks as percentage

        else:
            plt.xlabel(col)
            plt.ticklabel_format(style='plain', axis='x') # Turn off scientific notation for x-axis


        # Conditional formatting for y-axis label (target variable) and ticks
        if target_var in numeric_df.columns: # Ensure target variable is numeric before formatting
            plt.ylabel(f"{target_var} (Millions)")
            plt.ticklabel_format(style='plain', axis='y')
            plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # Format ticks in millions
        else:
             plt.ylabel(target_var) # No formatting if target is not numeric


        # Add correlation coefficient as text
        correlation = data[col].corr(data[target_var])
        # Position the text box based on formatted axis limits (more complex, so using a simple approach first)
        # Simple approach: Use relative positioning within the plot area
        plt.text(data[col].min() + (data[col].max() - data[col].min()) * 0.05,
                 data[target_var].max() - (data[target_var].max() - data[target_var].min()) * 0.05,
                 f"r = {correlation:.3f}",
                 fontsize=12, bbox=dict(facecolor='white', alpha=0.8))
        plt.show()


    # 5. Time Series Analysis (Using Matplotlib/Seaborn)
    print("\n\n5. TIME SERIES ANALYSIS")
    print("="*40)

    if 'Week_Ending' in data.columns:
        # Plot target variable over time
        plt.figure(figsize=(12, 6))
        sns.lineplot(data=data, x='Week_Ending', y=target_var)
        plt.title(f"{target_var} Over Time")
        plt.xlabel("Date")
        plt.ylabel(f"{target_var} (Millions)") # Indicate units in label
        # Format y-axis to show values in millions
        plt.ticklabel_format(style='plain', axis='y') # Turn off scientific notation
        plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))
        plt.show()


        # Add seasonal decomposition
        print("Seasonal Decomposition:")
        try:
            # Ensure the data is sorted by date and set as index
            temp_df = data.set_index('Week_Ending').sort_index()
            # Use a smaller period if data is limited or seasonality is expected to be short
            # For weekly data, a period of 52 (for annual seasonality) or 4 (for monthly/quarterly) might be appropriate
            # Let's keep 4 as in the original code, assuming quarterly or shorter seasonality might be relevant
            decomposition = seasonal_decompose(temp_df[target_var], period=4, model='additive', extrapolate_trend='freq')

            fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)

            # Apply millions formatting to seasonal decomposition plots
            axes[0].plot(decomposition.observed)
            axes[0].set_ylabel("Observed (M)")
            axes[0].set_title("Seasonal Decomposition")
            axes[0].ticklabel_format(style='plain', axis='y')
            axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))


            axes[1].plot(decomposition.trend)
            axes[1].set_ylabel("Trend (M)")
            axes[1].ticklabel_format(style='plain', axis='y')
            axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))


            axes[2].plot(decomposition.seasonal)
            axes[2].set_ylabel("Seasonal (M)")
            axes[2].ticklabel_format(style='plain', axis='y')
            axes[2].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))


            axes[3].plot(decomposition.resid)
            axes[3].set_ylabel("Residual (M)")
            axes[3].ticklabel_format(style='plain', axis='y')
            axes[3].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))


            plt.xlabel("Date")
            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"Could not perform seasonal decomposition: {str(e)}")

    # 6. Correlation Analysis (Using Matplotlib/Seaborn)
    print("\n\n6. CORRELATION ANALYSIS")
    print("="*40)

    # Full correlation matrix
    print("Full Correlation Matrix:")
    corr = numeric_df.corr()
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", linewidths=".5")
    plt.title("Correlation Matrix - All Variables")
    plt.show()

    # Media variables correlation
    media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video']
    # Ensure media_cols are numeric before calculating correlation
    media_cols = [col for col in numeric_df.columns if any(keyword in col.lower() for keyword in media_keywords)]
    media_cols = [col for col in media_cols if col in numeric_df.columns] # Filter to ensure they are in numeric_df

    if media_cols and target_var in numeric_df.columns:
        print("Media Variables Correlation with Target:")
        # Ensure only numeric columns are included in correlation calculation
        media_corr = numeric_df[media_cols + [target_var]].corr()
        # Extract only correlations with target variable
        target_corr = media_corr[target_var].drop(target_var).sort_values(ascending=False)

        # Plot correlations as vertical bars
        plt.figure(figsize=(10, 6))
        sns.barplot(x=target_corr.index, y=target_corr.values)
        plt.title(f"Correlation of Media Variables with {target_var}")
        plt.xlabel("Media Variables")
        plt.ylabel("Correlation Coefficient")
        plt.xticks(rotation=-45, ha='left') # Rotate x-axis labels
        plt.tight_layout() # Adjust layout to prevent labels overlapping
        plt.show()

        # Also show as a table
        print(target_corr.round(3).to_frame("Correlation"))

        # 6.1 Media Execution Share - Change to Bar Chart (Using Matplotlib/Seaborn)
        print("\n\n6.1 MEDIA EXECUTION SHARE")
        print("="*40)

        # Calculate total media execution (sum of all media variables) - Ensure only numeric columns are summed
        media_totals = numeric_df[media_cols].sum()
        total_media = media_totals.sum()

        if total_media > 0:
            # Calculate share percentage
            media_share = (media_totals / total_media) * 100

            # Sort by share for better visualization
            media_share = media_share.sort_values(ascending=False)

            # Create bar chart instead of pie chart
            plt.figure(figsize=(10, 8))
            sns.barplot(x=media_share.index, y=media_share.values, palette='husl')
            plt.title("Media Execution Share by Channel")
            plt.xlabel("Channel")
            plt.ylabel("Share (%)")
            plt.xticks(rotation=-45, ha='left') # Rotate x-axis labels
            plt.tight_layout() # Adjust layout
            plt.show()

            # Display share percentages as table
            print("Media Execution Share Percentage:")
            display(media_share.round(2).to_frame("Share (%)"))
        else:
            print("Total media execution is 0. Cannot generate share chart.")

    # 7. Outlier Analysis (Using Matplotlib)
    print("\n\n7. OUTLIER ANALYSIS")
    print("="*40)

    # Check for outliers in target variable
    # Ensure target variable is numeric
    if target_var in numeric_df.columns:
        Q1 = data[target_var].quantile(0.25)
        Q3 = data[target_var].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Identify outliers
        outliers = data[(data[target_var] < lower_bound) | (data[target_var] > upper_bound)]
        normal_data = data[~((data[target_var] < lower_bound) | (data[target_var] > upper_bound))]
        print(f"Number of potential outliers in {target_var}: {len(outliers)}")

        if len(outliers) > 0:
            print("Outlier values:")
            display(outliers[['Week_Ending', target_var]])

            # Plot with outliers highlighted
            plt.figure(figsize=(12, 6))
            sns.lineplot(data=normal_data, x='Week_Ending', y=target_var, label='Normal Values', color='darkgreen', marker='o', markersize=5)
            sns.scatterplot(data=outliers, x='Week_Ending', y=target_var, color='red', label='Outliers', s=100, marker='X') # Use scatterplot for outliers

            plt.axhline(upper_bound, color='red', linestyle='dashed', linewidth=1, label='Upper Bound')
            plt.axhline(lower_bound, color='red', linestyle='dashed', linewidth=1, label='Lower Bound')

            plt.title(f"Outlier Detection in {target_var}")
            plt.xlabel("Date")
            plt.ylabel(f"{target_var} (Millions)") # Indicate units in label
            plt.legend()
            plt.ticklabel_format(style='plain', axis='y') # Turn off scientific notation
            plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # Format ticks in millions
            plt.show()
    else:
        print(f"Target variable '{target_var}' is not numeric. Cannot perform outlier analysis.")

    # Return data, numeric_df, and correlation matrix for interactive use
    # Ensure returned dataframes are copies to prevent unintentional modification outside the function
    return data.copy(), numeric_df.copy(), corr.copy()

# ================================================
# OPTIMIZED FEATURE ENGINEERING AND MEDIA PERFORMANCE REPORTING MODULE
# ================================================
import pandas as pd
import numpy as np
from statsmodels.tsa.seasonal import seasonal_decompose
from IPython.display import display, HTML
import warnings
warnings.filterwarnings('ignore')

# Define helper function for YoY formatting with colors and special cases
def format_yoy(yoy_val, base_val):
    """
    Formats YoY percentage with colors and handles special cases (NaN, Inf, 0).
    Checks base_val to differentiate between 0->0 and 0->Inf.
    """
    if pd.isna(yoy_val):
        return 'N/A'
    elif np.isinf(yoy_val):
        if pd.notna(base_val) and base_val == 0:
            return 'No previous data'
        else:
            return '<span style="color:green;">Inf%</span>'
    elif yoy_val == 0.0:
        return '0.00%'
    else:
        color = 'green' if yoy_val > 0 else 'red'
        formatted_val = f'{yoy_val:,.2f}%'
        return f'<span style="color:{color};">{formatted_val}</span>'

# Global dictionary to store original channel names
original_paid_media_mapping = {}

def feature_engineering_and_reporting_module(df):
    """
    Optimized feature engineering module for time series data.
    Includes media spend estimation and performance reporting.
    """
    print("\n===== FEATURE ENGG & MEDIA REPORTING MODULE =====")

    # Check for required columns
    required_cols = ['Week_Ending', 'Sales']
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"DataFrame must contain a '{col}' column")

    # Create a copy but avoid unnecessary deep copying
    df = df.copy(deep=False)

    # Track original columns at the start
    initial_columns = set(df.columns)

    # Ensure datetime index and sort - optimized
    if not pd.api.types.is_datetime64_any_dtype(df['Week_Ending']):
        df['Week_Ending'] = pd.to_datetime(df['Week_Ending'], errors='coerce')

    df = df.set_index('Week_Ending').sort_index()
    df.index.name = 'Week_Ending'  # Ensure index has proper name

    # Define constants
    TARGET = "Sales"
    WEEK_COL = "Week_Ending"

    # Define keywords for identifying engineered volume features
    engineered_volume_keywords = ['_pre', '_post', 'combined_var', 'super', '_volume']
    non_media_keywords = [TARGET, WEEK_COL, 'Discount', 'Total SKU', 'Gasoline Price', 'Average Price', 'SIndex', 'Holiday_']

    # --- MEDIA SPEND ESTIMATION ---
    print("\n--- MEDIA SPEND ESTIMATION ---")

    # Identify original paid media columns
    media_keywords_check = ['impressions', 'impr', 'clicks', 'social', 'search', 'video', 'tv', 'display', 'email', 'paid social']

    original_paid_media_cols = [
        col for col in initial_columns
        if any(keyword in col.lower() for keyword in media_keywords_check)
        and 'organic' not in col.lower()
        and not any(nmk.lower() in col.lower() for nmk in non_media_keywords)
        and col != TARGET and col != WEEK_COL
    ]

    # Store globally for later reference
    global original_paid_media_mapping
    original_paid_media_mapping = {col: col for col in original_paid_media_cols}

    print(f"\nIdentified Original Paid Media Channels for Spend Estimation: {original_paid_media_cols}")

    estimated_rates_yearly = {}
    estimated_spend_cols = []

    if original_paid_media_cols:
        print("\n===== ENTER ESTIMATED CPM/CPC RATES BY YEAR =====")
        print("Please provide the estimated rate for each original paid media channel, for each relevant year.")
        print("Rates should correspond to either Impressions (CPM) or Clicks (CPC).")
        print("Enter 0 if you don't have a rate for a specific channel/year.")

        # Get years present in the data - optimized
        years_in_data = sorted(df.index.year.unique().tolist())
        df['Fiscal_Year_Temp'] = df.index.year  # Temporary column for year mapping

        for channel in original_paid_media_cols:
            estimated_rates_yearly[channel] = {}

            # Determine metric type for prompting
            channel_lower = channel.lower()
            is_cpm_name = any(kw in channel_lower for kw in ['impressions', 'impr', 'video', 'tv', 'display'])
            is_cpc_name = 'clicks' in channel_lower

            prompt_metric_type = "Impressions (CPM - Cost Per 1000)" if is_cpm_name else \
                                "Clicks (CPC - Cost Per Click)" if is_cpc_name else \
                                "Volume (Enter appropriate rate, e.g., CPM/CPC)"

            for year in years_in_data:
                while True:
                    try:
                        # Check for predefined rates
                        predefined_rate_key = f'rate_{channel}_{year}'
                        ipython = get_ipython()
                        if ipython and predefined_rate_key in ipython.user_ns:
                            rate = ipython.user_ns[predefined_rate_key]
                            print(f"Using predefined rate for '{channel}' for FY {year}: {rate}")
                            estimated_rates_yearly[channel][year] = float(rate)
                            break

                        # Interactive input
                        rate_input = input(f"Enter rate for '{channel}' for FY {year} ({prompt_metric_type}): ").strip()
                        if not rate_input:
                            print(f"?? No rate entered for '{channel}' for FY {year}. Assuming rate is 0.")
                            estimated_rates_yearly[channel][year] = 0.0
                            break

                        rate = float(rate_input)
                        if rate < 0:
                            print("?? Rate cannot be negative. Please enter a positive number or 0.")
                            continue

                        estimated_rates_yearly[channel][year] = rate
                        print(f"? Rate stored for '{channel}' for FY {year}: {rate}")
                        break
                    except ValueError:
                        print("?? Invalid input. Please enter a numeric value (e.g., 0.5, 10.25).")
                    except EOFError:
                        print("\nInput ended. Skipping remaining rate inputs.")
                        raise

        # Calculate estimated spend columns for original channels - optimized vectorized approach
        print("\n===== CALCULATING ESTIMATED SPEND FOR ORIGINAL CHANNELS =====")

        for channel, rates_by_year in estimated_rates_yearly.items():
            if channel not in df.columns:
                print(f"?? Warning: Original channel column '{channel}' not found in DataFrame. Cannot calculate spend.")
                continue

            spend_col_name = f"{channel}_Spend"
            channel_lower = channel.lower()

            # Determine calculation type
            is_cpm_calc = any(kw in channel_lower for kw in ['impressions', 'impr', 'video', 'tv', 'display'])
            is_cpc_calc = 'clicks' in channel_lower

            if not is_cpm_calc and not is_cpc_calc:
                print(f"?? Warning: Channel '{channel}' type unclear for calculation. Skipping spend for this channel.")
                continue

            # Convert channel data to numeric, handling errors
            try:
                # First try to convert to numeric
                channel_data = pd.to_numeric(df[channel], errors='coerce')

                # Check if conversion resulted in many NaN values
                if channel_data.isna().sum() > 0:
                    print(f"?? Warning: Channel '{channel}' has non-numeric values. Converting to 0.")
                    channel_data = channel_data.fillna(0)

                # Create a rate mapping series for vectorized calculation
                rate_series = df['Fiscal_Year_Temp'].map(rates_by_year).fillna(0)

                # Calculate spend based on type
                if is_cpm_calc:
                    df[spend_col_name] = (channel_data / 1000) * rate_series
                elif is_cpc_calc:
                    df[spend_col_name] = channel_data * rate_series

                # Check if any spend was calculated
                if (df[spend_col_name] > 0).any():
                    estimated_spend_cols.append(spend_col_name)
                    print(f"  - Calculated {spend_col_name} using yearly rates.")
                else:
                    df.drop(columns=[spend_col_name], inplace=True)
                    print(f"  - No positive rates provided for '{channel}' across years. {spend_col_name} not added.")

            except Exception as e:
                print(f"? Error calculating spend for '{channel}': {e}")
                if spend_col_name in df.columns:
                    df.drop(columns=[spend_col_name], inplace=True)

        # Drop the temporary column
        if 'Fiscal_Year_Temp' in df.columns:
            df.drop(columns=['Fiscal_Year_Temp'], inplace=True)

        # Display head of the DataFrame with new spend columns
        if estimated_spend_cols:
            print("\n?? DataFrame head with new estimated spend columns for original channels:")
            display_cols = [col for col in df.columns if any(media_col in col for media_col in original_paid_media_cols)]
            display(df[display_cols].head())
        else:
            print("\n?? No estimated spend columns were created for original channels.")
    else:
        print("\n?? No Original Paid Media Channels identified for spend estimation.")

    # --- FEATURE ENGINEERING STEPS ---
    print("\n--- FEATURE ENGINEERING STEPS (Applied to Volume & Spend) ---")

    # 1. Seasonal Index (SIndex) - Interactive selection
    print("\n===== SEASONAL DECOMPOSITION =====")
    print("Select Seasonal Decomposition Period:")
    print("Options: [1] 4 (Quarterly), [2] 13 (Lunar/Quarterly), [3] 26 (Bi-annual), [4] 52 (Annual)")

    seasonal_period_options = {1: 4, 2: 13, 3: 26, 4: 52}
    selected_period = 52  # Default to annual

    try:
        period_choice = input(f"Enter choice (1-4) or press Enter for default ({selected_period}): ").strip()
        if period_choice and period_choice.isdigit():
            period_idx = int(period_choice)
            selected_period = seasonal_period_options.get(period_idx, selected_period)
    except (EOFError, Exception):
        pass  # Use default on any error

    print(f"? Selected seasonal decomposition period: {selected_period}.")

    # Apply seasonal decomposition with selected period
    if TARGET in df.columns:
        print("\nPerforming Seasonal Decomposition:")
        try:
            temp_series = df[TARGET].copy()

            # Handle missing values
            if temp_series.isna().any():
                temp_series = temp_series.fillna(method='ffill').fillna(method='bfill')

            # Perform seasonal decomposition
            decomposition = seasonal_decompose(temp_series, period=selected_period,
                                             model='additive', extrapolate_trend='freq')

            # Add seasonal component as SIndex
            df['SIndex'] = decomposition.seasonal.values
            print(f"? Seasonal Index (SIndex) created using period {selected_period}.")
        except Exception as e:
            print(f"? Could not perform seasonal decomposition: {str(e)}. SIndex not created.")
    else:
        print(f"?? Target variable '{TARGET}' not found. Cannot perform seasonal decomposition.")

    # 2. Custom Dummy Variables - Interactive creation
    print("\n===== CUSTOM DUMMY VARIABLES =====")
    custom_dummy_cols = []

    try:
        dummy_choice = input("Do you want to add custom dummy variables? (Y/N): ").strip().lower()
        if dummy_choice == 'y':
            print("\nEnter comma-separated dates for the custom dummy variables (YYYY-MM-DD).")
            print("For example: '2022-11-26,2023-11-26,2024-01-01'")
            print("Dummy variables will be automatically named based on dates (dummy1_2022-11-26, dummy2_2023-11-26, etc.)")

            dates_input = input("Enter comma-separated dates: ").strip()

            if dates_input:
                try:
                    # Parse dates with proper error handling - optimized
                    date_strings = [d.strip() for d in dates_input.split(',') if d.strip()]
                    date_objects = pd.to_datetime(date_strings, format='%Y-%m-%d', errors='coerce')
                    valid_dates = date_objects[date_objects.notna()]

                    if len(valid_dates) > 0:
                        # Precompute index dates for vectorized comparison
                        index_dates = df.index.normalize()  # Normalize to remove time component

                        for i, date_obj in enumerate(valid_dates, 1):
                            dummy_name = f"dummy{i}_{date_obj.strftime('%Y-%m-%d')}"
                            # Vectorized creation of dummy variable
                            df[dummy_name] = (index_dates == date_obj).astype(int)
                            custom_dummy_cols.append(dummy_name)
                            print(f"? Custom dummy '{dummy_name}' created for date: {date_obj.strftime('%Y-%m-%d')}.")
                    else:
                        print("?? No valid dates entered. Skipping custom dummy creation.")
                except Exception as e:
                    print(f"? An error occurred while creating dummies: {e}")
    except (EOFError, Exception):
        print("?? Skipping custom dummy creation due to input issue.")

    # 3. Split Variable (Apply to Volume AND Spend)
    try:
        split_choice = input("\nDo you want to split a variable at a date? (Y/N): ").strip().lower()
        if split_choice == 'y':
            print("\nAvailable variables for splitting:")

            # Exclude certain columns from split options
            exclude_from_split = ['SIndex', TARGET, 'Fiscal_Year'] + custom_dummy_cols
            all_vars_for_split = [
                col for col in df.columns
                if col not in exclude_from_split
                and not col.startswith('Holiday_')
                and '_Spend' not in col
            ]

            for i, var in enumerate(all_vars_for_split, 1):
                print(f"{i}. {var}")

            try:
                var_idx = int(input("Select variable number to split: ")) - 1
                if 0 <= var_idx < len(all_vars_for_split):
                    var_name = all_vars_for_split[var_idx]

                    # Check if this variable has a spend counterpart
                    spend_counterpart = f"{var_name}_Spend"
                    has_spend_counterpart = spend_counterpart in df.columns

                    split_date_str = input("Enter split date (YYYY-MM-DD): ").strip()

                    try:
                        split_dt = pd.to_datetime(split_date_str)

                        # Check if the split date is within the data's date range
                        if split_dt < df.index.min() or split_dt > df.index.max():
                            print(f"?? Split date {split_date_str} is outside the data range. Split may not be meaningful.")

                        # Ensure the original variable is numeric before splitting
                        if not pd.api.types.is_numeric_dtype(df[var_name]):
                            # Try to convert to numeric
                            df[var_name] = pd.to_numeric(df[var_name], errors='coerce').fillna(0)
                            print(f"?? Converted '{var_name}' to numeric for splitting.")

                        # Vectorized splitting
                        pre_mask = df.index <= split_dt

                        # Split the volume column
                        df[f"{var_name}_pre"] = df[var_name].where(pre_mask, 0)
                        df[f"{var_name}_post"] = df[var_name].where(~pre_mask, 0)

                        # If there is a spend counterpart, split that too
                        if has_spend_counterpart:
                            # Ensure spend column is numeric
                            if not pd.api.types.is_numeric_dtype(df[spend_counterpart]):
                                df[spend_counterpart] = pd.to_numeric(df[spend_counterpart], errors='coerce').fillna(0)
                                print(f"?? Converted '{spend_counterpart}' to numeric for splitting.")

                            df[f"{spend_counterpart}_pre"] = df[spend_counterpart].where(pre_mask, 0)
                            df[f"{spend_counterpart}_post"] = df[spend_counterpart].where(~pre_mask, 0)
                            print(f"? Split spend counterpart {spend_counterpart}")

                        # Drop the original variable(s)
                        cols_to_drop = [var_name]
                        if has_spend_counterpart and spend_counterpart in df.columns:
                            cols_to_drop.append(spend_counterpart)
                            if spend_counterpart in estimated_spend_cols:
                                estimated_spend_cols.remove(spend_counterpart)

                        df.drop(columns=cols_to_drop, inplace=True)
                        print(f"? Split {var_name} at {split_dt.date()} and dropped original variable(s)")
                    except ValueError:
                        print("?? Invalid date format. Please use YYYY-MM-DD.")
                else:
                    print("?? Invalid selection")
            except ValueError:
                print("?? Invalid input for variable number.")
    except (EOFError, Exception):
        print("?? Skipping variable splitting due to input issue.")

    # 4. Super Campaign (Apply to Volume AND Spend)
    try:
        super_choice = input("\nDo you want to create a super campaign? (Y/N): ").strip().lower()
        if super_choice == 'y':
            print("\nAvailable variables for combining:")

            # Exclude certain columns from super campaign options
            exclude_from_super = ['SIndex', TARGET, 'Fiscal_Year'] + custom_dummy_cols
            all_vars_for_super = [
                col for col in df.columns
                if col not in exclude_from_super
                and not col.startswith('Holiday_')
                and '_Spend' not in col
                and not col.endswith(('_pre', '_post'))
            ]

            for i, var in enumerate(all_vars_for_super, 1):
                print(f"{i}. {var}")

            try:
                selected = input("Enter variable numbers to combine (comma separated): ")
                var_indices = [
                    int(x.strip())-1 for x in selected.split(",")
                    if x.strip().isdigit() and 0 <= int(x.strip())-1 < len(all_vars_for_super)
                ]
                vars_to_combine = [all_vars_for_super[i] for i in var_indices]

                if vars_to_combine:
                    # Ask for custom name
                    name_choice = input("Do you want to provide a custom name for the Super Campaign? (Y/N): ").strip().lower()
                    if name_choice == 'y':
                        super_base_name = input("Enter the custom name (e.g., Super_Media): ").strip()
                        if not super_base_name:
                            super_base_name = "Super_Campaign"
                    else:
                        super_base_name = "Super_Campaign"

                    super_volume_col = f"{super_base_name}_Volume"
                    super_spend_col = f"{super_base_name}_Spend"

                    # Check for non-numeric variables and convert them
                    for var in vars_to_combine:
                        if not pd.api.types.is_numeric_dtype(df[var]):
                            df[var] = pd.to_numeric(df[var], errors='coerce').fillna(0)
                            print(f"?? Converted '{var}' to numeric for super campaign.")

                    # Check for non-numeric variables
                    non_numeric_vars = [v for v in vars_to_combine if not pd.api.types.is_numeric_dtype(df[v])]
                    if non_numeric_vars:
                        print(f"?? Cannot combine non-numeric variables: {non_numeric_vars}. Skipping Super Campaign.")
                    else:
                        # Check for spend counterparts
                        spend_vars_to_sum = [
                            f"{v}_Spend" for v in vars_to_combine
                            if f"{v}_Spend" in df.columns
                        ]

                        # Ensure spend columns are numeric
                        for spend_var in spend_vars_to_sum:
                            if not pd.api.types.is_numeric_dtype(df[spend_var]):
                                df[spend_var] = pd.to_numeric(df[spend_var], errors='coerce').fillna(0)
                                print(f"?? Converted '{spend_var}' to numeric for super campaign.")

                        # Create Super Campaign Volume
                        df[super_volume_col] = df[vars_to_combine].sum(axis=1)

                        # Create Super Campaign Spend if any spend variables exist
                        if spend_vars_to_sum:
                            df[super_spend_col] = df[spend_vars_to_sum].sum(axis=1)
                            estimated_spend_cols.append(super_spend_col)
                            print(f"? Created Super Campaign Spend: {super_spend_col}")

                        # Drop the original volume variables
                        vars_to_drop = [v for v in vars_to_combine if v in df.columns]
                        if vars_to_drop:
                            df.drop(columns=vars_to_drop, inplace=True)
                            print(f"? Dropped original volume variable(s): {vars_to_drop}")
                else:
                    print("?? No valid variables selected")
            except ValueError:
                print("?? Invalid input for variable numbers.")
    except (EOFError, Exception):
        print("?? Skipping super campaign creation due to input issue.")


    # --- MEDIA PERFORMANCE REPORTING ---
    print("\n\n===== MEDIA SPEND ESTIMATION & PERFORMANCE REPORTING =====")
    print("-" * 60)

    # Ensure 'Fiscal_Year' is in df for grouping
    if 'Fiscal_Year' not in df.columns:
        # Use assign_fiscal_year function
        if 'Week_Ending' in df.columns:
            df['Fiscal_Year'] = assign_fiscal_year(df.index) # Use the index for assigning FY
            print("? Created 'Fiscal_Year' column from index using assign_fiscal_year.")
        else:
             print("?? 'Week_Ending' not found. Cannot create 'Fiscal_Year' column.")


    # Identify columns for reporting
    media_keywords_check_reporting = ['impressions', 'impr', 'clicks', 'social', 'search', 'video', 'tv', 'display', 'email', 'paid social']
    non_media_keywords_reporting = [TARGET, WEEK_COL, 'Discount', 'Total SKU', 'Gasoline Price', 'Average Price', 'SIndex', 'Holiday_']

    # Get all volume media columns
    media_volume_reporting_candidates = [
        col for col in df.columns
        if any(keyword in col.lower() for keyword in media_keywords_check_reporting)
        and 'organic' not in col.lower()
        and col != TARGET and col != WEEK_COL
        and '_Spend' not in col
        and not any(nmk.lower() in col.lower() for nmk in non_media_keywords_reporting)
    ]

    # Add engineered volume features
    engineered_volume_reporting = [
        col for col in df.columns
        if any(keyword in col.lower() for keyword in engineered_volume_keywords)
        and '_Spend' not in col
    ]

    all_volume_media_cols_report = list(set(media_volume_reporting_candidates + engineered_volume_reporting))
    all_volume_media_cols_report = [col for col in all_volume_media_cols_report if col in df.columns]


    # Get all estimated spend media columns
    all_spend_media_cols_report = [col for col in df.columns if '_Spend' in col]
    aggregation_cols_report = all_volume_media_cols_report + all_spend_media_cols_report

    # Perform aggregation if we have columns to aggregate
    if aggregation_cols_report and 'Fiscal_Year' in df.columns:
        # Ensure all aggregation columns are numeric
        for col in aggregation_cols_report:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)


        # Filter to ensure only numeric columns are included
        numeric_aggregation_cols = [
            col for col in aggregation_cols_report
            if col in df.columns and pd.api.types.is_numeric_dtype(df[col])
        ]

        if numeric_aggregation_cols:
            # Group by Fiscal_Year and calculate sum
            yearly_aggregation = df.groupby('Fiscal_Year')[numeric_aggregation_cols].sum()
            print("\n? Data aggregated by Fiscal Year.")

            # Calculate year-over-year changes
            print("\n===== CALCULATING YEAR-OVER-YEAR (YoY) CHANGES =====")
            yoy_changes = pd.DataFrame(index=yearly_aggregation.columns)

            # Define years present in the aggregated data
            agg_years = yearly_aggregation.index.tolist()

            # Calculate YoY changes for available years
            # Iterate through years and calculate YoY for each available transition
            for i in range(1, len(agg_years)):
                year_prev = agg_years[i-1]
                year_curr = agg_years[i]

                val_prev = yearly_aggregation.loc[year_prev]
                val_curr = yearly_aggregation.loc[year_curr]

                # Vectorized YoY calculation
                with np.errstate(divide='ignore', invalid='ignore'):
                    yoy_pct = np.where(
                        val_prev != 0,
                        ((val_curr - val_prev) / val_prev) * 100,
                        np.where(val_curr != 0, np.inf, 0)
                    )

                yoy_changes[f'YoY % ({year_prev} vs {year_curr})'] = yoy_pct


            print("\n? Year-over-Year changes calculated.")

            # Generate tabular report
            print("\n===== MEDIA PERFORMANCE REPORT BY FISCAL YEAR =====")

            # Create a combined report DataFrame
            yearly_agg_transposed = yearly_aggregation.T
            yearly_agg_transposed.index.name = 'Metric'

            # Rename columns for clarity (using original fiscal years)
            yearly_agg_transposed = yearly_agg_transposed.rename(columns={year: f'FY {year}' for year in yearly_aggregation.index})

            # Reset index to turn 'Metric' into a column
            yearly_agg_transposed = yearly_agg_transposed.reset_index()

            # Split 'Metric' into 'Channel' and 'Type'
            yearly_agg_transposed['Channel'] = yearly_agg_transposed['Metric'].str.replace('_Spend', '')
            yearly_agg_transposed['Type'] = yearly_agg_transposed['Metric'].apply(
                lambda x: 'Estimated Spend' if '_Spend' in x else 'Execution'
            )

            # Drop the original 'Metric' column
            yearly_agg_transposed = yearly_agg_transposed.drop(columns=['Metric'])

            # Set 'Channel' and 'Type' as index
            yearly_agg_transposed = yearly_agg_transposed.set_index(['Channel', 'Type'])

            # Merge with yoy_changes
            yoy_changes.index.name = 'Metric' # Rename index before merging
            yoy_changes = yoy_changes.reset_index() # Reset index for merge

            # Split 'Metric' into 'Channel' and 'Type' for yoy_changes
            yoy_changes['Channel'] = yoy_changes['Metric'].str.replace(r'_Spend.*| \(.*', '', regex=True) # Corrected regex
            yoy_changes['Type'] = yoy_changes['Metric'].apply(
                lambda x: 'Estimated Spend' if '_Spend' in x else 'Execution'
            )

            # Drop the original 'Metric' column
            yoy_changes = yoy_changes.drop(columns=['Metric'])

            # Set 'Channel' and 'Type' as index
            yoy_changes = yoy_changes.set_index(['Channel', 'Type'])


            # Merge the two dataframes
            combined_report = yearly_agg_transposed.merge(
                yoy_changes, left_index=True, right_index=True, how='left'
            )


            # Define the desired column order dynamically
            report_columns_order = []
            # Add FY columns and their corresponding YoY columns
            for i in range(len(agg_years)):
                fy_col = f'FY {agg_years[i]}'
                if fy_col in combined_report.columns:
                    report_columns_order.append(fy_col)
                yoy_col_name = f'YoY % ({agg_years[i-1]} vs {agg_years[i]})' if i > 0 else None
                if yoy_col_name and yoy_col_name in combined_report.columns:
                    report_columns_order.append(yoy_col_name)


            # Reindex the columns to the desired order
            combined_report = combined_report.reindex(columns=report_columns_order)


            # Sort by Channel and then Type
            combined_report = combined_report.sort_index(level=['Channel', 'Type'], ascending=[True, False])


            # Format the numeric values and handle Inf/NaN/0 for YoY
            final_report_display = combined_report.copy()

            # Format currency/volume columns and YoY columns
            for col in final_report_display.columns:
                if 'YoY %' in col:
                    # Dynamically determine the base year column name from the YoY column name
                    import re
                    match = re.search(r'\((\d{4}) vs \d{4}\)', col)
                    base_year_str = match.group(1) if match else None
                    base_col_name = f'FY {base_year_str}' if base_year_str else None

                    if base_col_name and base_col_name in combined_report.columns:
                        # Apply formatting
                        formatted_col_data = []
                        for index_tuple in final_report_display.index:
                            base_val = combined_report.loc[index_tuple, base_col_name]
                            yoy_val = combined_report.loc[index_tuple, col]
                            formatted_col_data.append(format_yoy(yoy_val, base_val))
                        final_report_display[col] = formatted_col_data
                    else:
                        # Handle cases where base column is missing
                        final_report_display[col] = final_report_display[col].apply(
                            lambda yoy_val: format_yoy(yoy_val, np.nan) # Pass np.nan for base_val if not found
                        )

                else:
                    # Format as integer with commas
                    final_report_display[col] = final_report_display[col].apply(
                        lambda x: f"{x:,.0f}" if pd.notna(x) else 'N/A'
                    )


            # Display the final formatted report table
            print("\n?? Media Performance Report by Fiscal Year:")
            display(HTML(final_report_display.to_html(index=True, escape=False)))

        else:
            print("?? None of the identified aggregation columns are numeric.")
    elif 'Fiscal_Year' not in df.columns:
         print("?? 'Fiscal_Year' column not found. Cannot perform aggregation.")
    else:
        print("?? No Paid Media Volume or Estimated Spend columns found for aggregation.")


    # --- FINALIZED VARIABLES BY BUCKET (for Modeling) ---
    print("\n\n===== FINALIZED VARIABLES BY BUCKET (for Modeling) =====")
    print("="*50)

    # Get all current columns
    all_cols_final = df.columns.tolist()

    # Define buckets
    target_var_modeling = TARGET if TARGET in all_cols_final else None

    # Base/External Variables
    base_vars_modeling = ['Average Price', 'Gasoline Price', 'SIndex', 'Total SKU']
    holiday_vars = [col for col in all_cols_final if col.startswith('Holiday_')] + custom_dummy_cols
    base_vars_modeling.extend(holiday_vars)
    base_vars_modeling = list(set([col for col in base_vars_modeling if col in df.columns]))
    base_vars_modeling.sort()


    # Promotional Variables
    promo_vars_modeling = [col for col in all_cols_final if 'discount' in col.lower()]
    promo_vars_modeling = list(set([col for col in promo_vars_modeling if col in df.columns]))
    promo_vars_modeling.sort()

    # Media Variables (Volume/Execution only)
    media_volume_keywords = ['impressions', 'impr', 'clicks', 'social', 'search', 'video', 'tv', 'display', 'email', 'paid social']
    engineered_volume_keywords_check = ['_pre', '_post', 'super', '_volume'] # Use a slightly different name to avoid conflict

    media_vars_modeling = [
        col for col in all_cols_final
        if (any(keyword in col.lower() for keyword in media_volume_keywords) or
            any(keyword in col.lower() for keyword in engineered_volume_keywords_check)) # Use the new check name
        and '_Spend' not in col
        and col not in base_vars_modeling
        and col not in promo_vars_modeling
        and col != TARGET and col != WEEK_COL and col != 'Fiscal_Year'
    ]
    media_vars_modeling = [col for col in media_vars_modeling if col in df.columns]
    media_vars_modeling.sort()


    # Print variable buckets
    print("TARGET VARIABLE (for Modeling):")
    print(f"• {target_var_modeling}" if target_var_modeling else "• None")

    print("\nBase/External Variables:")
    for var in base_vars_modeling:
        print(f"• {var}")

    print("\nMedia Variables (Volume/Execution Only):")
    for var in media_vars_modeling:
        print(f"• {var}")

    print("\nPromotional Variables:")
    for var in promo_vars_modeling:
        print(f"• {var}")

    print("\nSeasonal Index:")
    print("• None (Included in Base Variables)") # SIndex is now in base_vars_modeling

    print("\nHoliday Dummies:")
    print("• None (Included in Base Variables)") # Holiday dummies are now in base_vars_modeling

    # Show additional features created
    # Combine all potential new feature categories
    potential_new_features = (
        [col for col in df.columns if '_Spend' in col] +
        ['SIndex'] +
        [col for col in df.columns if col.startswith('Holiday_')] +
        [col for col in df.columns if col.startswith('dummy')] +
        [col for col in df.columns if col.endswith(('_pre', '_post')) and '_Spend' not in col] +
        [col for col in df.columns if ('super' in col.lower() or 'combined_var' in col.lower() or '_volume' in col.lower()) and '_Spend' not in col] +
        ['Fiscal_Year']
    )

    # Filter to get only the new features present in the DataFrame that were not in initial columns
    new_features = [
        col for col in df.columns
        if col in potential_new_features and col not in initial_columns and col != TARGET and col != WEEK_COL
    ]

    new_features = list(set(new_features)) # Remove duplicates
    new_features = [col for col in new_features if col in df.columns] # Final check to ensure they exist

    if new_features:
        print("\n?? ADDITIONAL FEATURES CREATED (Volume, Spend, etc.):")
        print("=" * 50)

        # Create a summary table
        feature_info = []
        for col in new_features:
            feature_type = "Unknown"

            if '_Spend' in col:
                feature_type = "Estimated Spend"
            elif col == "SIndex":
                feature_type = "Seasonal Index"
            elif col.startswith("Holiday_"): # Covers both pre-defined and potentially loaded holiday columns
                feature_type = "Holiday Dummy"
            elif col.startswith('dummy'): # Covers custom dummy variables
                 feature_type = "Custom Dummy"
            elif col.endswith(('_pre', '_post')) and '_Spend' not in col:
                feature_type = "Split Variable (Volume)"
            elif ('super' in col.lower() or 'combined_var' in col.lower() or '_volume' in col.lower()) and '_Spend' not in col:
                feature_type = "Super Campaign (Volume)"
            elif col == 'Fiscal_Year':
                feature_type = 'Fiscal Year (Reporting/Grouping)'


            feature_info.append({
                "Feature Name": col,
                "Type": feature_type,
                "Data Type": str(df[col].dtype),
                "Non-Zero Values": f"{(df[col] != 0).sum()} / {len(df)}"
            })

        display(pd.DataFrame(feature_info))

        # Show sample of new features
        print("\n?? SAMPLE OF NEW FEATURES:")
        display(df[new_features].head())
    else:
        print("\n?? No additional features were created.")


    # --- FINAL MODELING VARIABLES ORGANIZATION ---
    print("\n" + "="*60)
    print("FINAL MODELING VARIABLES ORGANIZATION")
    print("="*60)

    # Create final variable buckets for modeling
    base_vars = ['Average Price', 'Gasoline Price', 'SIndex', 'Total SKU']
    # Add all dummy variables (holiday and custom)
    dummy_vars = [col for col in df.columns if col.startswith('Holiday_') or col.startswith('dummy')]
    base_vars.extend(dummy_vars)
    base_vars = [var for var in base_vars if var in df.columns]
    base_vars = list(set(base_vars)) # Ensure uniqueness
    base_vars.sort()

    # Promo variables: Discount variables
    promo_vars = [col for col in df.columns if 'discount' in col.lower()]
    promo_vars = [var for var in promo_vars if var in df.columns]
    promo_vars = list(set(promo_vars)) # Ensure uniqueness
    promo_vars.sort()


    # Media variables: Media volume variables only (excluding spend)
    media_keywords_modeling = ['impressions', 'impr', 'clicks', 'social', 'search', 'video', 'tv', 'display', 'email', 'paid social']
    engineered_keywords_modeling = ['_pre', '_post', 'super', '_volume']

    media_vars = [
        col for col in df.columns
        if (any(keyword in col.lower() for keyword in media_keywords_modeling) or
            any(keyword in col.lower() for keyword in engineered_keywords_modeling))
        and '_Spend' not in col
        and col not in base_vars # Exclude if already in base_vars
        and col not in promo_vars # Exclude if already in promo_vars
        and col != 'Sales' and col != 'Week_Ending' and col != 'Fiscal_Year'
    ]
    media_vars = [var for var in media_vars if var in df.columns]
    media_vars = list(set(media_vars)) # Ensure uniqueness
    media_vars.sort()


    # Target variable
    target_var = 'Sales' if 'Sales' in df.columns else None

    # Print the final organization
    print("\nFINAL VARIABLE BUCKETS FOR MODELING:")
    print("-" * 40)

    print("\nBASE VARIABLES:")
    if base_vars:
        for var in base_vars:
            print(f"• {var}")
    else:
        print("• None")

    print("\nPROMOTIONAL VARIABLES:")
    if promo_vars:
        for var in promo_vars:
            print(f"• {var}")
    else:
        print("• None")

    print("\nMEDIA VARIABLES:")
    if media_vars:
        for var in media_vars:
            print(f"• {var}")
    else:
        print("• None")

    print(f"\nTARGET VARIABLE:")
    print(f"• {target_var}" if target_var else "• None")

    # Create a new DataFrame with only the modeling variables
    modeling_vars = base_vars + promo_vars + media_vars
    if target_var:
        modeling_vars.append(target_var)

    # Filter modeling_vars to ensure all columns exist in the DataFrame before subsetting
    modeling_vars = [col for col in modeling_vars if col in df.columns]

    if modeling_vars:
        df_model = df[modeling_vars].copy()

        print(f"\n? Created modeling DataFrame with {len(modeling_vars)} variables:")
        print(f"   - Base variables: {len(base_vars)}")
        print(f"   - Promotional variables: {len(promo_vars)}")
        print(f"   - Media variables: {len(media_vars)}")
        print(f"   - Target variable: {1 if target_var else 0}")

        # Display the first few rows of the modeling DataFrame
        print("\n?? Modeling DataFrame (first 5 rows):")
        display(df_model.head())

        # Store the modeling buckets in a dictionary for easy access
        modeling_buckets = {
            'base_vars': base_vars,
            'promo_vars': promo_vars,
            'media_vars': media_vars,
            'target_var': target_var
        }

        # Store the modeling DataFrame in the global namespace
        get_ipython().user_ns['df_model'] = df_model
        get_ipython().user_ns['modeling_buckets'] = modeling_buckets

        print("\n? Modeling DataFrame stored as 'df_model'")
        print("? Modeling buckets dictionary stored as 'modeling_buckets'")
        print("\nYou can now use these variables for your model training:")
        print("   - df_model: Contains all modeling variables")
        print("   - modeling_buckets: Dictionary with variable lists for each bucket")
    else:
        print("\n?? No valid modeling variables found. Cannot create df_model.")


    return df

# Main execution block - Fixed to handle cases where data is not available
if __name__ == "__main__":
    try:
        # Try to get the IPython instance
        try:
            from IPython import get_ipython
            ipython = get_ipython()
            if ipython is not None:
                # Try to get data from IPython namespace
                data = ipython.user_ns.get('data', None)
                if data is None:
                    raise NameError("'data' DataFrame is not defined in the IPython namespace.")
            else:
                # Not in IPython environment
                raise NameError("Not running in IPython environment. 'data' DataFrame must be defined.")
        except ImportError:
            # IPython is not available
            raise NameError("IPython is not available. 'data' DataFrame must be defined in the global scope.")


        # Check if data is a valid DataFrame
        if not isinstance(data, pd.DataFrame) or data.empty:
            raise NameError("'data' is not a valid DataFrame or is empty.")

        # Run the feature engineering and reporting module
        df_features = feature_engineering_and_reporting_module(data)

        # Store the result back to IPython namespace if available
        if ipython is not None:
            ipython.user_ns['df_features'] = df_features
            print("\n? Final 'df_features' DataFrame stored in IPython namespace.")
        else:
            print("\n? Feature engineering completed successfully.")
            print("To use the result, assign it to a variable: df_features = feature_engineering_and_reporting_module(data)")


    except NameError as e:
        print(f"? ERROR: {e}")
        print("Please ensure you have loaded your data into a variable named 'data' before running this code.")
    except Exception as e:
        print(f"? An unexpected error occurred: {e}")
        raise

"""# Task
Explain the error in the selected code. If possible, fix the error and incorporate the changes into the existing code. Otherwise, try to diagnose the error.

## Load and preprocess data

### Subtask:
Load the dataset (assuming it's provided or available in the environment as a pandas DataFrame named `data`), convert Indian number format strings to floats, handle missing values (specifically in 'Paid Search Impressions'), and convert the 'Week_Ending' column to datetime objects.

**Reasoning**:
The subtask is to load and preprocess the data. The provided code already contains a function `load_and_preprocess_data` that performs these steps. I will call this function with the `data` DataFrame (assuming it's available in the environment) and then display the head and info of the resulting DataFrame to verify the preprocessing.
"""

processed_data = load_and_preprocess_data(data)
display(processed_data.head())
display(processed_data.info())

"""**Reasoning**:
The traceback indicates that the `load_and_preprocess_data` function is trying to read a CSV file using `pd.read_csv(file_path)`, but the `file_path` variable is `None`. This happens because the function is designed to take a file path as input, but the subtask instructions specify that the data is already available as a DataFrame named `data` in the environment. The function needs to be modified to work directly with the provided DataFrame instead of trying to read a file. I will modify the function to accept a DataFrame as input and perform the preprocessing steps directly on it.


"""

# Modified load and preprocess data function to accept a DataFrame
def load_and_preprocess_data_from_df(data):
    """Preprocess the marketing mix data from a DataFrame"""
    # Create a copy to avoid modifying the original DataFrame
    data = data.copy()

    # First, identify all columns that might contain Indian number format
    # These are all columns except the date column
    all_columns = data.columns.tolist()
    date_column = 'Week_Ending'

    if date_column in all_columns:
        all_columns.remove(date_column)

    # Convert all numeric columns (including Sales) - Optimized conversion
    # Apply conversion only to object columns that are likely numbers
    for col in all_columns:
        if data[col].dtype == 'object':
            # Use a more robust check for potential numbers before applying conversion
            try:
                 # Attempt to convert a sample to detect if it's numeric strings
                 data[col].sample(min(10, len(data))).astype(str).str.replace(',', '').astype(float)
                 # If the sample conversion works, apply to the whole column
                 data[col] = data[col].apply(convert_indian_number)
            except (ValueError, AttributeError):
                 # If sample conversion fails, it's likely not a numeric column with commas
                 pass

    # Handle missing values in Paid Search Impressions
    if 'Paid Search Impressions' in data.columns:
        missing_count = data['Paid Search Impressions'].isna().sum()
        if missing_count > 0:
            print(f"Found {missing_count} missing values in 'Paid Search Impressions'. Imputing with 0.")
            data['Paid Search Impressions'] = data['Paid Search Impressions'].fillna(0)

    # Convert date column
    if 'Week_Ending' in data.columns:
        # Use infer_datetime_format=True and cache=True for potentially faster conversion
        data['Week_Ending'] = pd.to_datetime(data['Week_Ending'], format='%d-%m-%Y %H:%M', errors='coerce', infer_datetime_format=True, cache=True)
        data = data.sort_values('Week_Ending').reset_index(drop=True)

    return data

# Now call the modified function with the 'data' DataFrame
processed_data = load_and_preprocess_data_from_df(data)

# Verify the initial steps of preprocessing
display(processed_data.head())
display(processed_data.info())

"""**Reasoning**:
The traceback indicates that the `data` variable passed to the `load_and_preprocess_data_from_df` function is `None`, which causes an `AttributeError` when calling `.copy()`. This means the `data` DataFrame is not available in the environment as expected. To fix this, I need to ensure the `data` DataFrame is loaded before calling the preprocessing function. I will add code to load the data from the specified file path (`/content/fix.txt`) into the `data` variable and then proceed with calling the preprocessing function and displaying the results.


"""

# Load the data from the file
file_path = '/content/fix.txt'
try:
    data = pd.read_csv(file_path)
    print(f"Data loaded successfully from {file_path}")

    # Now call the modified preprocessing function with the loaded 'data' DataFrame
    processed_data = load_and_preprocess_data_from_df(data)

    # Verify the initial steps of preprocessing
    display(processed_data.head())
    display(processed_data.info())

except FileNotFoundError:
    print(f"Error: The file was not found at {file_path}")
except Exception as e:
    print(f"An error occurred while loading or processing the data: {e}")

"""**Reasoning**:
The traceback indicates a `UnicodeDecodeError: 'utf-8' codec can't decode byte 0x95`. This usually means the CSV file is not encoded in UTF-8. I will try loading the CSV file using a different encoding, such as 'latin-1' or 'ISO-8859-1', which are common for files with non-ASCII characters. I will keep the rest of the code the same, calling the preprocessing function and displaying the head and info.


"""

# Load the data from the file with a different encoding
file_path = '/content/fix.txt'
try:
    # Try 'latin-1' encoding
    data = pd.read_csv(file_path, encoding='latin-1')
    print(f"Data loaded successfully from {file_path} with 'latin-1' encoding")

    # Now call the modified preprocessing function with the loaded 'data' DataFrame
    processed_data = load_and_preprocess_data_from_df(data)

    # Verify the initial steps of preprocessing
    display(processed_data.head())
    display(processed_data.info())

except FileNotFoundError:
    print(f"Error: The file was not found at {file_path}")
except Exception as e:
    print(f"An error occurred while loading or processing the data: {e}")

"""**Reasoning**:
The traceback indicates a `ParserError: Error tokenizing data. C error: Expected 1 fields in line 14, saw 2`. This suggests that the CSV file is not properly delimited, and the parser is expecting a single field but encountering two on line 14. This could be due to an incorrect separator (delimiter) being used. The default separator for `pd.read_csv` is a comma (`,`). Given the context of data potentially having Indian number formats with commas, it's possible the file uses a different delimiter, or that commas within fields are causing issues. I will try specifying a different delimiter, such as a semicolon (`;`) or tab (`\t`), and also explicitly handle potential parsing issues by using the `engine='python'` parameter, which can sometimes be more robust with irregular files, although it might be slower. I will also keep the `encoding='latin-1'` as that resolved the previous error.


"""

# Load the data from the file with a different encoding and delimiter
file_path = '/content/fix.txt'
try:
    # Try 'latin-1' encoding and semicolon delimiter
    data = pd.read_csv(file_path, encoding='latin-1', sep=';', engine='python')
    print(f"Data loaded successfully from {file_path} with 'latin-1' encoding and ';' delimiter")

    # Now call the modified preprocessing function with the loaded 'data' DataFrame
    processed_data = load_and_preprocess_data_from_df(data)

    # Verify the initial steps of preprocessing
    display(processed_data.head())
    display(processed_data.info())

except FileNotFoundError:
    print(f"Error: The file was not found at {file_path}")
except Exception as e:
    print(f"An error occurred while loading or processing the data: {e}")

# Original load and preprocess data function
def load_and_preprocess_data(file_path):
    """Load and preprocess the marketing mix data"""
    # Load the data with latin-1 encoding to handle character issues
    # Try different delimiters and quoting to handle the ParserError
    try:
        # Attempt 1: default comma delimiter (keeping latin-1 encoding)
        data = pd.read_csv(file_path, encoding='latin-1')
        print("Attempted loading with default comma delimiter and latin-1 encoding.")
        # Check if it seems reasonable (e.g., more than 1 column)
        if data.shape[1] > 1:
             print("Initial comma-separated load seems successful.")
             return data # If this works, return immediately
        else:
             print("Initial comma-separated load resulted in only one column, likely incorrect.")


    except pd.errors.ParserError as e:
        print(f"Comma delimiter failed: {e}. Trying semicolon delimiter.")
        try:
            # Attempt 2: semicolon delimiter
            data = pd.read_csv(file_path, encoding='latin-1', sep=';')
            print("Attempted loading with semicolon delimiter and latin-1 encoding.")
            if data.shape[1] > 1:
                 print("Semicolon-separated load seems successful.")
                 return data # If this works, return immediately
            else:
                 print("Semicolon-separated load resulted in only one column, likely incorrect.")
        except pd.errors.ParserError as e_semi:
             print(f"Semicolon delimiter failed: {e_semi}. Trying comma delimiter with quoting.")
             try:
                 # Attempt 3: comma delimiter with quoting disabled or minimal
                 # QUOTE_MINIMAL will quote fields with special characters
                 # QUOTE_NONE will ignore quoting
                 # Let's try QUOTE_MINIMAL first, then QUOTE_NONE if needed
                 import csv
                 data = pd.read_csv(file_path, encoding='latin-1', sep=',', quoting=csv.QUOTE_MINIMAL, engine='python') # Use python engine for quoting
                 print("Attempted loading with comma delimiter, QUOTE_MINIMAL, and python engine.")
                 if data.shape[1] > 1:
                      print("Quoted comma-separated load seems successful.")
                      return data # If this works, return immediately
                 else:
                      print("Quoted comma-separated load resulted in only one column, likely incorrect.")
             except pd.errors.ParserError as e_quote_min:
                  print(f"Quoted comma-separated load (QUOTE_MINIMAL) failed: {e_quote_min}. Trying QUOTE_NONE.")
                  try:
                      # Attempt 4: comma delimiter with quoting explicitly none
                      data = pd.read_csv(file_path, encoding='latin-1', sep=',', quoting=csv.QUOTE_NONE, engine='python')
                      print("Attempted loading with comma delimiter, QUOTE_NONE, and python engine.")
                      if data.shape[1] > 1:
                           print("QUOTE_NONE comma-separated load seems successful.")
                           return data # If this works, return immediately
                      else:
                           print("QUOTE_NONE comma-separated load resulted in only one column, likely incorrect.")
                  except pd.errors.ParserError as e_quote_none:
                       print(f"Quoted comma-separated load (QUOTE_NONE) failed: {e_quote_none}. Trying tab delimiter.")
                       try:
                           # Attempt 5: tab delimiter
                           data = pd.read_csv(file_path, encoding='latin-1', sep='\t')
                           print("Attempted loading with tab delimiter and latin-1 encoding.")
                           if data.shape[1] > 1:
                                print("Tab-separated load seems successful.")
                                return data # If this works, return immediately
                           else:
                                print("Tab-separated load resulted in only one column, likely incorrect.")
                       except pd.errors.ParserError as e_tab:
                           print(f"Tab delimiter failed: {e_tab}. All standard loading attempts failed.")
                           raise pd.errors.ParserError(f"Failed to parse CSV with multiple attempts. Last error: {e_tab}") from e_tab
             except Exception as e_other:
                  print(f"An unexpected error occurred during quoted comma load: {e_other}")
                  raise # Re-raise the exception if it's not a ParserError


    except Exception as e:
        # Catch any other exceptions during the first attempt
        print(f"An unexpected error occurred during initial load attempt: {e}")
        raise # Re-raise the exception if it's not a ParserError


    # If none of the attempts returned data, this part is reached after the final exception
    # This line should technically not be reached if the last attempt raises an exception,
    # but as a fallback, we can return None or raise a general error.
    raise pd.errors.ParserError("Failed to load data with any of the attempted methods.")


    # First, identify all columns that might contain Indian number format
    # These are all columns except the date column
    all_columns = data.columns.tolist()
    date_column = 'Week_Ending'

    if date_column in all_columns:
        all_columns.remove(date_column)

    # Convert all numeric columns (including Sales) - Optimized conversion
    # Apply conversion only to object columns that are likely numbers
    for col in all_columns:
        if data[col].dtype == 'object':
            # Use a more robust check for potential numbers before applying conversion
            try:
                 # Attempt to convert a sample to detect if it's numeric strings
                 data[col].sample(min(10, len(data))).astype(str).str.replace(',', '').astype(float)
                 # If the sample conversion works, apply to the whole column
                 data[col] = data[col].apply(convert_indian_number)
            except (ValueError, AttributeError):
                 # If sample conversion fails, it's likely not a numeric column with commas
                 pass


    # Handle missing values in Paid Search Impressions
    if 'Paid Search Impressions' in data.columns:
        missing_count = data['Paid Search Impressions'].isna().sum()
        if missing_count > 0:
            print(f"Found {missing_count} missing values in 'Paid Search Impressions'. Imputing with 0.")
            data['Paid Search Impressions'] = data['Paid Search Impressions'].fillna(0)

    # Convert date column
    if 'Week_Ending' in data.columns:
        # Use infer_datetime_format=True and cache=True for potentially faster conversion
        data['Week_Ending'] = pd.to_datetime(data['Week_Ending'], format='%d-%m-%Y %H:%M', errors='coerce', infer_datetime_format=True, cache=True)
        data = data.sort_values('Week_Ending').reset_index(drop=True)

    return data

# Specify the file path
file_path = '/content/fix.txt'

# Call the original load and preprocess data function with the file path
try:
    processed_data = load_and_preprocess_data(file_path)

    # Verify the initial steps of preprocessing
    display(processed_data.head())
    display(processed_data.info())

except FileNotFoundError:
    print(f"Error: The file was not found at {file_path}")
except Exception as e:
    print(f"An error occurred while loading or processing the data: {e}")

# Removed the __main__ block and main() call as this cell is for testing the function
# if __name__ == "__main__":
#     main()

"""## Explain the error in the selected code. if possible, fix the error and incorporate the changes into the existing code. otherwise, try to diagnose the error.

### Subtask:
Diagnose and explain the `NameError: name 'assign_fiscal_year' is not defined` that occurred during the execution of the feature engineering and reporting module.

**Reasoning**:
Examine the provided code to diagnose the `NameError` related to `assign_fiscal_year`. The error message indicates that the name `assign_fiscal_year` is not defined. I will look for where this function is called and check if it is defined anywhere in the provided code.
"""

# Search for the call to assign_fiscal_year
# Search for the definition of assign_fiscal_year

# The error occurs in the feature_engineering_and_reporting_module function
# Specifically, in the "MEDIA PERFORMANCE REPORTING" section:
# if 'Fiscal_Year' not in df.columns:
#     # Use assign_fiscal_year function
#     if 'Week_Ending' in df.columns:
#         df['Fiscal_Year'] = assign_fiscal_year(df.index) # Use the index for assigning FY
#         print("? Created 'Fiscal_Year' column from index using assign_fiscal_year.")
#     else:
#          print("?? 'Week_Ending' not found. Cannot create 'Fiscal_Year' column.")

# I will now search the entire provided code block for a function definition named 'assign_fiscal_year'.
# Based on the provided code, there is no function defined with the name 'assign_fiscal_year'.

print("Diagnosis of NameError: name 'assign_fiscal_year' is not defined")
print("-" * 60)
print("1. The error traceback indicates that the name 'assign_fiscal_year' was referenced before it was defined.")
print("2. The code calls the `assign_fiscal_year` function within the `feature_engineering_and_reporting_module` function to create a 'Fiscal_Year' column:")
print("   `df['Fiscal_Year'] = assign_fiscal_year(df.index)`")
print("3. Upon examining the provided code block, there is no function definition for `assign_fiscal_year` present.")
print("4. Therefore, when the code attempts to execute the line calling `assign_fiscal_year`, Python does not find a function with that name in the current scope, resulting in a `NameError`.")
print("5. To fix this error, the definition of the `assign_fiscal_year` function is required. This function is likely intended to calculate the fiscal year based on the provided date index.")
print("6. Without the definition of `assign_fiscal_year`, the code cannot create the 'Fiscal_Year' column, which is necessary for the subsequent media performance reporting section that groups data by fiscal year.")

"""## Perform comprehensive eda

### Subtask:
Perform comprehensive exploratory data analysis, including basic dataset information, summary statistics, univariate analysis (distribution plots), bivariate analysis (scatter plots against the target variable), time series analysis (plotting target variable over time and seasonal decomposition), correlation analysis (full matrix and media variable correlations), and outlier analysis for the target variable.

**Reasoning**:
The previous step diagnosed the error but could not fix it because the `assign_fiscal_year` function was missing. To proceed with the comprehensive EDA as requested in the current subtask, we need to address this missing function. Since the definition is not provided, I will define a placeholder function for `assign_fiscal_year` based on common fiscal year logic (e.g., starting from April 1st) to allow the code to run past the `NameError`. This will enable the `feature_engineering_and_reporting_module` to complete, which in turn is necessary because the `perform_comprehensive_eda` function was executed in the previous turn and returned an error, and the instructions say "Call the perform_comprehensive_eda function with the processed_data DataFrame (which should be available from the previous successful step)". The previous step's result was failure. So, I need to execute the feature engineering step first, assuming it will produce the `processed_data` DataFrame (or a similar one) that can then be used for EDA. After defining the placeholder function and running feature engineering, I will then call the `perform_comprehensive_eda` function as requested by the current subtask instructions.
"""

# Define a placeholder assign_fiscal_year function
# Assuming fiscal year starts from April 1st
def assign_fiscal_year(date_series):
    """Assigns a fiscal year to a date series, assuming FY starts April 1st."""
    # Ensure the input is a pandas Series or Index of datetime objects
    if not pd.api.types.is_datetime64_any_dtype(date_series):
        raise TypeError("Input must be a datetime Series or Index.")

    year = date_series.year
    month = date_series.month

    # If the month is April or later, the fiscal year is the current calendar year
    # If the month is before April, the fiscal year is the previous calendar year
    fiscal_year = np.where(month >= 4, year, year - 1)
    return fiscal_year

# Re-run the feature engineering and reporting module now that assign_fiscal_year is defined
# Assuming 'data' DataFrame is available from previous successful loading attempts,
# or that the load_and_preprocess_data function successfully loaded it.
# Let's assume 'processed_data' from the last successful loading attempt is available.
# If not, the load_and_preprocess_data function needs to be called first.

# Check if processed_data is available, otherwise load it again
if 'processed_data' not in globals() or processed_data is None:
    print("processed_data not found, attempting to load data again.")
    file_path = '/content/fix.txt'
    try:
        # Use the fixed load and preprocess function
        processed_data = load_and_preprocess_data(file_path)
        print("Data loaded and preprocessed successfully.")
    except Exception as e:
        print(f"Error loading data: {e}")
        # If loading fails, we cannot proceed
        processed_data = None # Ensure processed_data is None

if processed_data is not None:
    try:
        # Run the feature engineering and reporting module
        df_features = feature_engineering_and_reporting_module(processed_data)

        # Now perform the comprehensive EDA using the potentially enhanced DataFrame
        # The perform_comprehensive_eda function is already defined in the notebook.
        # It returns the data, numeric_df, and corr.
        data_eda, numeric_df_eda, corr_eda = perform_comprehensive_eda(df_features, target_var='Sales')

        # Store the results in the IPython namespace if needed for later steps
        get_ipython().user_ns['data_eda'] = data_eda
        get_ipython().user_ns['numeric_df_eda'] = numeric_df_eda
        get_ipython().user_ns['corr_eda'] = corr_eda
        print("\n? EDA results stored as data_eda, numeric_df_eda, and corr_eda.")

    except Exception as e:
        print(f"An error occurred during feature engineering or EDA: {e}")
else:
    print("Cannot proceed with feature engineering or EDA as processed_data is not available.")

"""## Perform feature engineering and reporting

### Subtask:
Perform feature engineering and reporting using the `feature_engineering_and_reporting_module` function on the `processed_data` DataFrame. This includes media spend estimation, interactive feature creation (Seasonal Index, Custom Dummies, Split Variables, Super Campaign), and media performance reporting. The output will be stored in `df_features`, `df_model`, and `modeling_buckets`.

**Reasoning**:
The previous attempt to perform feature engineering failed because the `processed_data` DataFrame was not available due to data loading errors. The `assign_fiscal_year` function has now been defined in the previous turn, resolving one potential issue. This step will attempt to re-run the `feature_engineering_and_reporting_module` using the `processed_data` DataFrame that is expected to be available from a successful previous step (though it failed in the history, we must assume the user will fix the data loading outside this turn or it was a transient error). The interactive prompts will need to be handled during execution.
"""

# Attempt to run the feature engineering and reporting module again.
# This relies on 'processed_data' being available and correctly formatted
# after the data loading and initial preprocessing step, and the
# 'assign_fiscal_year' function being defined.

print("Attempting to run feature engineering and reporting module...")

# Check if processed_data is available in the environment
if 'processed_data' in globals() and isinstance(processed_data, pd.DataFrame) and not processed_data.empty:
    try:
        # Call the feature engineering and reporting module
        # This function is interactive and will prompt for inputs
        df_features = feature_engineering_and_reporting_module(processed_data)

        # The function is designed to store df_model and modeling_buckets
        # in the user namespace, so we don't need to explicitly assign them here.

        print("\n? Feature engineering and reporting module executed.")
        print("? Check the output above for interactive prompts and report details.")
        print("? df_features, df_model, and modeling_buckets should now be available.")

    except Exception as e:
        print(f"? An error occurred during the feature engineering step: {e}")
        df_features = None # Ensure df_features is None if it fails

else:
    print("?? Error: 'processed_data' DataFrame not found, is empty, or is not a DataFrame.")
    print("?? Please ensure the data loading and initial preprocessing step was successful.")
    df_features = None # Ensure df_features is None if processed_data is missing

"""## Summary:

### Data Analysis Key Findings

*   The primary obstacle encountered throughout the process was the inability to successfully load the data from the specified file (`/content/fix.txt`) into a pandas DataFrame.
*   Initial attempts to load the data failed due to encoding issues (`UnicodeDecodeError`).
*   After resolving the encoding issue by using 'latin-1', subsequent attempts to load the data failed due to a persistent parsing error (`ParserError: Error tokenizing data. C error: Expected 1 fields in line 14, saw 2`), indicating a structural problem within the file itself that `pd.read_csv` could not handle with standard parameters.
*   A `NameError` occurred because the `assign_fiscal_year` function, required by the `feature_engineering_and_reporting_module`, was called but not defined in the provided code. While a placeholder function was defined to address this specific error, the upstream data loading failure prevented this fix from being fully tested in the subsequent steps.
*   Due to the failure in loading and preprocessing the data, the necessary `processed_data` DataFrame was never successfully created.
*   Consequently, the subsequent steps for comprehensive EDA and feature engineering/reporting could not be executed as they depended on the availability of the `processed_data` DataFrame.

### Insights or Next Steps

*   The input data file (`/content/fix.txt`) needs to be manually inspected to identify and correct the structural issue causing the parsing error on line 14.
*   Ensure that the complete definition of the `assign_fiscal_year` function is available and included in the code before attempting to run the feature engineering and reporting module again.

"""