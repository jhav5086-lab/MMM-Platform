# -*- coding: utf-8 -*-
"""mmm_app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12FqUvs80KRZeP9pJxOT1s7ChumJDuP_D
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.tsa.seasonal import seasonal_decompose # Import seasonal_decompose
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from scipy.optimize import curve_fit, nnls, minimize, Bounds, LinearConstraint # Import for optimization
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import io # Import the io module
import warnings
warnings.filterwarnings('ignore')

# Import for tick formatting in plots
import matplotlib.ticker as mticker

# Import display and HTML for rendering the media report table (though st.write(..., unsafe_allow_html=True) is preferred)
from IPython.display import display, HTML


# --- Custom Functions (Copy these from your notebook or import them if in a separate file) ---



# --- Auto-inserted functions from beta_mmm.py (cleaned) ---

def load_and_preprocess_data(file_path):
    """Load and preprocess the marketing mix data"""
    # Load the data
    data = pd.read_csv(file_path)

    # First, identify all columns that might contain Indian number format
    # These are all columns except the date column
    all_columns = data.columns.tolist()
    date_column = 'Week_Ending'

    if date_column in all_columns:
        all_columns.remove(date_column)

    # Convert all numeric columns (including Sales) - Optimized conversion
    # Apply conversion only to object columns that are likely numbers
    for col in all_columns:
        if data[col].dtype == 'object':
            # Use a more robust check for potential numbers before applying conversion
            try:
                 # Attempt to convert a sample to detect if it's numeric strings
                 data[col].sample(min(10, len(data))).astype(str).str.replace(',', '').astype(float)
                 # If the sample conversion works, apply to the whole column
                 data[col] = data[col].apply(convert_indian_number)
            except (ValueError, AttributeError):
                 # If sample conversion fails, it's likely not a numeric column with commas
                 pass


    # Handle missing values in Paid Search Impressions
    if 'Paid Search Impressions' in data.columns:
        missing_count = data['Paid Search Impressions'].isna().sum()
        if missing_count > 0:
            print(f"Found {missing_count} missing values in 'Paid Search Impressions'. Imputing with 0.")
            data['Paid Search Impressions'] = data['Paid Search Impressions'].fillna(0)

    # Convert date column
    if 'Week_Ending' in data.columns:
        # Use infer_datetime_format=True and cache=True for potentially faster conversion
        data['Week_Ending'] = pd.to_datetime(data['Week_Ending'], format='%d-%m-%Y %H:%M', errors='coerce', infer_datetime_format=True, cache=True)
        data = data.sort_values('Week_Ending').reset_index(drop=True)

    return data


# Perform comprehensive EDA

def perform_comprehensive_eda(data, target_var='Sales'):
    """Perform comprehensive exploratory data analysis"""
    print("="*60)
    print("COMPREHENSIVE EXPLORATORY DATA ANALYSIS")
    print("="*60)

    # 1. Basic Information
    print("\n1. BASIC DATASET INFORMATION")
    print("="*40)
    print(f"Shape: {data.shape}")
    print(f"Columns: {list(data.columns)}")
    if 'Week_Ending' in data.columns:
        print(f"Date Range: {data['Week_Ending'].min()} to {data['Week_Ending'].max()}")
    print(f"Missing Values: {data.isnull().sum().sum()}")

    # Check if target variable is numeric
    if data[target_var].dtype == 'object':
         print(f"\nWARNING: Target variable '{target_var}' is not numeric after preprocessing.")


    # 2. Summary Statistics
    print("\n\n2. SUMMARY STATISTICS")
    print("="*40)

    # Numeric variables summary
    numeric_df = data.select_dtypes(include=[np.number])
    print("Numeric Variables Summary:")
    display(numeric_df.describe())

    # Add skewness and kurtosis
    skewness = numeric_df.skew().to_frame('Skewness')
    kurtosis = numeric_df.kurtosis().to_frame('Kurtosis')
    stats_df = pd.concat([skewness, kurtosis], axis=1)
    print("\nSkewness and Kurtosis:")
    display(stats_df)

    # 3. Univariate Analysis (Using Matplotlib/Seaborn)
    print("\n\n3. UNIVARIATE ANALYSIS")
    print("="*40)

    # Create distribution plots for all numeric variables
    numeric_cols = numeric_df.columns.tolist()
    for col in numeric_cols:
        plt.figure(figsize=(10, 6))
        sns.histplot(data=data, x=col, kde=True, bins=30)
        plt.title(f"Distribution of {col}")
        plt.ylabel("Frequency")

        # Add vertical lines for mean and median with improved formatting
        mean_val = data[col].mean()
        median_val = data[col].median()

        # Conditional formatting for labels and axis ticks based on column name
        if 'impressions' in col.lower() or 'clicks' in col.lower():
            plt.xlabel(f"{col} (Millions)") # Indicate units in label
            mean_label = f'Mean: {mean_val/1e6:.2f}M' # Show in millions for labels
            median_label = f'Median: {median_val/1e6:.2f}M'
            plt.ticklabel_format(style='plain', axis='x') # Turn off scientific notation for x-axis
            # Manually format x-axis ticks for millions
            plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))

        elif 'discount' in col.lower():
             plt.xlabel(f"{col} (%)") # Indicate units in label
             mean_label = f'Mean: {mean_val*100:.2f}%' # Show as percentage for labels
             median_label = f'Median: {median_val*100:.2f}%'
             # Format x-axis ticks as percentages
             plt.gca().xaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0)) # xmax=1.0 because data is 0-1 range

        elif col == target_var:
             plt.xlabel(col)
             mean_label = f'Mean: {mean_val/1e6:.2f}M' # Show in millions for labels
             median_label = f'Median: {median_val/1e6:.2f}M'
             plt.ticklabel_format(style='plain', axis='x') # Turn off scientific notation for x-axis
             plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # Format ticks in millions
        else:
            plt.xlabel(col)
            mean_label = f'Mean: {mean_val:,.2f}' # General formatting with commas
            median_label = f'Median: {median_val:,.2f}'
            plt.ticklabel_format(style='plain', axis='x') # Turn off scientific notation for x-axis


        plt.axvline(mean_val, color='red', linestyle='dashed', linewidth=1, label=mean_label)
        plt.axvline(median_val, color='green', linestyle='dashed', linewidth=1, label=median_label)
        plt.legend()
        plt.show()


    # 4. Bivariate Analysis (Using Matplotlib/Seaborn)
    print("\n\n4. BIVARIATE ANALYSIS: RELATIONSHIP WITH TARGET VARIABLE")
    print("="*40)

    # Create scatter plots against target variable
    if target_var in numeric_cols:
        numeric_cols_for_scatter = numeric_cols.copy()
        numeric_cols_for_scatter.remove(target_var)

    for col in numeric_cols_for_scatter:
        plt.figure(figsize=(10, 6))
        sns.scatterplot(data=data, x=col, y=target_var)
        plt.title(f"{target_var} vs {col}")

        # Conditional formatting for x-axis label and potentially ticks
        if 'impressions' in col.lower() or 'clicks' in col.lower():
            plt.xlabel(f"{col} (Millions)") # Indicate units in label
            plt.ticklabel_format(style='plain', axis='x')
            plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # Format ticks in millions
        elif 'discount' in col.lower():
             plt.xlabel(f"{col} (%)") # Indicate units in label
             plt.gca().xaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0)) # Format ticks as percentage

        else:
            plt.xlabel(col)
            plt.ticklabel_format(style='plain', axis='x') # Turn off scientific notation for x-axis


        # Conditional formatting for y-axis label (target variable) and ticks
        if target_var in numeric_df.columns: # Ensure target variable is numeric before formatting
            plt.ylabel(f"{target_var} (Millions)")
            plt.ticklabel_format(style='plain', axis='y')
            plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # Format ticks in millions
        else:
             plt.ylabel(target_var) # No formatting if target is not numeric


        # Add correlation coefficient as text
        correlation = data[col].corr(data[target_var])
        # Position the text box based on formatted axis limits (more complex, so using a simple approach first)
        # Simple approach: Use relative positioning within the plot area
        plt.text(data[col].min() + (data[col].max() - data[col].min()) * 0.05,
                 data[target_var].max() - (data[target_var].max() - data[target_var].min()) * 0.05,
                 f"r = {correlation:.3f}",
                 fontsize=12, bbox=dict(facecolor='white', alpha=0.8))
        plt.show()


    # 5. Time Series Analysis (Using Matplotlib/Seaborn)
    print("\n\n5. TIME SERIES ANALYSIS")
    print("="*40)

    if 'Week_Ending' in data.columns:
        # Plot target variable over time
        plt.figure(figsize=(12, 6))
        sns.lineplot(data=data, x='Week_Ending', y=target_var)
        plt.title(f"{target_var} Over Time")
        plt.xlabel("Date")
        plt.ylabel(f"{target_var} (Millions)") # Indicate units in label
        # Format y-axis to show values in millions
        plt.ticklabel_format(style='plain', axis='y') # Turn off scientific notation
        plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))
        plt.show()


        # Add seasonal decomposition
        print("Seasonal Decomposition:")
        try:
            # Ensure the data is sorted by date and set as index
            temp_df = data.set_index('Week_Ending').sort_index()
            # Use a smaller period if data is limited or seasonality is expected to be short
            # For weekly data, a period of 52 (for annual seasonality) or 4 (for monthly/quarterly) might be appropriate
            # Let's keep 4 as in the original code, assuming quarterly or shorter seasonality might be relevant
            decomposition = seasonal_decompose(temp_df[target_var], period=4, model='additive', extrapolate_trend='freq')

            fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)

            # Apply millions formatting to seasonal decomposition plots
            axes[0].plot(decomposition.observed)
            axes[0].set_ylabel("Observed (M)")
            axes[0].set_title("Seasonal Decomposition")
            axes[0].ticklabel_format(style='plain', axis='y')
            axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))


            axes[1].plot(decomposition.trend)
            axes[1].set_ylabel("Trend (M)")
            axes[1].ticklabel_format(style='plain', axis='y')
            axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))


            axes[2].plot(decomposition.seasonal)
            axes[2].set_ylabel("Seasonal (M)")
            axes[2].ticklabel_format(style='plain', axis='y')
            axes[2].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))


            axes[3].plot(decomposition.resid)
            axes[3].set_ylabel("Residual (M)")
            axes[3].ticklabel_format(style='plain', axis='y')
            axes[3].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))


            plt.xlabel("Date")
            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"Could not perform seasonal decomposition: {str(e)}")

    # 6. Correlation Analysis (Using Matplotlib/Seaborn)
    print("\n\n6. CORRELATION ANALYSIS")
    print("="*40)

    # Full correlation matrix
    print("Full Correlation Matrix:")
    corr = numeric_df.corr()
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", linewidths=".5")
    plt.title("Correlation Matrix - All Variables")
    plt.show()

    # Media variables correlation
    media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video']
    # Ensure media_cols are numeric before calculating correlation
    media_cols = [col for col in numeric_df.columns if any(keyword in col.lower() for keyword in media_keywords)]
    media_cols = [col for col in media_cols if col in numeric_df.columns] # Filter to ensure they are in numeric_df


    if media_cols and target_var in numeric_df.columns:
        print("Media Variables Correlation with Target:")
        # Ensure only numeric columns are included in correlation calculation
        media_corr = numeric_df[media_cols + [target_var]].corr()
        # Extract only correlations with target variable
        target_corr = media_corr[target_var].drop(target_var).sort_values(ascending=False)

        # Plot correlations as vertical bars
        plt.figure(figsize=(10, 6))
        sns.barplot(x=target_corr.index, y=target_corr.values)
        plt.title(f"Correlation of Media Variables with {target_var}")
        plt.xlabel("Media Variables")
        plt.ylabel("Correlation Coefficient")
        plt.xticks(rotation=-45, ha='left') # Rotate x-axis labels
        plt.tight_layout() # Adjust layout to prevent labels overlapping
        plt.show()


        # Also show as a table
        print(target_corr.round(3).to_frame("Correlation"))

        # 6.1 Media Execution Share - Change to Bar Chart (Using Matplotlib/Seaborn)
        print("\n\n6.1 MEDIA EXECUTION SHARE")
        print("="*40)

        # Calculate total media execution (sum of all media variables) - Ensure only numeric columns are summed
        media_totals = numeric_df[media_cols].sum()
        total_media = media_totals.sum()

        if total_media > 0:
            # Calculate share percentage
            media_share = (media_totals / total_media) * 100

            # Sort by share for better visualization
            media_share = media_share.sort_values(ascending=False)

            # Create bar chart instead of pie chart
            plt.figure(figsize=(10, 8))
            sns.barplot(x=media_share.index, y=media_share.values, palette='husl')
            plt.title("Media Execution Share by Channel")
            plt.xlabel("Channel")
            plt.ylabel("Share (%)")
            plt.xticks(rotation=-45, ha='left') # Rotate x-axis labels
            plt.tight_layout() # Adjust layout
            plt.show()


            # Display share percentages as table
            print("Media Execution Share Percentage:")
            display(media_share.round(2).to_frame("Share (%)"))
        else:
            print("Total media execution is 0. Cannot generate share chart.")


    # 7. Outlier Analysis (Using Matplotlib)
    print("\n\n7. OUTLIER ANALYSIS")
    print("="*40)

    # Check for outliers in target variable
    # Ensure target variable is numeric
    if target_var in numeric_df.columns:
        Q1 = data[target_var].quantile(0.25)
        Q3 = data[target_var].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Identify outliers
        outliers = data[(data[target_var] < lower_bound) | (data[target_var] > upper_bound)]
        normal_data = data[~((data[target_var] < lower_bound) | (data[target_var] > upper_bound))]
        print(f"Number of potential outliers in {target_var}: {len(outliers)}")

        if len(outliers) > 0:
            print("Outlier values:")
            display(outliers[['Week_Ending', target_var]])

            # Plot with outliers highlighted
            plt.figure(figsize=(12, 6))
            sns.lineplot(data=normal_data, x='Week_Ending', y=target_var, label='Normal Values', color='darkgreen', marker='o', markersize=5)
            sns.scatterplot(data=outliers, x='Week_Ending', y=target_var, color='red', label='Outliers', s=100, marker='X') # Use scatterplot for outliers

            plt.axhline(upper_bound, color='red', linestyle='dashed', linewidth=1, label='Upper Bound')
            plt.axhline(lower_bound, color='red', linestyle='dashed', linewidth=1, label='Lower Bound')

            plt.title(f"Outlier Detection in {target_var}")
            plt.xlabel("Date")
            plt.ylabel(f"{target_var} (Millions)") # Indicate units in label
            plt.legend()
            plt.ticklabel_format(style='plain', axis='y') # Turn off scientific notation
            plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # Format ticks in millions
            plt.show()
    else:
        print(f"Target variable '{target_var}' is not numeric. Cannot perform outlier analysis.")


    # Return data, numeric_df, and correlation matrix for interactive use
    # Ensure returned dataframes are copies to prevent unintentional modification outside the function
    return data.copy(), numeric_df.copy(), corr.copy()


# Main execution
# Load and preprocess your data
data = load_and_preprocess_data(file_path)

# Define the target variable here
target_var = 'Sales'

# Perform comprehensive EDA and get the data, numeric dataframe and correlation matrix
data, numeric_df, correlations = perform_comprehensive_eda(data, target_var=target_var)

# Save data to global scope

print("\nEDA Complete.")

# ================================================
# OPTIMIZED FEATURE ENGINEERING AND MEDIA PERFORMANCE REPORTING MODULE
# ================================================
import pandas as pd
import numpy as np
from statsmodels.tsa.seasonal import seasonal_decompose
from IPython.display import display, HTML
import warnings
warnings.filterwarnings('ignore')

# Define helper function for YoY formatting with colors and special cases

def format_yoy(yoy_val, base_val):
    """
    Formats YoY percentage with colors and handles special cases (NaN, Inf, 0).
    Checks base_val to differentiate between 0->0 and 0->Inf.
    """
    if pd.isna(yoy_val):
        return 'N/A'
    elif np.isinf(yoy_val):
        if pd.notna(base_val) and base_val == 0:
            return 'No previous data'
        else:
            return '<span style="color:green;">Inf%</span>'
    elif yoy_val == 0.0:
        return '0.00%'
    else:
        color = 'green' if yoy_val > 0 else 'red'
        formatted_val = f'{yoy_val:,.2f}%'
        return f'<span style="color:{color};">{formatted_val}</span>'

# Global dictionary to store original channel names
original_paid_media_mapping = {}

def feature_engineering_and_reporting_module(df):
    """
    Optimized feature engineering module for time series data.
    Includes media spend estimation and performance reporting.
    """
    print("\n===== FEATURE ENGG & MEDIA REPORTING MODULE =====")

    # Check for required columns
    required_cols = ['Week_Ending', 'Sales']
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"DataFrame must contain a '{col}' column")

    # Create a copy but avoid unnecessary deep copying
    df = df.copy(deep=False)

    # Track original columns at the start
    initial_columns = set(df.columns)

    # Ensure datetime index and sort - optimized
    if not pd.api.types.is_datetime64_any_dtype(df['Week_Ending']):
        df['Week_Ending'] = pd.to_datetime(df['Week_Ending'], errors='coerce')

    df = df.set_index('Week_Ending').sort_index()
    df.index.name = 'Week_Ending'  # Ensure index has proper name

    # Define constants
    TARGET = "Sales"
    WEEK_COL = "Week_Ending"

    # Define keywords for identifying engineered volume features
    engineered_volume_keywords = ['_pre', '_post', 'combined_var', 'super', '_volume']
    non_media_keywords = [TARGET, WEEK_COL, 'Discount', 'Total SKU', 'Gasoline Price', 'Average Price', 'SIndex', 'Holiday_']

    # --- MEDIA SPEND ESTIMATION ---
    print("\n--- MEDIA SPEND ESTIMATION ---")

    # Identify original paid media columns
    media_keywords_check = ['impressions', 'impr', 'clicks', 'social', 'search', 'video', 'tv', 'display', 'email', 'paid social']

    original_paid_media_cols = [
        col for col in initial_columns
        if any(keyword in col.lower() for keyword in media_keywords_check)
        and 'organic' not in col.lower()
        and not any(nmk.lower() in col.lower() for nmk in non_media_keywords)
        and col != TARGET and col != WEEK_COL
    ]

    # Store globally for later reference
    global original_paid_media_mapping
    original_paid_media_mapping = {col: col for col in original_paid_media_cols}

    print(f"\nIdentified Original Paid Media Channels for Spend Estimation: {original_paid_media_cols}")

    estimated_rates_yearly = {}
    estimated_spend_cols = []

    if original_paid_media_cols:
        print("\n===== ENTER ESTIMATED CPM/CPC RATES BY YEAR =====")
        print("Please provide the estimated rate for each original paid media channel, for each relevant year.")
        print("Rates should correspond to either Impressions (CPM) or Clicks (CPC).")
        print("Enter 0 if you don't have a rate for a specific channel/year.")

        # Get years present in the data - optimized
        years_in_data = sorted(df.index.year.unique().tolist())
        df['Fiscal_Year_Temp'] = df.index.year  # Temporary column for year mapping

        for channel in original_paid_media_cols:
            estimated_rates_yearly[channel] = {}

            # Determine metric type for prompting
            channel_lower = channel.lower()
            is_cpm_name = any(kw in channel_lower for kw in ['impressions', 'impr', 'video', 'tv', 'display'])
            is_cpc_name = 'clicks' in channel_lower

            prompt_metric_type = "Impressions (CPM - Cost Per 1000)" if is_cpm_name else \
                                "Clicks (CPC - Cost Per Click)" if is_cpc_name else \
                                "Volume (Enter appropriate rate, e.g., CPM/CPC)"

            for year in years_in_data:
                while True:
                    try:
                        # Check for predefined rates
                        predefined_rate_key = f'rate_{channel}_{year}'
                        if ipython and predefined_rate_key in ipython.user_ns:
                            rate = ipython.user_ns[predefined_rate_key]
                            print(f"Using predefined rate for '{channel}' for FY {year}: {rate}")
                            estimated_rates_yearly[channel][year] = float(rate)
                            break

                        # Interactive input
                        rate_input = input(f"Enter rate for '{channel}' for FY {year} ({prompt_metric_type}): ").strip()
                        if not rate_input:
                            print(f"ℹ️ No rate entered for '{channel}' for FY {year}. Assuming rate is 0.")
                            estimated_rates_yearly[channel][year] = 0.0
                            break

                        rate = float(rate_input)
                        if rate < 0:
                            print("⚠️ Rate cannot be negative. Please enter a positive number or 0.")
                            continue

                        estimated_rates_yearly[channel][year] = rate
                        print(f"✅ Rate stored for '{channel}' for FY {year}: {rate}")
                        break
                    except ValueError:
                        print("⚠️ Invalid input. Please enter a numeric value (e.g., 0.5, 10.25).")
                    except EOFError:
                        print("\nInput ended. Skipping remaining rate inputs.")
                        raise

        # Calculate estimated spend columns for original channels - optimized vectorized approach
        print("\n===== CALCULATING ESTIMATED SPEND FOR ORIGINAL CHANNELS =====")

        for channel, rates_by_year in estimated_rates_yearly.items():
            if channel not in df.columns:
                print(f"⚠️ Warning: Original channel column '{channel}' not found in DataFrame. Cannot calculate spend.")
                continue

            spend_col_name = f"{channel}_Spend"
            channel_lower = channel.lower()

            # Determine calculation type
            is_cpm_calc = any(kw in channel_lower for kw in ['impressions', 'impr', 'video', 'tv', 'display'])
            is_cpc_calc = 'clicks' in channel_lower

            if not is_cpm_calc and not is_cpc_calc:
                print(f"⚠️ Warning: Channel '{channel}' type unclear for calculation. Skipping spend for this channel.")
                continue

            # Convert channel data to numeric, handling errors
            try:
                # First try to convert to numeric
                channel_data = pd.to_numeric(df[channel], errors='coerce')

                # Check if conversion resulted in many NaN values
                if channel_data.isna().sum() > 0:
                    print(f"⚠️ Warning: Channel '{channel}' has non-numeric values. Converting to 0.")
                    channel_data = channel_data.fillna(0)

                # Create a rate mapping series for vectorized calculation
                rate_series = df['Fiscal_Year_Temp'].map(rates_by_year).fillna(0)

                # Calculate spend based on type
                if is_cpm_calc:
                    df[spend_col_name] = (channel_data / 1000) * rate_series
                elif is_cpc_calc:
                    df[spend_col_name] = channel_data * rate_series

                # Check if any spend was calculated
                if (df[spend_col_name] > 0).any():
                    estimated_spend_cols.append(spend_col_name)
                    print(f"  - Calculated {spend_col_name} using yearly rates.")
                else:
                    df.drop(columns=[spend_col_name], inplace=True)
                    print(f"  - No positive rates provided for '{channel}' across years. {spend_col_name} not added.")

            except Exception as e:
                print(f"❌ Error calculating spend for '{channel}': {e}")
                if spend_col_name in df.columns:
                    df.drop(columns=[spend_col_name], inplace=True)

        # Drop the temporary column
        if 'Fiscal_Year_Temp' in df.columns:
            df.drop(columns=['Fiscal_Year_Temp'], inplace=True)

        # Display head of the DataFrame with new spend columns
        if estimated_spend_cols:
            print("\n📋 DataFrame head with new estimated spend columns for original channels:")
            display_cols = [col for col in df.columns if any(media_col in col for media_col in original_paid_media_cols)]
            display(df[display_cols].head())
        else:
            print("\nℹ️ No estimated spend columns were created for original channels.")
    else:
        print("\nℹ️ No Original Paid Media Channels identified for spend estimation.")

    # --- FEATURE ENGINEERING STEPS ---
    print("\n--- FEATURE ENGINEERING STEPS (Applied to Volume & Spend) ---")

    # 1. Seasonal Index (SIndex) - Interactive selection
    print("\n===== SEASONAL DECOMPOSITION =====")
    print("Select Seasonal Decomposition Period:")
    print("Options: [1] 4 (Quarterly), [2] 13 (Lunar/Quarterly), [3] 26 (Bi-annual), [4] 52 (Annual)")

    seasonal_period_options = {1: 4, 2: 13, 3: 26, 4: 52}
    selected_period = 52  # Default to annual

    try:
        period_choice = input(f"Enter choice (1-4) or press Enter for default ({selected_period}): ").strip()
        if period_choice and period_choice.isdigit():
            period_idx = int(period_choice)
            selected_period = seasonal_period_options.get(period_idx, selected_period)
    except (EOFError, Exception):
        pass  # Use default on any error

    print(f"✅ Selected seasonal decomposition period: {selected_period}.")

    # Apply seasonal decomposition with selected period
    if TARGET in df.columns:
        print("\nPerforming Seasonal Decomposition:")
        try:
            temp_series = df[TARGET].copy()

            # Handle missing values
            if temp_series.isna().any():
                temp_series = temp_series.fillna(method='ffill').fillna(method='bfill')

            # Perform seasonal decomposition
            decomposition = seasonal_decompose(temp_series, period=selected_period,
                                             model='additive', extrapolate_trend='freq')

            # Add seasonal component as SIndex
            df['SIndex'] = decomposition.seasonal.values
            print(f"✅ Seasonal Index (SIndex) created using period {selected_period}.")
        except Exception as e:
            print(f"❌ Could not perform seasonal decomposition: {str(e)}. SIndex not created.")
    else:
        print(f"⚠️ Target variable '{TARGET}' not found. Cannot perform seasonal decomposition.")

    # 2. Custom Dummy Variables - Interactive creation
    print("\n===== CUSTOM DUMMY VARIABLES =====")
    custom_dummy_cols = []

    try:
        dummy_choice = input("Do you want to add custom dummy variables? (Y/N): ").strip().lower()
        if dummy_choice == 'y':
            print("\nEnter comma-separated dates for the custom dummy variables (YYYY-MM-DD).")
            print("For example: '2022-11-26,2023-11-26,2024-01-01'")
            print("Dummy variables will be automatically named based on dates (dummy1_2022-11-26, dummy2_2023-11-26, etc.)")

            dates_input = input("Enter comma-separated dates: ").strip()

            if dates_input:
                try:
                    # Parse dates with proper error handling - optimized
                    date_strings = [d.strip() for d in dates_input.split(',') if d.strip()]
                    date_objects = pd.to_datetime(date_strings, format='%Y-%m-%d', errors='coerce')
                    valid_dates = date_objects[date_objects.notna()]

                    if len(valid_dates) > 0:
                        # Precompute index dates for vectorized comparison
                        index_dates = df.index.normalize()  # Normalize to remove time component

                        for i, date_obj in enumerate(valid_dates, 1):
                            dummy_name = f"dummy{i}_{date_obj.strftime('%Y-%m-%d')}"
                            # Vectorized creation of dummy variable
                            df[dummy_name] = (index_dates == date_obj).astype(int)
                            custom_dummy_cols.append(dummy_name)
                            print(f"✅ Custom dummy '{dummy_name}' created for date: {date_obj.strftime('%Y-%m-%d')}.")
                    else:
                        print("⚠️ No valid dates entered. Skipping custom dummy creation.")
                except Exception as e:
                    print(f"❌ An error occurred while creating dummies: {e}")
    except (EOFError, Exception):
        print("⚠️ Skipping custom dummy creation due to input issue.")

    # 3. Split Variable (Apply to Volume AND Spend)
    try:
        split_choice = input("\nDo you want to split a variable at a date? (Y/N): ").strip().lower()
        if split_choice == 'y':
            print("\nAvailable variables for splitting:")

            # Exclude certain columns from split options
            exclude_from_split = ['SIndex', TARGET, 'Fiscal_Year'] + custom_dummy_cols
            all_vars_for_split = [
                col for col in df.columns
                if col not in exclude_from_split
                and not col.startswith('Holiday_')
                and '_Spend' not in col
            ]

            for i, var in enumerate(all_vars_for_split, 1):
                print(f"{i}. {var}")

            try:
                var_idx = int(input("Select variable number to split: ")) - 1
                if 0 <= var_idx < len(all_vars_for_split):
                    var_name = all_vars_for_split[var_idx]

                    # Check if this variable has a spend counterpart
                    spend_counterpart = f"{var_name}_Spend"
                    has_spend_counterpart = spend_counterpart in df.columns

                    split_date_str = input("Enter split date (YYYY-MM-DD): ").strip()

                    try:
                        split_dt = pd.to_datetime(split_date_str)

                        # Check if the split date is within the data's date range
                        if split_dt < df.index.min() or split_dt > df.index.max():
                            print(f"⚠️ Split date {split_date_str} is outside the data range. Split may not be meaningful.")

                        # Ensure the original variable is numeric before splitting
                        if not pd.api.types.is_numeric_dtype(df[var_name]):
                            # Try to convert to numeric
                            df[var_name] = pd.to_numeric(df[var_name], errors='coerce').fillna(0)
                            print(f"⚠️ Converted '{var_name}' to numeric for splitting.")

                        # Vectorized splitting
                        pre_mask = df.index <= split_dt

                        # Split the volume column
                        df[f"{var_name}_pre"] = df[var_name].where(pre_mask, 0)
                        df[f"{var_name}_post"] = df[var_name].where(~pre_mask, 0)

                        # If there is a spend counterpart, split that too
                        if has_spend_counterpart:
                            # Ensure spend column is numeric
                            if not pd.api.types.is_numeric_dtype(df[spend_counterpart]):
                                df[spend_counterpart] = pd.to_numeric(df[spend_counterpart], errors='coerce').fillna(0)
                                print(f"⚠️ Converted '{spend_counterpart}' to numeric for splitting.")

                            df[f"{spend_counterpart}_pre"] = df[spend_counterpart].where(pre_mask, 0)
                            df[f"{spend_counterpart}_post"] = df[spend_counterpart].where(~pre_mask, 0)
                            print(f"✅ Split spend counterpart {spend_counterpart}")

                        # Drop the original variable(s)
                        cols_to_drop = [var_name]
                        if has_spend_counterpart and spend_counterpart in df.columns:
                            cols_to_drop.append(spend_counterpart)
                            if spend_counterpart in estimated_spend_cols:
                                estimated_spend_cols.remove(spend_counterpart)

                        df.drop(columns=cols_to_drop, inplace=True)
                        print(f"✅ Split {var_name} at {split_dt.date()} and dropped original variable(s)")
                    except ValueError:
                        print("⚠️ Invalid date format. Please use YYYY-MM-DD.")
                else:
                    print("⚠️ Invalid selection")
            except ValueError:
                print("⚠️ Invalid input for variable number.")
    except (EOFError, Exception):
        print("⚠️ Skipping variable splitting due to input issue.")

    # 4. Super Campaign (Apply to Volume AND Spend)
    try:
        super_choice = input("\nDo you want to create a super campaign? (Y/N): ").strip().lower()
        if super_choice == 'y':
            print("\nAvailable variables for combining:")

            # Exclude certain columns from super campaign options
            exclude_from_super = ['SIndex', TARGET, 'Fiscal_Year'] + custom_dummy_cols
            all_vars_for_super = [
                col for col in df.columns
                if col not in exclude_from_super
                and not col.startswith('Holiday_')
                and '_Spend' not in col
                and not col.endswith(('_pre', '_post'))
            ]

            for i, var in enumerate(all_vars_for_super, 1):
                print(f"{i}. {var}")

            try:
                selected = input("Enter variable numbers to combine (comma separated): ")
                var_indices = [
                    int(x.strip())-1 for x in selected.split(",")
                    if x.strip().isdigit() and 0 <= int(x.strip())-1 < len(all_vars_for_super)
                ]
                vars_to_combine = [all_vars_for_super[i] for i in var_indices]

                if vars_to_combine:
                    # Ask for custom name
                    name_choice = input("Do you want to provide a custom name for the Super Campaign? (Y/N): ").strip().lower()
                    if name_choice == 'y':
                        super_base_name = input("Enter the custom name (e.g., Super_Media): ").strip()
                        if not super_base_name:
                            super_base_name = "Super_Campaign"
                    else:
                        super_base_name = "Super_Campaign"

                    super_volume_col = f"{super_base_name}_Volume"
                    super_spend_col = f"{super_base_name}_Spend"

                    # Check for non-numeric variables and convert them
                    for var in vars_to_combine:
                        if not pd.api.types.is_numeric_dtype(df[var]):
                            df[var] = pd.to_numeric(df[var], errors='coerce').fillna(0)
                            print(f"⚠️ Converted '{var}' to numeric for super campaign.")

                    # Check for non-numeric variables
                    non_numeric_vars = [v for v in vars_to_combine if not pd.api.types.is_numeric_dtype(df[v])]
                    if non_numeric_vars:
                        print(f"⚠️ Cannot combine non-numeric variables: {non_numeric_vars}. Skipping Super Campaign.")
                    else:
                        # Check for spend counterparts
                        spend_vars_to_sum = [
                            f"{v}_Spend" for v in vars_to_combine
                            if f"{v}_Spend" in df.columns
                        ]

                        # Ensure spend columns are numeric
                        for spend_var in spend_vars_to_sum:
                            if not pd.api.types.is_numeric_dtype(df[spend_var]):
                                df[spend_var] = pd.to_numeric(df[spend_var], errors='coerce').fillna(0)
                                print(f"⚠️ Converted '{spend_var}' to numeric for super campaign.")

                        # Create Super Campaign Volume
                        df[super_volume_col] = df[vars_to_combine].sum(axis=1)

                        # Create Super Campaign Spend if any spend variables exist
                        if spend_vars_to_sum:
                            df[super_spend_col] = df[spend_vars_to_sum].sum(axis=1)
                            estimated_spend_cols.append(super_spend_col)
                            print(f"✅ Created Super Campaign Spend: {super_spend_col}")

                        # Drop the original volume variables
                        vars_to_drop = [v for v in vars_to_combine if v in df.columns]
                        if vars_to_drop:
                            df.drop(columns=vars_to_drop, inplace=True)
                            print(f"✅ Dropped original volume variable(s): {vars_to_drop}")
                else:
                    print("⚠️ No valid variables selected")
            except ValueError:
                print("⚠️ Invalid input for variable numbers.")
    except (EOFError, Exception):
        print("⚠️ Skipping super campaign creation due to input issue.")

    # --- MEDIA PERFORMANCE REPORTING ---
    print("\n\n===== MEDIA SPEND ESTIMATION & PERFORMANCE REPORTING =====")
    print("-" * 60)

    # Ensure 'Fiscal_Year' is in df for grouping
    if 'Fiscal_Year' not in df.columns:
        df['Fiscal_Year'] = df.index.year
        print("✅ Created 'Fiscal_Year' column from index.")

    # Identify columns for reporting
    media_keywords_check_reporting = ['impressions', 'impr', 'clicks', 'social', 'search', 'video', 'tv', 'display', 'email', 'paid social']
    non_media_keywords_reporting = [TARGET, WEEK_COL, 'Discount', 'Total SKU', 'Gasoline Price', 'Average Price', 'SIndex', 'Holiday_']

    # Get all volume media columns
    media_volume_reporting_candidates = [
        col for col in df.columns
        if any(keyword in col.lower() for keyword in media_keywords_check_reporting)
        and 'organic' not in col.lower()
        and col != TARGET and col != WEEK_COL
        and '_Spend' not in col
        and not any(nmk.lower() in col.lower() for nmk in non_media_keywords_reporting)
    ]

    # Add engineered volume features
    engineered_volume_reporting = [
        col for col in df.columns
        if any(keyword in col.lower() for keyword in engineered_volume_keywords)
        and '_Spend' not in col
    ]

    all_volume_media_cols_report = list(set(media_volume_reporting_candidates + engineered_volume_reporting))
    all_volume_media_cols_report = [col for col in all_volume_media_cols_report if col in df.columns]

    # Get all estimated spend media columns
    all_spend_media_cols_report = [col for col in df.columns if '_Spend' in col]
    aggregation_cols_report = all_volume_media_cols_report + all_spend_media_cols_report

    # Perform aggregation if we have columns to aggregate
    if aggregation_cols_report:
        # Ensure all aggregation columns are numeric
        for col in aggregation_cols_report:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

        # Filter to ensure only numeric columns are included
        numeric_aggregation_cols = [
            col for col in aggregation_cols_report
            if col in df.columns and pd.api.types.is_numeric_dtype(df[col])
        ]

        if numeric_aggregation_cols:
            # Group by Fiscal_Year and calculate sum
            yearly_aggregation = df.groupby('Fiscal_Year')[numeric_aggregation_cols].sum()
            print("\n✅ Data aggregated by Fiscal Year.")

            # Calculate year-over-year changes
            print("\n===== CALCULATING YEAR-OVER-YEAR (YoY) CHANGES =====")
            yoy_changes = pd.DataFrame(index=yearly_aggregation.columns)

            # Define years present in the aggregated data
            agg_years = yearly_aggregation.index.tolist()

            # Calculate YoY changes for available years
            if 2022 in agg_years and 2023 in agg_years:
                val_2022 = yearly_aggregation.loc[2022]
                val_2023 = yearly_aggregation.loc[2023]

                # Vectorized YoY calculation
                with np.errstate(divide='ignore', invalid='ignore'):
                    yoy_22_23 = np.where(
                        val_2022 != 0,
                        ((val_2023 - val_2022) / val_2022) * 100,
                        np.where(val_2023 != 0, np.inf, 0)
                    )

                yoy_changes['FY22_to_FY23_YoY_Pct'] = yoy_22_23

            if 2023 in agg_years and 2024 in agg_years:
                val_2023 = yearly_aggregation.loc[2023]
                val_2024 = yearly_aggregation.loc[2024]

                # Vectorized YoY calculation
                with np.errstate(divide='ignore', invalid='ignore'):
                    yoy_23_24 = np.where(
                        val_2023 != 0,
                        ((val_2024 - val_2023) / val_2023) * 100,
                        np.where(val_2024 != 0, np.inf, 0)
                    )

                yoy_changes['FY23_to_FYTD24_YoY_Pct'] = yoy_23_24

            print("\n✅ Year-over-Year changes calculated.")

            # Generate tabular report
            print("\n===== MEDIA PERFORMANCE REPORT BY FISCAL YEAR =====")

            # Create a combined report DataFrame
            yearly_agg_transposed = yearly_aggregation.T
            yearly_agg_transposed.index.name = 'Metric'

            # Rename columns for clarity
            year_cols_map = {year: f'FY {year}' for year in yearly_aggregation.index}
            if 2024 in year_cols_map:
                year_cols_map[2024] = 'FYTD 2024'
            yearly_agg_transposed = yearly_agg_transposed.rename(columns=year_cols_map)

            # Reset index to turn 'Metric' into a column
            yearly_agg_transposed = yearly_agg_transposed.reset_index()

            # Split 'Metric' into 'Channel' and 'Type'
            yearly_agg_transposed['Channel'] = yearly_agg_transposed['Metric'].str.replace('_Spend', '')
            yearly_agg_transposed['Type'] = yearly_agg_transposed['Metric'].apply(
                lambda x: 'Estimated Spend' if '_Spend' in x else 'Execution'
            )

            # Drop the original 'Metric' column
            yearly_agg_transposed = yearly_agg_transposed.drop(columns=['Metric'])

            # Set 'Channel' and 'Type' as index
            yearly_agg_transposed = yearly_agg_transposed.set_index(['Channel', 'Type'])

            # Merge with yoy_changes
            yoy_changes.index.name = 'Metric'
            yoy_changes = yoy_changes.reset_index()

            # Split 'Metric' into 'Channel' and 'Type'
            yoy_changes['Channel'] = yoy_changes['Metric'].str.replace(
                r'_Spend_.*|_.*_YoY_Pct', '', regex=True
            )
            yoy_changes['Type'] = yoy_changes['Metric'].apply(
                lambda x: 'Estimated Spend' if '_Spend' in x else 'Execution'
            )

            # Drop the original 'Metric' column
            yoy_changes = yoy_changes.drop(columns=['Metric'])

            # Set 'Channel' and 'Type' as index
            yoy_changes = yoy_changes.set_index(['Channel', 'Type'])

            # Merge the two dataframes
            combined_report = yearly_agg_transposed.merge(
                yoy_changes, left_index=True, right_index=True, how='left'
            )

            # Define the desired column order
            report_columns_order = []
            available_fy_cols = [col for col in ['FY 2022', 'FY 2023', 'FYTD 2024'] if col in combined_report.columns]
            report_columns_order.extend(available_fy_cols)

            available_yoy_cols = [
                col for col in ['FY22_to_FY23_YoY_Pct', 'FY23_to_FYTD24_YoY_Pct']
                if col in combined_report.columns
            ]

            # Insert YoY columns after their corresponding FY columns
            if 'FY 2023' in report_columns_order and 'FY22_to_FY23_YoY_Pct' in available_yoy_cols:
                report_columns_order.insert(report_columns_order.index('FY 2023') + 1, 'FY22_to_FY23_YoY_Pct')
            if 'FYTD 2024' in report_columns_order and 'FY23_to_FYTD24_YoY_Pct' in available_yoy_cols:
                report_columns_order.insert(report_columns_order.index('FYTD 2024') + 1, 'FY23_to_FYTD24_YoY_Pct')

            # Reindex the columns to the desired order
            combined_report = combined_report.reindex(columns=report_columns_order)

            # Sort by Channel and then Type
            combined_report = combined_report.sort_index(level=['Channel', 'Type'], ascending=[True, False])

            # Format the numeric values and handle Inf/NaN/0 for YoY
            final_report_display = combined_report.copy()

            # Format currency/volume columns
            for col in final_report_display.columns:
                if 'YoY_Pct' in col:
                    # Determine the base year column name
                    base_col_name = 'FY 2022' if 'FY22_to_FY23_YoY_Pct' in col else 'FY 2023'

                    if base_col_name in combined_report.columns:
                        # Apply formatting
                        formatted_col_data = []
                        for index_tuple in final_report_display.index:
                            base_val = combined_report.loc[index_tuple, base_col_name]
                            yoy_val = combined_report.loc[index_tuple, col]
                            formatted_col_data.append(format_yoy(yoy_val, base_val))

                        final_report_display[col] = formatted_col_data
                else:
                    # Format as integer with commas
                    final_report_display[col] = final_report_display[col].apply(
                        lambda x: f"{x:,.0f}" if pd.notna(x) else 'N/A'
                    )

            # Rename YoY percentage columns
            final_report_display = final_report_display.rename(columns={
                'FY22_to_FY23_YoY_Pct': 'YoY % (FY22 vs FY23)',
                'FY23_to_FYTD24_YoY_Pct': 'YoY % (FY23 vs FYTD24)'
            })

            # Display the final formatted report table
            print("\n📋 Media Performance Report by Fiscal Year:")
            display(HTML(final_report_display.to_html(index=True, escape=False)))
        else:
            print("⚠️ None of the identified aggregation columns are numeric.")
    else:
        print("⚠️ No Paid Media Volume or Estimated Spend columns found for aggregation.")

    # --- FINALIZED VARIABLES BY BUCKET (for Modeling) ---
    print("\n\n===== FINALIZED VARIABLES BY BUCKET (for Modeling) =====")
    print("-" * 50)

    # Get all current columns
    all_cols_final = df.columns.tolist()

    # Define buckets
    target_var_modeling = TARGET if TARGET in all_cols_final else None

    # Base/External Variables
    base_vars_modeling = ['Average Price', 'Gasoline Price', 'SIndex', 'Total SKU']
    holiday_vars = [col for col in all_cols_final if col.startswith('Holiday_')] + custom_dummy_cols
    base_vars_modeling.extend(holiday_vars)
    base_vars_modeling = list(set([col for col in base_vars_modeling if col in df.columns]))
    base_vars_modeling.sort()

    # Promotional Variables
    promo_vars_modeling = [col for col in all_cols_final if 'discount' in col.lower()]
    promo_vars_modeling = list(set([col for col in promo_vars_modeling if col in df.columns]))
    promo_vars_modeling.sort()

    # Media Variables (Volume/Execution only)
    media_volume_keywords = ['impressions', 'impr', 'clicks', 'social', 'search', 'video', 'tv', 'display', 'email', 'paid social']
    media_vars_modeling = [
        col for col in all_cols_final
        if (any(keyword in col.lower() for keyword in media_volume_keywords) or
            any(keyword in col.lower() for keyword in engineered_volume_keywords))
        and '_Spend' not in col
        and col not in base_vars_modeling
        and col not in promo_vars_modeling
        and col != TARGET and col != WEEK_COL and col != 'Fiscal_Year'
    ]
    media_vars_modeling = [col for col in media_vars_modeling if col in df.columns]
    media_vars_modeling.sort()

    # Print variable buckets
    print("TARGET VARIABLE (for Modeling):")
    print(f"• {target_var_modeling}" if target_var_modeling else "• None")

    print("\nBase/External Variables:")
    for var in base_vars_modeling:
        print(f"• {var}")

    print("\nMedia Variables (Volume/Execution Only):")
    for var in media_vars_modeling:
        print(f"• {var}")

    print("\nPromotional Variables:")
    for var in promo_vars_modeling:
        print(f"• {var}")

    print("\nSeasonal Index:")
    print("• None (Included in Base Variables)")

    print("\nHoliday Dummies:")
    print("• None (Included in Base Variables)")

    # Show additional features created
    new_features = [
        col for col in df.columns
        if col not in initial_columns
        and col != TARGET
        and col != WEEK_COL
    ]
    new_features = list(set(new_features + ['SIndex', 'Fiscal_Year'] + custom_dummy_cols))
    new_features = [col for col in new_features if col in df.columns]

    if new_features:
        print("\n📊 ADDITIONAL FEATURES CREATED (Volume, Spend, etc.):")
        print("=" * 50)

        # Create a summary table
        feature_info = []
        for col in new_features:
            feature_type = "Unknown"

            if '_Spend' in col:
                feature_type = "Estimated Spend"
            elif col == "SIndex":
                feature_type = "Seasonal Index"
            elif col.startswith("Holiday_") and col not in custom_dummy_cols:
                feature_type = "Holiday Dummy (Pre-defined)"
            elif col in custom_dummy_cols:
                feature_type = "Custom Dummy"
            elif col.endswith(('_pre', '_post')) and '_Spend' not in col:
                feature_type = "Split Variable (Volume)"
            elif ('super' in col.lower() or 'combined_var' in col.lower() or '_volume' in col.lower()) and '_Spend' not in col:
                feature_type = "Super Campaign (Volume)"
            elif col == 'Fiscal_Year':
                feature_type = 'Fiscal Year (Reporting/Grouping)'

            feature_info.append({
                "Feature Name": col,
                "Type": feature_type,
                "Data Type": str(df[col].dtype),
                "Non-Zero Values": f"{(df[col] != 0).sum()} / {len(df)}"
            })

        display(pd.DataFrame(feature_info))

        # Show sample of new features
        print("\n📋 SAMPLE OF NEW FEATURES:")
        display(df[new_features].head())
    else:
        print("\nℹ️ No additional features were created.")

    # --- FINAL MODELING VARIABLES ORGANIZATION ---
    print("\n" + "="*60)
    print("FINAL MODELING VARIABLES ORGANIZATION")
    print("="*60)

    # Create final variable buckets for modeling
    base_vars = ['Average Price', 'Gasoline Price', 'SIndex', 'Total SKU']
    # Add all dummy variables (holiday and custom)
    dummy_vars = [col for col in df.columns if col.startswith('Holiday_') or col.startswith('dummy')]
    base_vars.extend(dummy_vars)
    base_vars = [var for var in base_vars if var in df.columns]
    base_vars.sort()

    # Promo variables: Discount variables
    promo_vars = [col for col in df.columns if 'discount' in col.lower()]
    promo_vars = [var for var in promo_vars if var in df.columns]
    promo_vars.sort()

    # Media variables: Media volume variables only (excluding spend)
    media_keywords = ['impressions', 'impr', 'clicks', 'social', 'search', 'video', 'tv', 'display', 'email', 'paid social']
    engineered_keywords = ['_pre', '_post', 'super', '_volume']

    media_vars = [
        col for col in df.columns
        if (any(keyword in col.lower() for keyword in media_keywords) or
            any(keyword in col.lower() for keyword in engineered_keywords))
        and '_Spend' not in col
        and col not in base_vars
        and col not in promo_vars
        and col != 'Sales' and col != 'Week_Ending' and col != 'Fiscal_Year'
    ]
    media_vars = [var for var in media_vars if var in df.columns]
    media_vars.sort()

    # Target variable
    target_var = 'Sales' if 'Sales' in df.columns else None

    # Print the final organization
    print("\nFINAL VARIABLE BUCKETS FOR MODELING:")
    print("-" * 40)

    print("\nBASE VARIABLES:")
    if base_vars:
        for var in base_vars:
            print(f"• {var}")
    else:
        print("• None")

    print("\nPROMOTIONAL VARIABLES:")
    if promo_vars:
        for var in promo_vars:
            print(f"• {var}")
    else:
        print("• None")

    print("\nMEDIA VARIABLES:")
    if media_vars:
        for var in media_vars:
            print(f"• {var}")
    else:
        print("• None")

    print(f"\nTARGET VARIABLE:")
    print(f"• {target_var}" if target_var else "• None")

    # Create a new DataFrame with only the modeling variables
    modeling_vars = base_vars + promo_vars + media_vars
    if target_var:
        modeling_vars.append(target_var)

    df_model = df[modeling_vars].copy()

    print(f"\n✅ Created modeling DataFrame with {len(modeling_vars)} variables:")
    print(f"   - Base variables: {len(base_vars)}")
    print(f"   - Promotional variables: {len(promo_vars)}")
    print(f"   - Media variables: {len(media_vars)}")
    print(f"   - Target variable: {1 if target_var else 0}")

    # Display the first few rows of the modeling DataFrame
    print("\n📋 Modeling DataFrame (first 5 rows):")
    display(df_model.head())

    # Store the modeling buckets in a dictionary for easy access
    modeling_buckets = {
        'base_vars': base_vars,
        'promo_vars': promo_vars,
        'media_vars': media_vars,
        'target_var': target_var
    }

    # Store the modeling DataFrame in the global namespace

    print("\n✅ Modeling DataFrame stored as 'df_model'")
    print("✅ Modeling buckets dictionary stored as 'modeling_buckets'")
    print("\nYou can now use these variables for your model training:")
    print("   - df_model: Contains all modeling variables")
    print("   - modeling_buckets: Dictionary with variable lists for each bucket")

    return df


# Main execution block - Fixed to handle cases where data is not available
