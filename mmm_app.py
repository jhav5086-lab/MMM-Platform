# -*- coding: utf-8 -*-
"""mmm_app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12FqUvs80KRZeP9pJxOT1s7ChumJDuP_D
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.tsa.seasonal import seasonal_decompose # Import seasonal_decompose
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from scipy.optimize import curve_fit, nnls, minimize, Bounds, LinearConstraint # Import for optimization
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import io # Import the io module
import warnings
warnings.filterwarnings('ignore')

# Import for tick formatting in plots
import matplotlib.ticker as mticker

# Import display and HTML for rendering the media report table (though st.write(..., unsafe_allow_html=True) is preferred)
from IPython.display import display, HTML


# --- Custom Functions (Copy these from your notebook or import them if in a separate file) ---

def convert_indian_number(value):
    """Convert Indian number format string to float"""
    if isinstance(value, str):
        # Remove commas and strip whitespace
        cleaned_value = value.replace(',', '').strip()

        # Handle special cases like ' -   ' which should be treated as NaN
        if cleaned_value in ['-', ''] or cleaned_value.isspace():
            return np.nan

        try:
            return float(cleaned_value)
        except ValueError:
            print(f"Could not convert value: '{value}'")
            return np.nan
    return value

# Load and preprocess data

def assign_fiscal_year(date_series):
    """
    Assign fiscal year based on date.
    Assuming fiscal year starts in April (Q2) - adjust as needed
    """
    fiscal_years = []
    for date in date_series:
        if date.month >= 4:  # April or later
            fiscal_years.append(f"FY{date.year % 100:02d}/{(date.year + 1) % 100:02d}")
        else:  # January to March
            fiscal_years.append(f"FY{(date.year - 1) % 100:02d}/{date.year % 100:02d}")
    return fiscal_years

# Enhanced adstock transformation with fiscal year awareness

def enhanced_adstock(series, theta=0.4, fiscal_years=None):
    """
    Apply adstock transformation with fiscal year reset
    """
    adstocked = []
    current_adstock = 0
    current_fy = None

    for i, value in enumerate(series):
        # Reset adstock at fiscal year boundaries if fiscal_years provided
        if fiscal_years is not None and i > 0 and fiscal_years.iloc[i] != fiscal_years.iloc[i-1]: # Use .iloc to access fiscal year by position
            current_adstock = 0

        current_adstock = value + theta * current_adstock
        adstocked.append(current_adstock)

    return pd.Series(adstocked, index=series.index)

# Weibull CDF saturation function

def weibull_saturation(series, lambda_val=1.0, k=1.0):
    """
    Apply Weibull CDF saturation transformation
    Weibull CDF: 1 - exp(-(x/lambda)^k)
    """
    # Avoid division by zero and handle negative values
    series_non_neg = np.maximum(series, 0)
    # Handle lambda_val being zero to avoid division errors
    lambda_val_safe = np.maximum(lambda_val, 1e-9) # Add small epsilon for safety
    return 1 - np.exp(-(series_non_neg / lambda_val_safe) ** k)

# Enhanced transformation application

def apply_enhanced_transformations(df, media_vars, theta_params, weibull_params, fiscal_years=None):
    """
    Apply adstock and Weibull saturation transformations with fiscal year awareness
    """
    df_transformed = df.copy()

    for var in media_vars:
        if var in df_transformed.columns:
            # Apply adstock with fiscal year reset
            theta = theta_params.get(var, ADSTOCK_DEFAULT_THETA) # Use default if not found
            df_transformed[f"{var}_adstock"] = enhanced_adstock(
                df_transformed[var], theta, fiscal_years
            )

            # Apply Weibull saturation
            # Ensure weibull_params has the structure expected for each var
            weibull_params_var = weibull_params.get(var, {})
            lambda_val = weibull_params_var.get('lambda', SAT_DEFAULT_LAMBDA) # Use default if not found
            k_val = weibull_params_var.get('k', SAT_DEFAULT_K) # Use default if not found

            df_transformed[f"{var}_saturated"] = weibull_saturation(
                df_transformed[f"{var}_adstock"], lambda_val, k_val
            )

    return df_transformed

# Parameter optimization function

class ConstrainedLinearRegression(BaseEstimator, RegressorMixin):
    def __init__(self, media_vars_indices=None, alpha=1.0, l1_ratio=0.5):
        self.media_vars_indices = media_vars_indices
        self.alpha = alpha
        self.l1_ratio = l1_ratio
        self.coef_ = None
        self.intercept_ = None
        self.feature_names_ = None # Add attribute to store feature names

    def fit(self, X, y):
        # Check input
        X, y = check_X_y(X, y)

        # Store feature names if X is a DataFrame
        if isinstance(X, pd.DataFrame):
            self.feature_names_ = X.columns.tolist()
        else:
            # Generate generic feature names if X is not a DataFrame
            self.feature_names_ = [f'x{i}' for i in range(X.shape[1])]


        # Add intercept
        X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])

        if self.media_vars_indices is not None:
            # Use non-negative least squares for media variables
            # First, fit the non-media variables without constraints
            non_media_indices = [i for i in range(X.shape[1]) if i not in self.media_vars_indices]

            if non_media_indices:
                # Fit non-media variables using regular regression
                X_non_media = X_with_intercept[:, [0] + [i+1 for i in non_media_indices]]
                coef_non_media, _, _, _ = np.linalg.lstsq(X_non_media, y, rcond=None)

                # Calculate residuals for media variables
                y_residual = y - X_non_media @ coef_non_media

                # Fit media variables with non-negative constraint
                X_media = X_with_intercept[:, [i+1 for i in self.media_vars_indices]]
                # Add a small epsilon to avoid issues with all zeros in nnls if needed
                X_media_eps = X_media + 1e-9 # Add small epsilon
                coef_media, _ = nnls(X_media_eps, y_residual)

                # Combine coefficients
                self.coef_ = np.zeros(X.shape[1])
                self.intercept_ = coef_non_media[0]

                for i, idx in enumerate(non_media_indices):
                    self.coef_[idx] = coef_non_media[i+1]

                for i, idx in enumerate(self.media_vars_indices):
                    self.coef_[idx] = coef_media[i]
            else:
                # All variables are media variables
                X_media = X_with_intercept[:, 1:]
                X_media_eps = X_media + 1e-9 # Add small epsilon
                coef_media, _ = nnls(X_media_eps, y)
                self.coef_ = coef_media
                self.intercept_ = 0
        else:
            # Regular linear regression without constraints
            coef, _, _, _ = np.linalg.lstsq(X_with_intercept, y, rcond=None)
            self.intercept_ = coef[0]
            self.coef_ = coef[1:]

        return self

    def predict(self, X):
        check_is_fitted(self)
        X = check_array(X)
        return self.intercept_ + X @ self.coef_

# Define fiscal year mapping function

def enhanced_mmm_analysis(df_features, model_choice, holdout_weeks=8, enforce_positive_media=True):
    """
    Enhanced MMM analysis with automated parameter optimization
    """
    # Ensure necessary variables are defined from global scope or modeling_buckets
    global BASE_VARS, PROMO_VARS, MEDIA_VARS, TARGET, HOLDOUT_WEEKS, ADSTOCK_DEFAULT_THETA, SAT_DEFAULT_LAMBDA, SAT_DEFAULT_K

    if 'modeling_buckets' in globals():
        BASE_VARS = modeling_buckets.get('base_vars', [])
        PROMO_VARS = modeling_buckets.get('promo_vars', [])
        MEDIA_VARS = modeling_buckets.get('media_vars', [])
        TARGET = modeling_buckets.get('target_var', 'Sales')
        print("✅ Using modeling buckets from previous feature engineering step.")
    else:
        print("⚠️ modeling_buckets not found. Defining default variable lists.")
        # Define default variable lists if modeling_buckets is not available
        BASE_VARS = ['Average Price', 'Gasoline Price', 'SIndex', 'Total SKU']
        PROMO_VARS = ['Discount1', 'Discount2']
        MEDIA_VARS = ['Email Clicks', 'Modular Video Impressions', 'Organic Search Impressions', 'Paid Search Impressions', 'Paid Social Impressions'] # Use original names if transformed not available
        TARGET = 'Sales'

    if 'HOLDOUT_WEEKS' not in globals():
        HOLDOUT_WEEKS = 12 # Default holdout weeks
        print(f"✅ Setting holdout period to the last {HOLDOUT_WEEKS} weeks.")

    if 'ADSTOCK_DEFAULT_THETA' not in globals():
        ADSTOCK_DEFAULT_THETA = 0.4
        print(f"✅ Setting default adstock theta: {ADSTOCK_DEFAULT_THETA}")

    if 'SAT_DEFAULT_LAMBDA' not in globals():
        SAT_DEFAULT_LAMBDA = 1.0
        print(f"✅ Setting default Weibull lambda: {SAT_DEFAULT_LAMBDA}")

    if 'SAT_DEFAULT_K' not in globals():
        SAT_DEFAULT_K = 1.0
        print(f"✅ Setting default Weibull k: {SAT_DEFAULT_K}")


    # Check if data is a valid DataFrame before proceeding
    if not isinstance(df_features, pd.DataFrame) or df_features.empty:
        print("❌ ERROR: Input data 'df_features' is not a valid DataFrame or is empty.")
        print("Please ensure you have run the feature engineering module and it successfully created 'df_features'.")
        return None, None

    # Ensure the target variable exists in the DataFrame
    if TARGET not in df_features.columns:
        print(f"❌ ERROR: Target variable '{TARGET}' not found in the input data.")
        return None, None

    # 1. Optimize transformation parameters
    print("Optimizing transformation parameters...")

    # Prepare fiscal year data if available
    fy_for_optimization = None
    if 'Fiscal_Year' in df_features.columns:
        fy_for_optimization = df_features['Fiscal_Year']
    elif 'Week_Ending' in df_features.columns:
        # Assign fiscal year before optimization if needed
        try:
            df_features['Fiscal_Year'] = assign_fiscal_year(df_features.index) # Use index if datetime
            fy_for_optimization = df_features['Fiscal_Year']
            print("✅ Created 'Fiscal_Year' column for optimization.")
        except Exception as e:
            print(f"⚠️ Could not create 'Fiscal_Year' column for optimization: {e}")


    # Optimize parameters
    theta_params, weibull_params = optimize_transformation_params(
        df_features, MEDIA_VARS, TARGET, BASE_VARS, PROMO_VARS,
        fiscal_years=fy_for_optimization
    )

    # 2. Apply enhanced transformations with optimized parameters
    print("Applying enhanced transformations with optimized parameters...")

    # Ensure Fiscal_Year is in df_features for transformation if needed
    if 'Fiscal_Year' not in df_features.columns and 'Week_Ending' in df_features.columns:
         try:
             df_features['Fiscal_Year'] = assign_fiscal_year(df_features.index)
             print("✅ Created 'Fiscal_Year' column for transformation.")
         except Exception as e:
             print(f"⚠️ Could not create 'Fiscal_Year' column for transformation: {e}")


    fy_for_transform = df_features.get('Fiscal_Year', None) # Get Fiscal_Year if it exists


    df_transformed = apply_enhanced_transformations(
        df_features.copy(), # Use a copy to avoid modifying the original df_features in place during transformation application
        MEDIA_VARS,
        theta_params,
        weibull_params,
        fy_for_transform
    )

    # 3. Prepare modeling data
    print("Preparing modeling data...")
    # Need to use the _saturated variable names for modeling features
    transformed_media_vars = [f"{var}_saturated" for var in MEDIA_VARS]
    all_features = BASE_VARS + PROMO_VARS + transformed_media_vars
    # Filter to ensure features exist in df_transformed
    all_features = [f for f in all_features if f in df_transformed.columns]

    X = df_transformed[all_features]
    y = df_transformed[TARGET]

    # Add scaling
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index) # Preserve column names and index

    # 4. Time-based train-test split
    print("Splitting data into training and testing sets...")
    if len(X_scaled) <= HOLDOUT_WEEKS:
        print(f"❌ ERROR: Not enough data ({len(X_scaled)} weeks) for a {HOLDOUT_WEEKS}-week holdout.")
        return None, None

    X_train_scaled = X_scaled[:-HOLDOUT_WEEKS]
    y_train = y[:-HOLDOUT_WEEKS]
    X_test_scaled = X_scaled[-HOLDOUT_WEEKS:]
    y_test = y[-HOLDOUT_WEEKS:]

    print(f"✅ Data split: {len(X_train_scaled)} training weeks, {len(X_test_scaled)} testing weeks (last {HOLDOUT_WEEKS} weeks).")

    # 5. Model training with time-series cross-validation and parameter tuning (if applicable)
    print("Training model...")

    # Identify media variable indices for constraint
    media_var_indices = [
        i for i, col in enumerate(X_train_scaled.columns)
        if col in transformed_media_vars
    ]

    if enforce_positive_media:
        print("🔄 Using constrained regression with non-negative coefficients for media variables")
        # Use our custom constrained model
        model = ConstrainedLinearRegression(media_vars_indices=media_var_indices)
        # Simple grid search for the constrained model's alpha
        param_grid = {'alpha': [0.1, 1.0, 10.0]}
    else:
        # Use regular model selection based on MODEL_CHOICE
        if model_choice['model'] == 'ridge':
            model = Ridge()
            param_grid = model_choice.get('grid', {'alpha': [0.1, 1.0, 10.0]})
        elif model_choice['model'] == 'lasso':
            model = Lasso()
            param_grid = model_choice.get('grid', {'alpha': [0.1, 1.0, 10.0], 'max_iter': [10000]})
        elif model_choice['model'] == 'elasticnet':
            model = ElasticNet()
            param_grid = model_choice.get('grid', {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.2, 0.5, 0.8], 'max_iter': [10000]})
        else:
            print(f"❌ ERROR: Unsupported model type '{model_choice['model']}'.")
            return None, None

    tscv = TimeSeriesSplit(n_splits=min(5, len(X_train_scaled) // (HOLDOUT_WEEKS + 1))) # Adjust n_splits based on data size


    try:
        if param_grid:
            # Check if the param_grid is valid for the chosen model
            valid_params = model.get_params().keys()
            param_grid_filtered = {k: v for k, v in param_grid.items() if k in valid_params}
            if len(param_grid_filtered) != len(param_grid):
                invalid_params = [k for k in param_grid.keys() if k not in valid_params]
                print(f"⚠️ Warning: Invalid parameters in param_grid for model {type(model).__name__}: {invalid_params}. Using filtered grid.")
                param_grid = param_grid_filtered

            if param_grid: # Check if filtered grid is not empty
                grid_search = GridSearchCV(model, param_grid, cv=tscv, scoring='r2')
                grid_search.fit(X_train_scaled, y_train)
                best_model = grid_search.best_estimator_
                print(f"✅ Best parameters: {grid_search.best_params_}")
                print(f"✅ Best CV score: {grid_search.best_score_:.4f}")
            else:
                 print("⚠️ Empty or invalid parameter grid after filtering. Training model with default parameters.")
                 best_model = model
                 best_model.fit(X_train_scaled, y_train)
                 print("✅ Model trained successfully with default parameters.")

        else:
            # For models with no hyperparameters or empty grid
            best_model = model
            best_model.fit(X_train_scaled, y_train)
            print("✅ Model trained successfully with default parameters.")

        # --- FIX: Ensure best_model has feature_names_ attribute ---
        if not hasattr(best_model, 'feature_names_'):
             if isinstance(X_train_scaled, pd.DataFrame):
                  best_model.feature_names_ = X_train_scaled.columns.tolist()
             else:
                  # Generate generic names if X_train_scaled is not a DataFrame
                  best_model.feature_names_ = [f'x{i}' for i in range(X_train_scaled.shape[1])]
        # --- END FIX ---

    except Exception as e:
        print(f"❌ Error during model training: {e}")
        return None, None

    # Store the trained model and scaled test data in the global namespace for scenario analysis


    # 6. Generate predictions
    y_pred_train = best_model.predict(X_train_scaled)
    y_pred_test = best_model.predict(X_test_scaled)
    y_pred_full = np.concatenate([y_pred_train, y_pred_test])

    # 7. Calculate metrics
    train_metrics = {
        'R2': r2_score(y_train, y_pred_train),
        'RMSE': np.sqrt(mean_squared_error(y_train, y_pred_train)),
        'MAPE': mean_absolute_percentage_error(y_train, y_pred_train)
    }

    test_metrics = {
        'R2': r2_score(y_test, y_pred_test),
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_test)),
        'MAPE': mean_absolute_percentage_error(y_test, y_pred_test)
    }

    print("\n📊 Model Performance Metrics:")
    print("Training:", train_metrics)
    print(f"  R2: {train_metrics['R2']:.2%}")
    print(f"  RMSE: {train_metrics['RMSE']:.2f}")
    print(f"  MAPE: {train_metrics['MAPE']:.2%}")

    print("Test:", test_metrics)
    print(f"  R2: {test_metrics['R2']:.2%}")
    print(f"  RMSE: {test_metrics['RMSE']:.2f}")
    print(f"  MAPE: {test_metrics['MAPE']:.2%}")

    # Check media coefficients
    coef_dict = dict(zip(best_model.feature_names_, best_model.coef_)) # Use feature_names_
    media_coefs = {var: coef_dict[var] for var in transformed_media_vars if var in coef_dict}
    negative_media = {var: coef for var, coef in media_coefs.items() if coef < 0}

    if negative_media and enforce_positive_media:
        # This block should theoretically not be reached if nnls works correctly for media variables
        print("❌ WARNING: Some media coefficients are negative despite constraints:")
        for var, coef in negative_media.items():
            print(f"   {var}: {coef}")
    elif negative_media:
        print("⚠️ WARNING: Some media coefficients are negative:")
        for var, coef in negative_media.items():
            print(f"   {var}: {coef}")
    else:
        print("✅ All media coefficients are non-negative")


    # 8. Generate enhanced visualizations and analyses
    print("\nGenerating enhanced visualizations and analyses...")

    # A. AVP chart with holdout period
    dates = df_features.index # Use original index for plotting dates
    avp_fig = plot_avp_with_holdout(
        y, y_pred_full, len(y_train), dates,
        "Actual vs Predicted Sales with Holdout Period"
    )
    print("✅ AVP chart generated.")

    # B. Contribution by bucket over time
    # Correctly map original variable names to scaled variable names for bucket contribution calculation
    scaled_feature_names = X_scaled.columns.tolist()
    original_feature_names = all_features # List of original names before scaling

    # Create a mapping from original name to scaled name
    original_to_scaled_map = dict(zip(original_feature_names, scaled_feature_names))

    # Create bucket_vars with scaled variable names
    bucket_vars_scaled = {
        'Base': [original_to_scaled_map[var] for var in BASE_VARS if var in original_to_scaled_map],
        'Media': [original_to_scaled_map[var] for var in transformed_media_vars if var in original_to_scaled_map], # Use transformed media vars here
        'Promo': [original_to_scaled_map[var] for var in PROMO_VARS if var in original_to_scaled_map]
    }

    contribution_fig = plot_contribution_by_bucket(
        X_scaled, best_model, bucket_vars_scaled, dates, # Pass scaled data to plot_contribution_by_bucket
        "Contribution by Variable Bucket Over Time"
    )
    print("✅ Contribution timeline chart generated.")

    # C. Fiscal year ROI analysis
    if 'Fiscal_Year' in df_transformed.columns:
        # Pass the original media variable names to calculate_roi_by_fiscal_year
        # The function uses df_transformed which contains both original and transformed variables
        fy_roi_df = calculate_roi_by_fiscal_year(best_model, df_transformed, MEDIA_VARS)
        print("✅ Fiscal year ROI analysis calculated.")
    else:
        fy_roi_df = pd.DataFrame()
        print("⚠️ 'Fiscal_Year' column not available. Skipping fiscal year ROI analysis.")

    # D. Media effectiveness analysis
    # Pass the original media variable names to calculate_media_effectiveness
    effectiveness_df = calculate_media_effectiveness(best_model, df_transformed, MEDIA_VARS)
    print("✅ Media effectiveness analysis calculated.")

    # E. Fiscal year contribution comparison
    if 'Fiscal_Year' in df_transformed.columns:
        fy_contributions = {}
        # Use the same scaled feature names for summing contributions by fiscal year
        scaled_feature_names = X_scaled.columns.tolist()
        coef_dict = dict(zip(best_model.feature_names_, best_model.coef_)) # Use feature_names_

        # Create a mapping from scaled name back to original name for grouping
        scaled_to_original_map = dict(zip(scaled_feature_names, original_feature_names))

        # Group scaled features by original bucket names
        bucket_vars_scaled_to_original = {
             'Base': [scaled_to_original_map[var] for var in bucket_vars_scaled.get('Base', [])],
             'Media': [scaled_to_original_map[var] for var in bucket_vars_scaled.get('Media', [])],
             'Promo': [scaled_to_original_map[var] for var in bucket_vars_scaled.get('Promo', [])]
        }


        # Include Baseline contribution
        baseline_contrib = best_model.intercept_

        for fy in df_transformed['Fiscal_Year'].unique():
            fy_mask = df_transformed['Fiscal_Year'] == fy
            # Use the scaled data for contribution calculation
            fy_data_scaled = X_scaled[fy_mask]

            fy_contribution = {'Baseline': baseline_contrib}

            for bucket_name, scaled_vars_in_bucket in bucket_vars_scaled.items():
                bucket_total = 0
                for scaled_var in scaled_vars_in_bucket:
                    if scaled_var in coef_dict:
                         # Ensure the scaled variable also exists in the scaled data for this fiscal year
                         if scaled_var in fy_data_scaled.columns:
                             # Ensure both are numeric before summing
                             if pd.api.types.is_numeric_dtype(fy_data_scaled[scaled_var]) and pd.api.types.is_numeric_dtype(coef_dict[scaled_var]):
                                bucket_total += (fy_data_scaled[scaled_var] * coef_dict[scaled_var]).sum()
                             else:
                                 print(f"⚠️ Warning: Scaled variable '{scaled_var}' or its coefficient is not numeric for FY {fy}. Skipping contribution.")
                         else:
                             print(f"⚠️ Warning: Scaled variable '{scaled_var}' not found in scaled FY {fy} data. Skipping contribution.")

                fy_contribution[bucket_name] = bucket_total

            # Convert the values to float to avoid potential issues with numpy dtypes in plotting/display
            fy_contributions[fy] = {k: float(v) for k, v in fy_contribution.items()}


        fy_contribution_df = pd.DataFrame(fy_contributions).T
        # Ensure the index is named 'Fiscal_Year'
        fy_contribution_df.index.name = 'Fiscal_Year'
        print("✅ Fiscal year contribution comparison calculated.")
    else:
        fy_contribution_df = pd.DataFrame()
        print("⚠️ 'Fiscal_Year' column not available. Skipping fiscal year contribution comparison.")

    # F. Generate comparison visualizations
    if not fy_contribution_df.empty:
        fy_bar_fig = plot_fy_comparison(fy_contribution_df, "Total Contribution by Fiscal Year")
        print("✅ Fiscal year contribution bar chart generated.")

        # Filter for relevant FYs for pie charts - using the index names directly
        pie_years = [fy for fy in ['FY22/23', 'FY23/24'] if fy in fy_contribution_df.index]
        if pie_years:
            fy_pie_fig = plot_fy_pie_charts(fy_contribution_df.loc[pie_years])
            if fy_pie_fig: # Check if plotting was successful
                print(f"✅ Fiscal year incremental contribution pie charts generated for: {pie_years}")
            else:
                print("⚠️ Could not generate fiscal year incremental contribution pie charts.")
        else:
            fy_pie_fig = None
            print("⚠️ No data for FY22/23 or FY23/24 found for pie charts.")
    else:
        fy_bar_fig = None
        fy_pie_fig = None
        print("⚠️ Fiscal year contribution data not available. Skipping fiscal year contribution plots.")

    # G. Media response curves
    # Pass the original media variable names to plot_media_response_curves
    response_curve_fig = plot_media_response_curves(
        best_model,
        df_transformed,
        MEDIA_VARS,
        theta_params,
        weibull_params
    )

    if response_curve_fig:
        print("✅ Media response curves generated.")
    else:
        print("⚠️ Could not generate media response curves.")


    # 9. Generate optimization recommendations
    print("\n=== MEDIA OPTIMIZATION RECOMMENDATIONS ===")

    # Analyze media effectiveness for recommendations
    if not effectiveness_df.empty:
        # Find most effective media channels (e.g., by ROI if available)
        if 'ROI' in effectiveness_df.columns and effectiveness_df['ROI'].notna().any():
            # Sort by ROI, handling Inf values if necessary (Inf > any number)
            top_roi = effectiveness_df.sort_values(by='ROI', ascending=False).head(3)

            print("\nTop Media Channels by ROI:")
            if not top_roi.empty:
                # Format ROI values for printing
                def format_print_roi(val):
                    if pd.isna(val): return 'N/A'
                    if np.isinf(val): return 'Inf' if val > 0 else '-Inf'
                    return f'{val:.2%}'

                for _, row in top_roi.iterrows():
                    print(f"- {row['Media_Channel']}: {format_print_roi(row['ROI'])}")
            else:
                print("  No media channels with calculated ROI.")
        else:
            print("\n⚠️ ROI data not available for effectiveness recommendations.")

        # Simple heuristic for diminishing returns (based on average saturated volume)
        print("\nDiminishing Returns Check (Based on Average Saturated Volume > 80%):")
        for media_var in MEDIA_VARS:
            saturated_var = f"{media_var}_saturated"
            if saturated_var in df_transformed.columns:
                # Ensure the saturated column is numeric before calculating mean
                if pd.api.types.is_numeric_dtype(df_transformed[saturated_var]):
                    # Calculate average saturation relative to max possible saturation (which is 1 for Weibull)
                    # A simpler check is just the average value of the saturated variable itself.
                    avg_saturated_value = df_transformed[saturated_var].mean()
                    status = "Potential Diminishing Returns" if avg_saturated_value > 0.8 * weibull_saturation(pd.Series([df_transformed[media_var].max()]), weibull_params.get(media_var,{}).get('lambda',SAT_DEFAULT_LAMBDA), weibull_params.get(media_var,{}).get('k',SAT_DEFAULT_K)).iloc[0] else "Below 80% Estimated Maximum Saturation"
                    print(f"- {media_var}: {status} (Avg Saturated Value: {avg_saturated_value:.2f})")
                else:
                    print(f"- {media_var}: Saturated variable '{saturated_var}' is not numeric.")

            else:
                print(f"- {media_var}: Saturated variable '{saturated_var}' not found.")
    else:
        print("\n⚠️ Media effectiveness data not available for recommendations.")

    return {
        'model': best_model,
        'transformed_data': df_transformed,
        'metrics': {'train': train_metrics, 'test': test_metrics},
        'visualizations': {
            'avp_chart': avp_fig,
            'contribution_timeline': contribution_fig,
            'fy_contribution_bars': fy_bar_fig,
            'fy_contribution_pies': fy_pie_fig,
            'response_curves': response_curve_fig
        },
        'analyses': {
            'fy_roi': fy_roi_df,
            'media_effectiveness': effectiveness_df,
            'fy_contributions': fy_contribution_df
        },
        'params': {
            'theta': theta_params,
            'weibull': weibull_params
        }
    }, scaler

# Run the enhanced analysis
# Ensure df_features and MODEL_CHOICE are available from previous cells
if 'df_features' in globals() and isinstance(df_features, pd.DataFrame) and not df_features.empty and 'MODEL_CHOICE' in globals():
    print("Running Enhanced MMM Analysis...")
    # Set enforce_positive_media to True to ensure non-negative media coefficients
    results, scaler = enhanced_mmm_analysis(df_features.copy(), MODEL_CHOICE, HOLDOUT_WEEKS, enforce_positive_media=False)

    # Display results if analysis was successful
    if results is not None:
        print("\n=== ENHANCED MMM ANALYSIS RESULTS ===")

        # Show metrics
        print("\nModel Performance Metrics:")
        print("Training:", results['metrics']['train'])
        print(f"  R2: {results['metrics']['train']['R2']:.2%}")
        print(f"  RMSE: {results['metrics']['train']['RMSE']:.2f}")
        print(f"  MAPE: {results['metrics']['train']['MAPE']:.2%}")

        print("Test:", results['metrics']['test'])
        print(f"  R2: {results['metrics']['test']['R2']:.2%}")
        print(f"  RMSE: {results['metrics']['test']['RMSE']:.2f}")
        print(f"  MAPE: {results['metrics']['test']['MAPE']:.2%}")

        # Show optimized parameters
        print("\nOptimized Transformation Parameters:")
        if 'params' in results and 'theta' in results['params'] and 'weibull' in results['params']:
            for var in MEDIA_VARS:
                theta_val = results['params']['theta'].get(var, ADSTOCK_DEFAULT_THETA)
                weibull_params_var = results['params']['weibull'].get(var, {})
                lambda_val = weibull_params_var.get('lambda', SAT_DEFAULT_LAMBDA)
                k_val = weibull_params_var.get('k', SAT_DEFAULT_K)
                print(f"- {var}: theta={theta_val:.3f}, lambda={lambda_val:.3f}, k={k_val:.3f}")
        else:
             print("⚠️ Optimized transformation parameters not available.")


        # Show fiscal year ROI
        if 'analyses' in results and 'fy_roi' in results['analyses'] and not results['analyses']['fy_roi'].empty:
            print("\nROI by Fiscal Year:")
            # Apply formatting for display
            def format_roi_display(value):
                if pd.isna(value):
                    return 'N/A'
                elif np.isinf(value):
                     return '<span style="color:green;">Inf</span>' if value > 0 else '<span style="color:red;">-Inf</span>'
                elif value >= 0:
                    return f'<span style="color:green;">{value:.2%}</span>'
                else:
                    return f'<span style="color:red;">{value:.2%}</span>'

            # Create a formatted copy for display
            fy_roi_df_formatted = results['analyses']['fy_roi'].applymap(format_roi_display)
            display(HTML(fy_roi_df_formatted.to_html(escape=False))) # Use HTML to render colors
        else:
            print("\nNo Fiscal Year ROI data to display.")

        # Show media effectiveness
        if 'analyses' in results and 'media_effectiveness' in results['analyses'] and not results['analyses']['media_effectiveness'].empty:
            print("\nMedia Effectiveness Metrics:")
            display(results['analyses']['media_effectiveness'])
        else:
            print("\nNo Media Effectiveness data to display.")

        # Show fiscal year contributions
        if 'analyses' in results and 'fy_contributions' in results['analyses'] and not results['analyses']['fy_contributions'].empty:
            print("\nContributions by Fiscal Year:")
            display(results['analyses']['fy_contributions'])
        else:
            print("\nNo Fiscal Year Contribution data to display.")

        # Display visualizations
        print("\nGenerating visualizations...")
        if 'visualizations' in results:
            if results['visualizations'].get('avp_chart'):
                results['visualizations']['avp_chart'].show()
            if results['visualizations'].get('contribution_timeline'):
                results['visualizations']['contribution_timeline'].show()
            if results['visualizations'].get('fy_contribution_bars'):
                results['visualizations']['fy_contribution_bars'].show()
            if results['visualizations'].get('fy_contribution_pies'):
                results['visualizations']['fy_contribution_pies'].show()
            if results['visualizations'].get('response_curves'):
                results['visualizations']['response_curves'].show()
        else:
            print("⚠️ Visualizations dictionary not available in results.")


        print("\n✅ Enhanced MMM Analysis Complete.")

else:
    if 'df_features' not in globals() or not isinstance(df_features, pd.DataFrame) or df_features.empty:
        print("❌ Cannot run Enhanced MMM Analysis: 'df_features' DataFrame is not available, not a DataFrame, or is empty.")
        print("Please ensure you have run the feature engineering module first.")
    if 'MODEL_CHOICE' not in globals():
        print("❌ Cannot run Enhanced MMM Analysis: 'MODEL_CHOICE' is not available.")

import subprocess
import sys

# Run pip list to get installed packages
# Decode as utf-8 to handle potential encoding issues
process = subprocess.run([sys.executable, '-m', 'pip', 'freeze'], capture_output=True, text=True, check=True)
requirements_content = process.stdout

print("Generated requirements.txt content:")
print("```")
print(requirements_content)
print("```")

# Optional: Save to a file
# with open("requirements.txt", "w") as f:
#     f.write(requirements_content)
# print("\nrequirements.txt file created.")

# Print the optimized Weibull parameters after running the analysis
if 'results' in globals() and results is not None and 'params' in results and 'weibull' in results['params']:
    print("\nOptimized Weibull Parameters:")
    display(results['params']['weibull'])
else:
    print("\nOptimized Weibull parameters not available. Run the enhanced MMM analysis cell first.")

# To check test_values, you would need to modify the plot_media_response_curves function
# For example, add this line inside the function:
# print(f"Test values range for {media_var}: min={test_values.min()}, max={test_values.max()}")

# Check if the trained model is available in the global namespace
if 'model' in globals() and hasattr(model, 'coef_') and hasattr(model, 'feature_names_'):
    print("Model Coefficients:")
    # Create a dictionary of feature names and coefficients
    coef_dict = dict(zip(model.feature_names_, model.coef_))

    # Print the coefficients
    for feature, coef in coef_dict.items():
        print(f"{feature}: {coef:.4f}")

    # Also print the intercept (baseline)
    if hasattr(model, 'intercept_'):
        print(f"Intercept (Baseline): {model.intercept_:.4f}")
else:
    print("Trained model or its coefficients not found. Please run the enhanced MMM analysis cell first.")

# Check if the scaled DataFrame is available
if 'X_scaled' in globals() and isinstance(X_scaled, pd.DataFrame):
    print("Correlation Matrix of Scaled, Transformed Data:")
    # Add the target variable (scaled or unscaled, depending on preference; unscaled is fine for correlation)
    # Assuming 'y' (unscaled target) is available from the last run of the modeling cell
    if 'y' in globals() and isinstance(y, pd.Series):
        temp_df = X_scaled.copy()
        temp_df['Sales'] = y # Add unscaled Sales for correlation calculation
        correlation_matrix_scaled = temp_df.corr()

        # Display correlations, focusing on 'Sales'
        display(correlation_matrix_scaled[['Sales']].sort_values(by='Sales', ascending=False))

        # Optionally, display the full correlation matrix
        # plt.figure(figsize=(12, 10))
        # sns.heatmap(correlation_matrix_scaled, annot=True, fmt=".2f", cmap="coolwarm", linewidths=".5")
        # plt.title("Correlation Matrix - Scaled, Transformed Variables")
        # plt.show()

    else:
        print("❌ Target variable 'y' not found. Cannot calculate correlation with Sales.")
        # Display full correlation matrix of features only if target is not available
        # plt.figure(figsize=(10, 8))
        # sns.heatmap(X_scaled.corr(), annot=True, fmt=".2f", cmap="coolwarm", linewidths=".5")
        # plt.title("Correlation Matrix - Scaled, Transformed Features")
        # plt.show()


else:
    print("❌ Scaled DataFrame 'X_scaled' not found. Please run the enhanced MMM analysis cell first.")

# ================================================
# SCENARIO ANALYSIS MODULE
# ================================================
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from IPython.display import display, HTML
import warnings
warnings.filterwarnings('ignore')

# Ensure necessary variables are defined from global scope
if 'model' not in globals() or 'scaler' not in globals() or 'df_features' not in globals() or 'modeling_buckets' not in globals():
    print("❌ ERROR: Required variables (model, scaler, df_features, modeling_buckets) not found.")
    print("Please ensure you have run the enhanced MMM analysis cell successfully.")
    # Set placeholder variables to prevent errors in function definitions
    model = None
    scaler = None
    df_features = pd.DataFrame()
    modeling_buckets = {}

# Retrieve variables from global scope
trained_model = globals().get('model')
feature_scaler = globals().get('scaler')
original_df = globals().get('df_features') # Use df_features as the base for scenarios
modeling_vars_info = globals().get('modeling_buckets', {})
transformed_media_names = globals().get('transformed_media_vars', []) # List of transformed media variable names used in the model features
all_model_features = globals().get('all_features', []) # List of all feature names used in the model (original names before scaling, but after transformation if applicable)


# Ensure buckets are defined even if modeling_buckets was not found
BASE_VARS_MODELING = modeling_vars_info.get('base_vars', [])
PROMO_VARS_MODELING = modeling_vars_info.get('promo_vars', [])
MEDIA_VARS_MODELING = modeling_vars_info.get('media_vars', []) # Original media variable names
TARGET_VAR_MODELING = modeling_vars_info.get('target_var', 'Sales')


# Ensure transformation parameters are available from the results dictionary
if 'results' in globals() and results is not None and 'params' in results:
     adstock_params_optimized = results['params'].get('theta', {})
     weibull_params_optimized = results['params'].get('weibull', {})
     print("✅ Using optimized transformation parameters for scenario analysis.")
else:
     # Use default parameters if optimized ones are not found
     adstock_params_optimized = {var: globals().get('ADSTOCK_DEFAULT_THETA', 0.4) for var in MEDIA_VARS_MODELING}
     weibull_params_optimized = {var: {'lambda': globals().get('SAT_DEFAULT_LAMBDA', 1.0), 'k': globals().get('SAT_DEFAULT_K', 1.0)} for var in MEDIA_VARS_MODELING}
     print("⚠️ Optimized transformation parameters not found. Using default parameters for scenario analysis.")


# Ensure Fiscal_Year is in original_df for adstock transformation if needed
if original_df is not None and not original_df.empty and 'Fiscal_Year' not in original_df.columns and 'Week_Ending' in original_df.columns:
     try:
         # Assuming original_df index is Week_Ending datetime
         original_df['Fiscal_Year'] = original_df.index.year # Simpler year extraction if index is datetime
         print("✅ Created 'Fiscal_Year' column in original_df for adstock transformation.")
     except Exception as e:
         print(f"⚠️ Could not create 'Fiscal_Year' column in original_df: {e}")

fy_for_transform_scenario = original_df.get('Fiscal_Year', None) if original_df is not None else None


# Define a function to apply the transformations used in modeling

def calculate_roi_by_fiscal_year(model, df_transformed, media_vars, fiscal_year_col='Fiscal_Year'):
    """
    Calculate ROI and MROI metrics aggregated by fiscal year
    """
    roi_results = {}

    # Get coefficients using the stored feature names
    if model.feature_names_ is not None and model.coef_ is not None:
        coef_dict = dict(zip(model.feature_names_, model.coef_))
    else:
        print("⚠️ Model does not have feature names or coefficients. Cannot calculate ROI.")
        return pd.DataFrame()


    if fiscal_year_col not in df_transformed.columns:
        print(f"⚠️ Fiscal year column '{fiscal_year_col}' not found in transformed data. Cannot calculate ROI by fiscal year.")
        return pd.DataFrame()

    for fy in df_transformed[fiscal_year_col].unique():
        fy_mask = df_transformed[fiscal_year_col] == fy
        fy_data = df_transformed[fy_mask]

        fy_roi = {}
        for media_var in media_vars:
            saturated_var = f"{media_var}_saturated"
            if saturated_var in coef_dict:
                # Calculate media contribution for this fiscal year
                if saturated_var in fy_data.columns:
                    media_contribution = (fy_data[saturated_var] * coef_dict[saturated_var]).sum()
                else:
                    print(f"⚠️ Warning: Saturated variable '{saturated_var}' not found in FY {fy} data. Skipping ROI for this channel/year.")
                    continue


                # Get spend (assuming spend column follows naming convention)
                spend_var = f"{media_var}_Spend"
                if spend_var in fy_data.columns:
                    total_spend = fy_data[spend_var].sum()
                    # Avoid division by zero when calculating ROI
                    roi = (media_contribution - total_spend) / total_spend if total_spend != 0 else (float('inf') if media_contribution > 0 else (float('-inf') if media_contribution < 0 else 0))

                    fy_roi[media_var] = roi
                else:
                     print(f"⚠️ Warning: Spend variable '{spend_var}' not found in FY {fy} data. Cannot calculate ROI for this channel/year.")


        if fy_roi: # Only add if there's any ROI data for the year
             roi_results[fy] = fy_roi
        else:
             print(f"⚠️ No ROI data calculated for FY {fy}.")


    # Convert dictionary to DataFrame and sort by index (Fiscal Year)
    roi_df = pd.DataFrame(roi_results).T.sort_index()

    # Format ROI as percentage with color
    def format_roi(value):
        if pd.isna(value):
            return 'N/A'
        elif np.isinf(value):
             return '<span style="color:green;">Inf</span>' if value > 0 else '<span style="color:red;">-Inf</span>'
        elif value >= 0:
            return f'<span style="color:green;">{value:.2%}</span>'
        else:
            return f'<span style="color:red;">{value:.2%}</span>'

    # Apply formatting for display (optional, can also format later)
    # This formatting is better applied when displaying the final table
    # roi_df_formatted = roi_df.applymap(format_roi)

    return roi_df # Return the raw numeric DataFrame

# Media effectiveness calculation

def calculate_media_effectiveness(model, df_transformed, media_vars):
    """
    Calculate media effectiveness metrics (incremental volume per unit)
    """
    effectiveness_data = []
    # Get coefficients using the stored feature names
    if model.feature_names_ is not None and model.coef_ is not None:
        coef_dict = dict(zip(model.feature_names_, model.coef_))
    else:
        print("⚠️ Model does not have feature names or coefficients. Cannot calculate media effectiveness.")
        return pd.DataFrame()


    for media_var in media_vars:
        saturated_var = f"{media_var}_saturated"
        if saturated_var in coef_dict:
            coef = coef_dict[saturated_var]

            metrics = {'Media_Channel': media_var}

            # Calculate metrics based on original (non-transformed) media volume
            # Ensure the original media variable is in the DataFrame
            if media_var in df_transformed.columns:
                total_original_volume = df_transformed[media_var].sum()
                total_contribution = (df_transformed[saturated_var] * coef).sum() # Contribution from saturated volume


                # Incremental Volume per Original Volume Unit
                metrics['Incremental_Volume_Per_Unit'] = total_contribution / total_original_volume if total_original_volume != 0 else float('nan')


                # Per 1M impressions
                if 'impression' in media_var.lower() and total_original_volume > 0:
                    metrics['Incremental_Volume_Per_1M_Impressions'] = (total_contribution / total_original_volume) * 1e6
                elif 'impression' in media_var.lower():
                     metrics['Incremental_Volume_Per_1M_Impressions'] = float('nan')


                # Per 1000 clicks
                if 'click' in media_var.lower() and total_original_volume > 0:
                    metrics['Incremental_Volume_Per_1000_Clicks'] = (total_contribution / total_original_volume) * 1000
                elif 'click' in media_var.lower():
                     metrics['Incremental_Volume_Per_1000_Clicks'] = float('nan')


                # ROI (requires spend data)
                spend_var = f"{media_var}_Spend"
                if spend_var in df_transformed.columns:
                    total_spend = df_transformed[spend_var].sum()
                    # Avoid division by zero when calculating ROI
                    metrics['ROI'] = (total_contribution - total_spend) / total_spend if total_spend != 0 else (float('inf') if total_contribution > 0 else (float('-inf') if total_contribution < 0 else 0))
                else:
                     metrics['ROI'] = float('nan') # Cannot calculate ROI without spend data


                effectiveness_data.append(metrics)
            else:
                 print(f"⚠️ Warning: Original media variable '{media_var}' not found in df_transformed. Skipping effectiveness calculation for this channel.")


    return pd.DataFrame(effectiveness_data)

# Visualization functions

def plot_avp_with_holdout(y_actual, y_pred, holdout_start_index, dates=None, title="Actual vs Predicted Sales"):
    """
    Plot Actual vs Predicted with holdout period highlighted
    """
    fig = go.Figure()

    # Determine x-axis values
    if dates is None:
        x_values = y_actual.index # Use DataFrame index if dates is None
    else:
        x_values = dates # Use provided dates

    # Add actual sales
    fig.add_trace(go.Scatter(
        x=x_values,
        y=y_actual,
        mode='lines',
        name='Actual Sales',
        line=dict(color='blue')
    ))

    # Add predicted sales
    fig.add_trace(go.Scatter(
        x=x_values,
        y=y_pred,
        mode='lines',
        name='Predicted Sales',
        line=dict(color='red', dash='dash')
    ))

    # Add holdout region
    if holdout_start_index < len(y_actual):
        # Use index to determine start and end dates for the holdout region
        holdout_start_date = x_values[holdout_start_index]
        holdout_end_date = x_values[-1]

        fig.add_vrect(
            x0=holdout_start_date,
            x1=holdout_end_date,
            fillcolor="lightgray",
            opacity=0.3,
            layer="below",
            line_width=0,
            annotation_text=f"Holdout Period ({len(y_actual) - holdout_start_index} weeks)"
        )

    fig.update_layout(
        title=title,
        xaxis_title="Time",
        yaxis_title="Sales",
        hovermode="x unified"
    )

    return fig

def plot_contribution_by_bucket(df_transformed, model, bucket_vars, dates=None, title="Contribution by Bucket"):
    """
    Plot contribution over time by variable bucket
    """
    # Calculate contributions
    contributions = {}
    # Get coefficients using the stored feature names
    if model.feature_names_ is not None and model.coef_ is not None:
        coef_dict = dict(zip(model.feature_names_, model.coef_))
    else:
        print("⚠️ Model does not have feature names or coefficients. Cannot calculate contribution by bucket.")
        return go.Figure() # Return empty figure


    # Determine x-axis values
    if dates is None:
        x_values = df_transformed.index # Use DataFrame index if dates is None
    else:
        x_values = dates # Use provided dates

    # Add baseline contribution as a separate trace
    baseline_contrib = model.intercept_

    for bucket_name, vars_in_bucket in bucket_vars.items():
        bucket_contribution = pd.Series(0.0, index=df_transformed.index) # Initialize with float
        for var in vars_in_bucket:
            if var in coef_dict:
                # Ensure the variable exists in the DataFrame before multiplying
                if var in df_transformed.columns:
                     # Ensure both are numeric before multiplication
                     if pd.api.types.is_numeric_dtype(df_transformed[var]) and pd.api.types.is_numeric_dtype(coef_dict[var]):
                        bucket_contribution += df_transformed[var] * coef_dict[var]
                     else:
                        print(f"⚠️ Warning: Variable '{var}' or its coefficient is not numeric. Skipping contribution calculation for this variable.")
                else:
                     print(f"⚠️ Warning: Variable '{var}' not found in df_transformed for contribution calculation.")

        contributions[bucket_name] = bucket_contribution

    # Create stacked area chart
    fig = go.Figure()

    # Add baseline contribution
    fig.add_trace(go.Scatter(
         x=x_values,
         y=[baseline_contrib] * len(df_transformed), # Constant baseline over time
         mode='lines',
         name='Baseline',
         line=dict(color='gray', dash='dash') # Make baseline distinct
    ))

    # Add stacked contributions for other buckets
    for bucket_name, contribution in contributions.items():
        fig.add_trace(go.Scatter(
            x=x_values,
            y=contribution,
            mode='lines',
            stackgroup='one', # Stack other contributions on top of each other
            name=bucket_name
        ))

    fig.update_layout(
        title=title,
        xaxis_title="Time",
        yaxis_title="Contribution",
        hovermode="x unified",
        legend_title="Variable Bucket"
    )

    return fig

def plot_fy_comparison(contributions_df, title="Contribution by Fiscal Year"):
    """
    Plot bar chart comparing contributions by fiscal year
    """
    # Ensure the index is the fiscal year and it's sorted
    if contributions_df.index.name != 'Fiscal_Year':
        contributions_df = contributions_df.set_index('Fiscal_Year').sort_index()

    fig = px.bar(
        contributions_df,
        x=contributions_df.index,
        y=contributions_df.columns,
        title=title,
        barmode='group'
    )

    fig.update_layout(
        xaxis_title="Fiscal Year",
        yaxis_title="Contribution",
        legend_title="Variable Bucket"
    )

    return fig

def plot_fy_pie_charts(contributions_df, title_template="Incremental Contribution - {}"):
    """
    Plot pie charts showing contribution breakdown by fiscal year
    """
    # Ensure the index is the fiscal year
    if contributions_df.index.name != 'Fiscal_Year':
         print("⚠️ contributions_df index is not Fiscal_Year. Cannot generate pie charts.")
         return None # Return None if data is not in expected format

    years = contributions_df.index.tolist()
    n_years = len(years)

    if n_years == 0:
        print("⚠️ No fiscal year data available for pie charts.")
        return None

    # Filter out the baseline contribution if it's in the dataframe for incremental pies
    incremental_contributions_df = contributions_df.drop(columns=['Baseline'], errors='ignore')

    if incremental_contributions_df.empty:
         print("⚠️ No incremental contribution data available after dropping Baseline.")
         return None


    fig = make_subplots(
        rows=1,
        cols=n_years,
        subplot_titles=[title_template.format(year) for year in years],
        specs=[[{"type": "domain"}]] * n_years
    )

    for i, year in enumerate(years, 1):
        fig.add_trace(
            go.Pie(
                labels=incremental_contributions_df.columns,
                values=incremental_contributions_df.loc[year].values,
                name=year,
                scalegroup='pies', # Scale pies relative to each other
                hovertemplate='<b>%{label}</b><br>Contribution: %{value:,.0f}<br>(%{percent})<extra></extra>' # Improved hover
            ),
            row=1, col=i
        )

    fig.update_layout(title_text="Incremental Contribution by Fiscal Year (Excluding Baseline)")
    return fig

def plot_media_response_curves(model, df_transformed, media_vars, theta_params, weibull_params):
    """
    Plot response curves for paid media variables (excluding organic)
    """
    # Filter out organic media variables from the response curve plotting
    paid_media_vars_for_plot = [var for var in media_vars if 'organic' not in var.lower()]

    if not paid_media_vars_for_plot:
        print("No paid media variables found for response curves")
        return None

    # Create subplots
    n_cols = 2
    n_rows = (len(paid_media_vars_for_plot) + n_cols - 1) // n_cols # Correct calculation for number of rows
    fig = make_subplots(
        rows=n_rows,
        cols=n_cols,
        subplot_titles=[f"Response Curve: {var.replace('_saturated', '')}" for var in paid_media_vars_for_plot] # Clean up title
    )

    # Get coefficients using the stored feature names
    if model.feature_names_ is not None and model.coef_ is not None:
        coef_dict = dict(zip(model.feature_names_, model.coef_))
        # Create a mapping from transformed media variable name to scaled feature name
        # Need access to original media vars and transformed media vars list
        if 'MEDIA_VARS' in globals() and 'transformed_media_vars' in globals() and 'all_features' in globals():
             # Map original feature names to scaled feature names
             original_to_scaled_map = dict(zip(all_features, model.feature_names_))
             # Create a mapping from transformed media name to scaled feature name
             transformed_media_to_scaled_map = {f"{var}_saturated": original_to_scaled_map.get(f"{var}_saturated", None) for var in MEDIA_VARS}
        else:
            print("⚠️ Required global variables (MEDIA_VARS, transformed_media_vars, all_features) not found for mapping.")
            return None
    else:
        print("⚠️ Model does not have feature names or coefficients. Cannot plot response curves.")
        return None

    for i, media_var_original in enumerate(paid_media_vars_for_plot): # Iterate through original media names
        row = (i // n_cols) + 1
        col = (i % n_cols) + 1

        saturated_var_name = f"{media_var_original}_saturated"
        scaled_feature_name = transformed_media_to_scaled_map.get(saturated_var_name, None)


        if scaled_feature_name is not None and scaled_feature_name in coef_dict:
             coef = coef_dict[scaled_feature_name] # Use scaled feature name to get coefficient
        else:
            print(f"⚠️ Warning: Scaled feature for saturated variable '{saturated_var_name}' not found in model coefficients. Cannot plot response curve.")
            continue # Skip plotting for this channel


        # Ensure original variable exists in df_transformed for max value
        if media_var_original not in df_transformed.columns:
             print(f"⚠️ Warning: Original variable '{media_var_original}' not found in df_transformed. Cannot plot response curve.")
             continue

        # Get transformation parameters for the original variable name
        theta = theta_params.get(media_var_original, ADSTOCK_DEFAULT_THETA) # Use original name for params
        weibull_params_var = weibull_params.get(media_var_original, {}) # Use original name for params
        lambda_val = weibull_params_var.get('lambda', SAT_DEFAULT_LAMBDA)
        k_val = weibull_params_var.get('k', SAT_DEFAULT_K)


        # Create test values (0 to 2x max observed value) - Use original var for max calculation
        max_val = df_transformed[media_var_original].max()
        test_values = np.linspace(0, max_val * 2, 100)

        # Apply transformations - Need a dummy series with fiscal year if adstock uses it
        # For plotting a general response curve, we can simplify and assume no FY reset within the test range
        # However, if the adstock implementation is strictly tied to FY, this might be complex.
        # Let's use a simplified adstock for the plot that doesn't reset for test values.
        # This might slightly differ from the model's training adstock if FY resets were frequent.
        # A more accurate approach would simulate adstock across a long period, but this is simpler for plotting.
        # Let's stick to the enhanced_adstock but assume a single FY for the test values.
        dummy_fy_series = pd.Series([df_transformed['Fiscal_Year'].iloc[0]] * len(test_values), index=pd.to_datetime(range(len(test_values)))) # Create a dummy FY series


        adstocked = enhanced_adstock(pd.Series(test_values), theta, fiscal_years=dummy_fy_series if 'Fiscal_Year' in df_transformed.columns else None)
        saturated = weibull_saturation(adstocked, lambda_val, k_val)


        # Calculate response (contribution)
        response = saturated * coef # Use the retrieved coefficient


        # Add trace
        fig.add_trace(
            go.Scatter(
                x=test_values,
                y=response,
                mode='lines',
                name=media_var_original, # Use original name for legend
                hovertemplate=f"{media_var_original}: %{{x:.0f}}<br>Response: %{{y:.0f}}<extra></extra>"
            ),
            row=row, col=col
        )

        # Add vertical line at max observed value
        fig.add_vline(
            x=max_val,
            line_dash="dash",
            line_color="red",
            row=row, col=col
        )

    fig.update_layout(
        height=max(300, 300 * n_rows), # Ensure minimum height
        title_text="Media Response Curves (Red line = max observed value)",
        showlegend=False
    )

    fig.update_xaxes(title_text="Media Investment")
    fig.update_yaxes(title_text="Sales Contribution")

    return fig

# Main enhanced modeling process with automated parameter optimization

def compare_scenarios(scenario_results):
    """
    Compares results from multiple scenarios and displays them in a table.
    """
    comparison_data = []
    for result in scenario_results:
        if result is not None:
            comparison_data.append({
                'Scenario': result['scenario_name'],
                'Total Predicted Sales': result['predicted_sales'].sum(),
                'Total Incremental Sales (vs Base)': result['incremental_sales'].sum(),
                'Total Scenario Spend': result['total_spend'],
                'Total Incremental Spend (vs Base)': result['incremental_spend'],
                'Scenario ROI': result['scenario_roi']
            })

    comparison_df = pd.DataFrame(comparison_data)

    if not comparison_df.empty:
        print("\n===== SCENARIO COMPARISON =====")

        # Format numeric columns for display
        comparison_df_formatted = comparison_df.copy()
        for col in ['Total Predicted Sales', 'Total Incremental Sales (vs Base)', 'Total Scenario Spend', 'Total Incremental Spend (vs Base)']:
             comparison_df_formatted[col] = comparison_df_formatted[col].apply(lambda x: f"{x:,.0f}")

        # Format ROI column with color
        def format_roi_display(value):
            if pd.isna(value):
                return 'N/A'
            elif np.isinf(value):
                 return '<span style="color:green;">Inf</span>' if value > 0 else '<span style="color:red;">-Inf</span>'
            elif value >= 0:
                return f'<span style="color:green;">{value:.2%}</span>'
            else:
                return f'<span style="color:red;">{value:.2%}</span>'

        comparison_df_formatted['Scenario ROI'] = comparison_df_formatted['Scenario ROI'].apply(format_roi_display)

        # Display the formatted table
        display(HTML(comparison_df_formatted.to_html(index=False, escape=False)))

        # Visualize scenario comparison (e.g., Incremental Sales vs Incremental Spend)
        if 'Total Incremental Sales (vs Base)' in comparison_df.columns and 'Total Incremental Spend (vs Base)' in comparison_df.columns:
             fig = px.scatter(comparison_df,
                              x='Total Incremental Spend (vs Base)',
                              y='Total Incremental Sales (vs Base)',
                              text='Scenario',
                              size='Total Predicted Sales', # Size points by total predicted sales
                              hover_name='Scenario',
                              title='Scenario Comparison: Incremental Sales vs Incremental Spend')

             fig.update_traces(textposition='top center')
             fig.update_layout(showlegend=False)
             fig.show()
        else:
            print("⚠️ Cannot generate scatter plot: Incremental Sales or Incremental Spend columns not found.")

    else:
        print("⚠️ No scenario results to compare.")


# --- Interactive Scenario Definition ---
if 'df_features' in globals() and isinstance(df_features, pd.DataFrame) and not df_features.empty:
    print("\n===== INTERACTIVE SCENARIO ANALYSIS =====")
    print("Enter scenarios to simulate. You can add multiple scenarios.")
    print("Enter 'done' when you are finished adding scenarios.")

    interactive_scenario_results = []

    # Display available media channels for the user to choose from
    if MEDIA_VARS_MODELING:
        print("\nAvailable Media Channels:")
        for i, channel in enumerate(MEDIA_VARS_MODELING, 1):
            print(f"{i}. {channel}")
    else:
        print("\n⚠️ No media variables found in modeling buckets. Cannot run media scenarios.")


    while True:
        scenario_name = input("\nEnter Scenario Name (or 'done' to finish): ").strip()
        if scenario_name.lower() == 'done':
            break

        if not MEDIA_VARS_MODELING:
            print("Skipping scenario as no media variables are available.")
            continue

        while True:
            try:
                channel_choice_input = input(f"Enter number for Media Channel to change (1-{len(MEDIA_VARS_MODELING)}): ").strip()
                channel_index = int(channel_choice_input) - 1
                if 0 <= channel_index < len(MEDIA_VARS_MODELING):
                    media_channel_to_change = MEDIA_VARS_MODELING[channel_index]
                    break
                else:
                    print("⚠️ Invalid channel number. Please enter a number from the list.")
            except ValueError:
                print("⚠️ Invalid input. Please enter a number.")

        while True:
            try:
                change_type = input("Change type: [1] Percentage change  [2] Fixed spend value: ").strip()
                if change_type == '1':
                    pct_input = input(f"Enter percentage change for {media_channel_to_change} (e.g., 10 for +10%, -5 for -5%): ").strip()
                    spend_change_pct = float(pct_input) / 100.0
                    fixed_spend_value = None
                    break
                elif change_type == '2':
                    fixed_spend_input = input(f"Enter fixed spend value for {media_channel_to_change}: ").strip()
                    fixed_spend_value = float(fixed_spend_input)
                    spend_change_pct = 0.0 # Reset percentage change
                    break
                else:
                    print("⚠️ Invalid change type. Enter 1 or 2.")
            except ValueError:
                print("⚠️ Invalid input. Please enter a numeric value.")


        # Run the scenario
        scenario_result = run_scenario(
            original_df,
            scenario_name=scenario_name,
            media_channel_to_change=media_channel_to_change,
            spend_change_pct=spend_change_pct,
            fixed_spend_value=fixed_spend_value # Pass fixed value if provided
        )
        if scenario_result:
            interactive_scenario_results.append(scenario_result)

    # Compare all interactively defined scenarios
    if interactive_scenario_results:
        compare_scenarios(interactive_scenario_results)
    else:
        print("\nNo scenarios were defined.")


else:
    print("\n❌ Cannot run scenario analysis: 'df_features' DataFrame is not available, not a DataFrame, or is empty.")
    print("Please ensure you have run the feature engineering module first.")

"""### Optimization Module with Budget Constraints

This module will use the trained MMM model and the SciPy optimization library to find the optimal media spend allocation that maximizes predicted sales under given budget constraints.
"""

# ================================================
# MEDIA SPEND OPTIMIZATION MODULE
# ================================================
import pandas as pd
import numpy as np
from scipy.optimize import minimize, Bounds, LinearConstraint
from IPython.display import display, HTML
import warnings
warnings.filterwarnings('ignore')

# Ensure necessary variables are defined from global scope
if 'model' not in globals() or 'scaler' not in globals() or 'df_features' not in globals() or 'modeling_buckets' not in globals() or 'adstock_params_optimized' not in globals() or 'weibull_params_optimized' not in globals():
    print("❌ ERROR: Required variables (model, scaler, df_features, modeling_buckets, adstock_params_optimized, weibull_params_optimized) not found.")
    print("Please ensure you have run the enhanced MMM analysis cell successfully.")
    # Set placeholder variables to prevent errors
    trained_model = None
    feature_scaler = None
    original_df = pd.DataFrame()
    modeling_vars_info = {}
    adstock_params_optimized = {}
    weibull_params_optimized = {}
    MEDIA_VARS_MODELING = []
    all_model_features = []
    TARGET_VAR_MODELING = 'Sales'


# Retrieve variables from global scope
trained_model = globals().get('model')
feature_scaler = globals().get('scaler')
original_df = globals().get('df_features') # Use df_features as the base for optimization
modeling_vars_info = globals().get('modeling_buckets', {})
adstock_params_optimized = globals().get('adstock_params_optimized', {})
weibull_params_optimized = globals().get('weibull_params_optimized', {})
all_model_features = globals().get('all_features', []) # List of all feature names used in the model (original names before scaling, but after transformation if applicable)


# Ensure buckets are defined
BASE_VARS_MODELING = modeling_vars_info.get('base_vars', [])
PROMO_VARS_MODELING = modeling_vars_info.get('promo_vars', [])
MEDIA_VARS_MODELING = modeling_vars_info.get('media_vars', []) # Original media variable names
TARGET_VAR_MODELING = modeling_vars_info.get('target_var', 'Sales')


# Ensure Fiscal_Year is in original_df for adstock transformation if needed
if original_df is not None and not original_df.empty and 'Fiscal_Year' not in original_df.columns and 'Week_Ending' in original_df.columns:
     try:
         # Assuming original_df index is Week_Ending datetime
         original_df['Fiscal_Year'] = original_df.index.year # Simpler year extraction if index is datetime
         print("✅ Created 'Fiscal_Year' column in original_df for adstock transformation.")
     except Exception as e:
         print(f"⚠️ Could not create 'Fiscal_Year' column in original_df: {e}")

fy_for_transform_optimization = original_df.get('Fiscal_Year', None) if original_df is not None else None


# Define the objective function (to minimize negative predicted sales)



# You MUST include the definitions for these functions in your final .py file,
# either by copying them directly above this line or importing them from another local file.
# Examples of functions you'll need to copy:
# - convert_indian_number (already included below as it was available from previous interactions)
# - assign_fiscal_year
# - enhanced_adstock
# - weibull_saturation
# - apply_enhanced_transformations
# - ConstrainedLinearRegression
# - optimize_transformation_params
# - enhanced_mmm_analysis
# - calculate_roi_by_fiscal_year
# - calculate_media_effectiveness
# - plot_avp_with_holdout (Adapt to return Plotly figs)
# - plot_contribution_by_bucket (Adapt to return Plotly figs)
# - plot_fy_comparison (Adapt to return Plotly figs)
# - plot_fy_pie_charts (Adapt to return Plotly figs)
# - plot_media_response_curves (Adapt to return Plotly figs)
# - run_scenario
# - compare_scenarios
# - objective_function
# - run_optimization


# --- Function to convert Indian number format to float (from your original code) ---
# This function is included here as it was provided in previous interactions
def convert_indian_number(value):
    """Convert Indian number format string to float"""
    if isinstance(value, str):
        cleaned_value = value.replace(',', '').strip()
        if cleaned_value in ['-', ''] or cleaned_value.isspace():
            return np.nan
        try:
            return float(cleaned_value)
        except ValueError:
            # In a Streamlit app, printing to console might not be seen easily.
            # You might want to add a warning to the app itself if conversion fails.
            # st.warning(f"Could not convert value: '{value}' to numeric.")
            return np.nan
    return value

# --- Simplified EDA function for Streamlit display (from previous integration) ---
# Adapting your original perform_comprehensive_eda to use Streamlit components
# You might need to copy the full implementation from your notebook if this is not sufficient
def perform_streamlit_eda(data, target_var='Sales'):
    """Perform EDA and display results using Streamlit"""
    st.subheader("Exploratory Data Analysis Results")

    # 1. Basic Information
    st.write("### 1. Basic Dataset Information")
    st.write(f"Shape: {data.shape}")
    st.write(f"Columns: {list(data.columns)}")
    if 'Week_Ending' in data.index.name: # Check if index is the date column
        st.write(f"Date Range: {data.index.min()} to {data.index.max()}") # Use index as date
    st.write(f"Missing Values: {data.isnull().sum().sum()}")

    # Check if target variable is numeric
    if target_var in data.columns and not pd.api.types.is_numeric_dtype(data[target_var]):
         st.warning(f"WARNING: Target variable '{target_var}' is not numeric after preprocessing.")


    # 2. Summary Statistics
    st.write("### 2. Summary Statistics")
    numeric_df = data.select_dtypes(include=[np.number])
    st.write("Numeric Variables Summary:")
    st.write(numeric_df.describe())
    st.write("Skewness and Kurtosis:")
    st.write(pd.concat([numeric_df.skew().to_frame('Skewness'), numeric_df.kurtosis().to_frame('Kurtosis')], axis=1))


    # 3. Univariate Analysis (Histograms)
    st.write("### 3. Univariate Analysis (Distributions)")
    numeric_cols = numeric_df.columns.tolist()
    for col in numeric_cols:
        fig, ax = plt.subplots(figsize=(10, 6)) # Create figure and axes
        sns.histplot(data=data, x=col, kde=True, bins=30, ax=ax)
        ax.set_title(f"Distribution of {col}")
        ax.set_ylabel("Frequency")

        # Add vertical lines for mean and median with improved formatting
        mean_val = data[col].mean()
        median_val = data[col].median()

        # Conditional formatting for labels and axis ticks based on column name
        if 'impressions' in col.lower() or 'clicks' in col.lower() or col == target_var:
            ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))
        elif 'discount' in col.lower():
             ax.xaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0)) # xmax=1.0 because data is 0-1 range


        ax.axvline(mean_val, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean_val:,.2f}')
        ax.axvline(median_val, color='green', linestyle='dashed', linewidth=1, label=f'Median: {median_val:,.2f}')
        ax.legend()
        st.pyplot(fig) # Display the matplotlib figure in Streamlit
        plt.close(fig) # Close the figure to free memory


    # 4. Bivariate Analysis (Scatter plots vs Target)
    st.write("### 4. Bivariate Analysis: Relationship with Target Variable")
    if target_var in numeric_cols:
        numeric_cols_for_scatter = numeric_cols.copy()
        numeric_cols_for_scatter.remove(target_var)

        for col in numeric_cols_for_scatter:
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.scatterplot(data=data, x=col, y=target_var, ax=ax)
            ax.set_title(f"{target_var} vs {col}")

            # Conditional formatting for axis ticks
            if 'impressions' in col.lower() or 'clicks' in col.lower() or col == target_var:
                ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))
                ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))
            elif 'discount' in col.lower():
                ax.xaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0))


            # Add correlation coefficient as text
            correlation = data[col].corr(data[target_var])
            ax.text(data[col].min() + (data[col].max() - data[col].min()) * 0.05,
                     data[target_var].max() - (data[target_var].max() - data[target_var].min()) * 0.05,
                     f"r = {correlation:.3f}",
                     fontsize=12, bbox=dict(facecolor='white', alpha=0.8))
            st.pyplot(fig)
            plt.close(fig)

    # 5. Time Series Analysis
    st.write("### 5. Time Series Analysis")
    if 'Week_Ending' in data.index.name: # Check if index is the date column
        fig, ax = plt.subplots(figsize=(12, 6))
        sns.lineplot(data=data, x=data.index, y=target_var, ax=ax) # Use index for x
        ax.set_title(f"{target_var} Over Time")
        ax.set_xlabel("Date")
        ax.set_ylabel(target_var)
        st.pyplot(fig)
        plt.close(fig)

        # Seasonal decomposition (simplified for app)
        st.write("Seasonal Decomposition:")
        try:
            # Ensure the data index is a datetime index for decomposition
            if isinstance(data.index, pd.DatetimeIndex):
                temp_series = data[target_var].copy()
                if temp_series.isna().any():
                    temp_series = temp_series.fillna(method='ffill').fillna(method='bfill')
                # Use a reasonable default period or allow user input in sidebar
                decomposition = seasonal_decompose(temp_series, period=st.session_state.get('seasonal_period', 52), model='additive', extrapolate_trend='freq') # Use session state for period

                fig = decomposition.plot() # seasonal_decompose plot is a matplotlib figure
                st.pyplot(fig)
                plt.close(fig)
            else:
                 st.warning("Date index required for seasonal decomposition.")
        except Exception as e:
            st.warning(f"Could not perform seasonal decomposition: {str(e)}")


    # 6. Correlation Analysis
    st.write("### 6. Correlation Analysis")
    st.write("Full Correlation Matrix:")
    numeric_df = data.select_dtypes(include=[np.number]) # Re-calculate numeric_df in case data changed
    corr = numeric_df.corr()
    fig, ax = plt.subplots(figsize=(12, 10))
    sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", linewidths=".5", ax=ax)
    ax.set_title("Correlation Matrix - All Variables")
    st.pyplot(fig)
    plt.close(fig)

    # 6.1 Media Execution Share (Bar Chart)
    st.write("### 6.1 Media Execution Share")
    media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video']
    media_cols = [col for col in numeric_df.columns if any(keyword in col.lower() for keyword in media_keywords)]
    media_cols = [col for col in media_cols if col in numeric_df.columns] # Ensure they are in numeric_df

    if media_cols:
        media_totals = numeric_df[media_cols].sum()
        total_media = media_totals.sum()

        if total_media > 0:
            media_share = (media_totals / total_media) * 100
            media_share = media_share.sort_values(ascending=False)

            fig, ax = plt.subplots(figsize=(10, 8))
            sns.barplot(x=media_share.index, y=media_share.values, palette='husl', ax=ax)
            ax.set_title("Media Execution Share by Channel")
            ax.set_xlabel("Channel")
            ax.set_ylabel("Share (%)")
            plt.xticks(rotation=-45, ha='left')
            plt.tight_layout()
            st.pyplot(fig)
            plt.close(fig)

            st.write("Media Execution Share Percentage:")
            st.write(media_share.round(2).to_frame("Share (%)"))
        else:
            st.info("Total media execution is 0. Cannot generate share chart.")


    # 7. Outlier Analysis
    st.write("### 7. Outlier Analysis")
    if target_var in numeric_df.columns:
        Q1 = data[target_var].quantile(0.25)
        Q3 = data[target_var].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = data[(data[target_var] < lower_bound) | (data[target_var] > upper_bound)]
        normal_data = data[~((data[target_var] < lower_bound) | (data[target_var] > upper_bound))]

        st.write(f"Number of potential outliers in {target_var}: {len(outliers)}")

        if len(outliers) > 0:
            st.write("Outlier values:")
            st.write(outliers[[target_var]]) # Display relevant columns

            fig, ax = plt.subplots(figsize=(12, 6))
            sns.lineplot(data=normal_data, x=normal_data.index, y=target_var, label='Normal Values', color='darkgreen', marker='o', markersize=5, ax=ax)
            sns.scatterplot(data=outliers, x=outliers.index, y=target_var, color='red', label='Outliers', s=100, marker='X', ax=ax)

            ax.axhline(upper_bound, color='red', linestyle='dashed', linewidth=1, label='Upper Bound')
            ax.axhline(lower_bound, color='red', linestyle='dashed', linewidth=1, label='Lower Bound')

            ax.set_title(f"Outlier Detection in {target_var}")
            ax.set_xlabel("Date")
            ax.set_ylabel(target_var)
            ax.legend()
            st.pyplot(fig)
            plt.close(fig)
    else:
        st.warning(f"Target variable '{target_var}' is not numeric. Cannot perform outlier analysis.")


# --- Feature Engineering Function (Adapted for Streamlit) ---
# You'll need to adapt the logic from your original feature_engineering_and_reporting_module
# This is a simplified placeholder to show the structure.
# You NEED to copy the full implementation of your feature_engineering_and_reporting_module
# and any helper functions it calls (like enhanced_adstock, weibull_saturation, assign_fiscal_year, etc.)
# below this comment block.
def perform_streamlit_feature_engineering(data, seasonal_period, target_var='Sales'):
    """Perform Feature Engineering and display results using Streamlit"""
    st.subheader("Feature Engineering & Media Reporting Results")

    df_engineered = data.copy()

    # 1. Seasonal Index (SIndex)
    st.write("### 1. Seasonal Decomposition (for SIndex)")
    try:
        if isinstance(df_engineered.index, pd.DatetimeIndex) and target_var in df_engineered.columns:
            temp_series = df_engineered[target_var].copy()
            if temp_series.isna().any():
                temp_series = temp_series.fillna(method='ffill').fillna(method='bfill')

            # Ensure the seasonal_period is valid
            if seasonal_period > 0 and seasonal_period <= len(temp_series):
                 decomposition = seasonal_decompose(temp_series, period=seasonal_period, model='additive', extrapolate_trend='freq')
                 df_engineered['SIndex'] = decomposition.seasonal.values
                 st.write(f"✅ Seasonal Index (SIndex) created using period {seasonal_period}.")
                 # Display seasonal decomposition plot
                 fig = decomposition.plot()
                 st.pyplot(fig)
                 plt.close(fig)
            else:
                 st.warning(f"⚠️ Invalid seasonal period ({seasonal_period}). Cannot create SIndex.")
        else:
             st.warning("⚠️ Date index and target variable required for SIndex creation.")
    except Exception as e:
        st.warning(f"❌ Could not perform seasonal decomposition: {str(e)}. SIndex not created.")


    # 2. Custom Dummy Variables (Add logic here based on user input widgets)
    st.write("### 2. Custom Dummy Variables")
    st.info("Add Streamlit widgets here for user to define custom dummy dates.")
    # Example: dummy_dates_input = st.text_input("Enter comma-separated dates (YYYY-MM-DD) for custom dummies:")
    # if dummy_dates_input:
    #     # Parse dates and create dummy columns
    #     pass # Add your dummy creation logic


    # 3. Split Variable (Add logic here based on user input widgets)
    st.write("### 3. Split Variable")
    st.info("Add Streamlit widgets here for user to select variable and split date.")
    # Example: split_var = st.selectbox("Select variable to split:", data.columns.tolist())
    # split_date = st.date_input("Select split date:")
    # if split_var and split_date:
    #     # Add your splitting logic (for volume and spend if applicable)
    #     pass

    # 4. Super Campaign (Add logic here based on user input widgets)
    st.write("### 4. Super Campaign")
    st.info("Add Streamlit widgets here for user to select variables for super campaign.")
    # Example: vars_to_combine = st.multiselect("Select variables for Super Campaign:", data.columns.tolist())
    # if vars_to_combine:
    #     # Add your super campaign creation logic (for volume and spend)
    #     pass


    # --- Media Spend Estimation & Reporting (Simplified) ---
    st.write("### 5. Media Spend Estimation & Reporting")
    st.info("Integrate your media spend estimation and reporting logic here.")
    st.info("You'll need widgets for users to input CPM/CPC rates by year.")
    # Example:
    # media_channels = ['Paid Social Impressions', 'Email Clicks', ...] # Get from data/config
    # estimated_rates = {}
    # for channel in media_channels:
    #     rate = st.number_input(f"Estimated rate for {channel}:", value=0.0) # Add year logic
    #     estimated_rates[channel] = rate
    #
    # # Calculate estimated spend and generate report table
    # if st.button("Generate Media Report"):
    #    media_report_df = calculate_media_report(df_engineered, estimated_rates) # Your function
    #    st.write("Media Performance Report:")
    #    st.dataframe(media_report_df) # Use st.dataframe or st.write(..., unsafe_allow_html=True) for formatted table


    # Return the engineered dataframe (including SIndex, dummies, splits, supers)
    return df_engineered


# --- Streamlit App Structure ---

st.set_page_config(layout="wide")

st.title("Marketing Mix Modeling (MMM) App")

# --- Sidebar for Inputs ---
st.sidebar.header("Upload Data")
uploaded_file = st.sidebar.file_uploader("Choose a CSV file", type="csv")

# Initialize session state variables
if 'data' not in st.session_state:
    st.session_state['data'] = None
if 'df_engineered' not in st.session_state:
    st.session_state['df_engineered'] = None
if 'seasonal_period' not in st.session_state:
    st.session_state['seasonal_period'] = 52 # Default seasonal period
if 'model_results' not in st.session_state:
    st.session_state['model_results'] = None
if 'scaler' not in st.session_state:
    st.session_state['scaler'] = None
if 'modeling_buckets' not in st.session_state:
     st.session_state['modeling_buckets'] = None # Store modeling buckets for later use


# --- Main Content Area ---
if uploaded_file is not None:
    # Load and preprocess data from the uploaded file
    try:
        # Simple loading with pandas, assuming similar preprocessing as your function
        data = pd.read_csv(uploaded_file)

        # Assuming 'Week_Ending' is the date column and 'Sales' is the target
        date_column = 'Week_Ending'
        target_variable = 'Sales'

        # Apply Indian number conversion to relevant columns if needed
        all_columns = data.columns.tolist()
        if date_column in all_columns:
            all_columns.remove(date_column)

        for col in all_columns:
             if data[col].dtype == 'object':
                 try:
                      data[col] = data[col].apply(convert_indian_number)
                 except Exception as e:
                      st.warning(f"Could not apply number conversion to column '{col}': {e}")


        # Convert date column and set index
        if date_column in data.columns:
            data[date_column] = pd.to_datetime(data[date_column], format='%d-%m-%Y %H:%M', errors='coerce', infer_datetime_format=True, cache=True)
            data = data.dropna(subset=[date_column]) # Drop rows where date conversion failed
            data = data.sort_values(date_column).set_index(date_column)
            data.index.name = 'Week_Ending' # Ensure index name is set
            st.sidebar.success("Data loaded and date index set successfully!")
            st.subheader("Raw Data Preview (with Date Index)")
            st.write(data.head())

            # Store loaded data in session state
            st.session_state['data'] = data.copy() # Store a copy
            st.session_state['target_variable'] = target_variable # Store target variable

        else:
            st.error(f"Date column '{date_column}' not found. Cannot proceed with analysis.")
            st.stop() # Stop the app if date column is missing


        # --- EDA Section ---
        st.sidebar.header("Exploratory Data Analysis (EDA)")
        if st.sidebar.button("Run EDA"):
            if st.session_state['data'] is not None:
                 # Assuming perform_streamlit_eda is defined (copied/imported) above
                 perform_streamlit_eda(st.session_state['data'].copy(), target_var=st.session_state['target_variable']) # Use data from session state
            else:
                 st.warning("Please upload data first.")


        # --- Feature Engineering Section ---
        st.sidebar.header("Feature Engineering")
        # Add controls for feature engineering options
        st.session_state['seasonal_period'] = st.sidebar.selectbox(
            "Seasonal Decomposition Period",
            [4, 13, 26, 52],
            index=[4, 13, 26, 52].index(st.session_state['seasonal_period']), # Set default based on session state
            key='seasonal_period_selectbox' # Add a unique key
        )

        # Placeholder for other FE controls (dummies, splits, supers) - Add Streamlit widgets for these
        st.sidebar.info("Add controls for Custom Dummies, Variable Splitting, Super Campaigns here.")
        # Example for dummy dates:
        # dummy_dates_input = st.text_input("Enter comma-separated dates (YYYY-MM-DD) for custom dummies:")
        # if dummy_dates_input:
        #     # Parse dates and create dummy columns
        #     pass # Add your dummy creation logic


        if st.sidebar.button("Run Feature Engineering"):
             if st.session_state['data'] is not None:
                 with st.spinner("Running Feature Engineering..."):
                     # Assuming perform_streamlit_feature_engineering is defined (copied/imported) above
                     # You'll need to pass other FE options (dummies, splits, supers) to this function
                     st.session_state['df_engineered'] = perform_streamlit_feature_engineering(
                         st.session_state['data'].copy(), # Use data from session state
                         st.session_state['seasonal_period'],
                         target_var=st.session_state['target_variable']
                         # Pass other FE parameters here
                     )
                     # The perform_streamlit_feature_engineering function should also return modeling_buckets
                     # For now, we'll just assume df_engineered is returned.
                     # You might need to adapt your function to return both.
                     # Example adaptation: return df_engineered, modeling_buckets

                 if st.session_state['df_engineered'] is not None:
                     st.subheader("Engineered Features Preview:")
                     st.write(st.session_state['df_engineered'].head())
                     st.success("Feature Engineering complete!")
                     # Display Media Performance Report if perform_streamlit_feature_engineering generates it

             else:
                 st.warning("Please upload data first.")


        # --- Model Training Section ---
        st.sidebar.header("Model Training")
        # Add controls for Model Training
        model_type = st.sidebar.selectbox("Select Model", ["Ridge", "Lasso", "ElasticNet"], key='model_type_selectbox')
        holdout_weeks = st.sidebar.number_input("Holdout Weeks", min_value=1, value=12, key='holdout_weeks_input')
        enforce_positive = st.sidebar.checkbox("Enforce Positive Media Coefficients", value=True, key='enforce_positive_checkbox')

        if st.sidebar.button("Train Model"):
           if st.session_state['df_engineered'] is not None:
               with st.spinner("Training Model..."):
                   # Prepare model_choice dictionary
                   model_choice = {
                       'model': model_type.lower(),
                       'grid': {}, # Define default grids or add widgets for hyperparameter tuning
                       'reasons': [], # Add widgets for reasons if needed
                       'notes': '',
                       'timestamp': pd.Timestamp.utcnow().isoformat() + 'Z',
                       'ready_to_train': True
                   }

                   # Assuming enhanced_mmm_analysis is defined (copied/imported) above
                   # enhanced_mmm_analysis function expects df_features (which is df_engineered here)
                   # and MODEL_CHOICE, HOLDOUT_WEEKS, enforce_positive
                   # It should return results dictionary and scaler
                   # It should also return modeling_buckets or ensure it's set in global scope if needed

                   # Need to get modeling_buckets here if enhanced_mmm_analysis doesn't return it
                   # Or adapt enhanced_mmm_analysis to take modeling_buckets as input

                   # For now, let's assume modeling_buckets is generated/available after FE or passed
                   # If your FE function returns modeling_buckets, update the FE section call.
                   # Example: st.session_state['df_engineered'], st.session_state['modeling_buckets'] = perform_streamlit_feature_engineering(...)

                   # If modeling_buckets is not returned by FE, you might need to reconstruct it
                   # or ensure your enhanced_mmm_analysis function can handle it.

                   # Temporary placeholder for modeling_buckets if not from FE:
                   if st.session_state['modeling_buckets'] is None:
                        st.warning("Modeling buckets not found in session state. Attempting to reconstruct basic buckets.")
                        # Basic reconstruction (may not match exact buckets from notebook FE)
                        numeric_cols = st.session_state['df_engineered'].select_dtypes(include=[np.number]).columns.tolist()
                        target_var = st.session_state['target_variable']
                        if target_var in numeric_cols:
                             numeric_cols.remove(target_var)

                        # Simple heuristic for buckets (needs refinement based on your actual data/FE)
                        base_vars = [col for col in numeric_cols if col not in ['Discount1', 'Discount2'] and 'impression' not in col.lower() and 'click' not in col.lower() and '_Spend' not in col]
                        promo_vars = [col for col in numeric_cols if 'Discount' in col]
                        media_vars = [col for col in numeric_cols if ('impression' in col.lower() or 'click' in col.lower()) and '_Spend' not in col]

                        st.session_state['modeling_buckets'] = {
                           'base_vars': base_vars,
                           'promo_vars': promo_vars,
                           'media_vars': media_vars,
                           'target_var': target_var
                       }
                        st.write("Reconstructed Basic Modeling Buckets:", st.session_state['modeling_buckets'])


                   if st.session_state['modeling_buckets'] is not None:
                       # Pass modeling_buckets to enhanced_mmm_analysis if needed
                       results, scaler = enhanced_mmm_analysis(
                           st.session_state['df_engineered'].copy(), # Use engineered data
                           model_choice,
                           holdout_weeks,
                           enforce_positive
                           # Pass modeling_buckets here if your function expects it
                       )

                       if results:
                           st.session_state['model_results'] = results
                           st.session_state['scaler'] = scaler # Store scaler

                           st.subheader("Model Training & Analysis Results")
                           st.write("Model Performance Metrics:", results['metrics'])

                           # Display visualizations (assuming Plotly figures are returned)
                           if results['visualizations'].get('avp_chart'):
                               st.plotly_chart(results['visualizations']['avp_chart'])
                           if results['visualizations'].get('contribution_timeline'):
                               st.plotly_chart(results['visualizations']['contribution_timeline'])
                           if results['visualizations'].get('fy_contribution_bars'):
                               st.plotly_chart(results['visualizations']['fy_contribution_bars'])
                           if results['visualizations'].get('fy_contribution_pies'):
                                # Check if pie charts were generated (can be None)
                                if results['visualizations']['fy_contribution_pies']:
                                    st.plotly_chart(results['visualizations']['fy_contribution_pies'])
                                else:
                                    st.info("Fiscal Year Pie Charts not generated (likely insufficient data).")
                           if results['visualizations'].get('response_curves'):
                                # Check if response curves were generated (can be None)
                                if results['visualizations']['response_curves']:
                                    st.plotly_chart(results['visualizations']['response_curves'])
                                else:
                                     st.info("Media Response Curves not generated (likely no paid media).")


                           # Display analysis tables
                           if 'fy_roi' in results['analyses'] and not results['analyses']['fy_roi'].empty:
                                st.write("ROI by Fiscal Year:")
                                # Display formatted HTML table (assuming your function returns HTML or format here)
                                # If fy_roi is a DataFrame, display it:
                                st.dataframe(results['analyses']['fy_roi'])
                           else:
                                st.info("No Fiscal Year ROI data to display.")

                           if 'media_effectiveness' in results['analyses'] and not results['analyses']['media_effectiveness'].empty:
                                st.write("Media Effectiveness Metrics:")
                                st.dataframe(results['analyses']['media_effectiveness'])
                           else:
                                st.info("No Media Effectiveness data to display.")

                           if 'fy_contributions' in results['analyses'] and not results['analyses']['fy_contributions'].empty:
                                st.write("Contributions by Fiscal Year:")
                                st.dataframe(results['analyses']['fy_contributions'])
                           else:
                                st.info("No Fiscal Year Contribution data to display.")


                           st.success("Model Training complete!")

                       else:
                           st.error("Model training failed.")

           else:
              st.warning("Please run Feature Engineering first.")


        # --- Scenario Analysis Section ---
        st.sidebar.header("Scenario Analysis")
        # Add controls for Scenario Analysis
        st.sidebar.info("Scenario analysis controls will be here. Requires a trained model.")

        # Example Scenario Controls (will need to be integrated):
        # if st.session_state.get('model_results') is not None and st.session_state.get('df_engineered') is not None:
        #     st.sidebar.subheader("Define Scenario")
        #     media_channels_for_scenario = st.session_state['modeling_buckets'].get('media_vars', [])
        #     if media_channels_for_scenario:
        #         scenario_channel = st.sidebar.selectbox("Select channel to change:", [''] + media_channels_for_scenario)
        #         if scenario_channel:
        #             change_type = st.sidebar.radio("Change type:", ["Percentage change", "Fixed spend value"])
        #             if change_type == "Percentage change":
        #                 spend_change_pct = st.sidebar.number_input("Percentage change (+/-):", value=0.0) / 100.0
        #                 fixed_spend_value = None
        #             else:
        #                 fixed_spend_value = st.sidebar.number_input("Fixed spend value:")
        #                 spend_change_pct = 0.0
        #
        #             if st.sidebar.button("Run Scenario"):
        #                 with st.spinner(f"Running Scenario: {scenario_channel} change..."):
        #                     # Assuming run_scenario function is defined (copied/imported)
        #                     # run_scenario expects base_df, scenario_name, media_channel_to_change, spend_change_pct, fixed_spend_value
        #                     # It also needs access to the trained model, scaler, and transformation parameters (via global scope or passed)
        #                     # You'll need to ensure these are accessible to the run_scenario function in your .py file
        #
        #                     # Need to ensure adstock/weibull params are available to run_scenario
        #                     # If your enhanced_mmm_analysis returns them in results['params'], you can store them
        #                     # st.session_state['adstock_params_optimized'] = results['params']['theta']
        #                     # st.session_state['weibull_params_optimized'] = results['params']['weibull']
        #
        #                     scenario_result = run_scenario(
        #                         st.session_state['df_engineered'].copy(), # Base data for scenario
        #                         scenario_name=f"{scenario_channel} Change",
        #                         media_channel_to_change=scenario_channel,
        #                         spend_change_pct=spend_change_pct,
        #                         fixed_spend_value=fixed_spend_value
        #                         # Pass model, scaler, adstock/weibull params if run_scenario expects them
        #                     )
        #
        #                     if scenario_result:
        #                         st.subheader(f"Scenario: {scenario_result['scenario_name']} Results")
        #                         st.write(f"Total Predicted Sales: {scenario_result['predicted_sales'].sum():,.0f}")
        #                         st.write(f"Total Incremental Sales (vs Base): {scenario_result['incremental_sales'].sum():,.0f}")
        #                         st.write(f"Total Scenario Spend: {scenario_result['total_spend']:,.0f}")
        #                         st.write(f"Total Incremental Spend (vs Base): {scenario_result['incremental_spend']:,.0f}")
        #                         # Format and display ROI
        #                         roi_formatted = f"{scenario_result['scenario_roi']:.2%}" if not np.isinf(scenario_result['scenario_roi']) and not np.isnan(scenario_result['scenario_roi']) else str(scenario_result['scenario_roi'])
        #                         st.write(f"Scenario ROI: {roi_formatted}")
        #                     else:
        #                         st.error("Scenario analysis failed.")
        #             else:
        #                 st.info("Select a channel to define a scenario.")
        #     else:
        #         st.info("No media channels identified for scenario analysis.")
        # else:
        #    st.info("Train the model first to enable scenario analysis.")


        # --- Optimization Section ---
        st.sidebar.header("Optimization")
        # Add controls for Optimization
        st.sidebar.info("Optimization controls will be here. Requires a trained model.")

        # Example Optimization Controls (will need to be integrated):
        # if st.session_state.get('model_results') is not None and st.session_state.get('df_engineered') is not None:
        #      st.sidebar.subheader("Run Optimization")
        #      total_volume_budget = st.sidebar.number_input(
        #          "Total Volume Budget",
        #          value=st.session_state['df_engineered'][st.session_state['modeling_buckets'].get('media_vars', [])].sum().sum() if st.session_state.get('modeling_buckets') else 0.0
        #      )
        #      # Add controls for min/max percentage constraints per channel
        #      # min_pct = st.sidebar.number_input("Min % of historical volume:", value=0.0)
        #      # max_pct = st.sidebar.number_input("Max % of historical volume:", value=1.0)
        #      st.sidebar.info("Add detailed channel constraints here.")
        #
        #      if st.sidebar.button("Run Optimization"):
        #          with st.spinner("Running Optimization..."):
        #              # Assuming run_optimization function is defined (copied/imported)
        #              # run_optimization expects total_budget, spend_min_pct, spend_max_pct, channel_min_spend, channel_max_spend
        #              # It also needs access to the trained model, scaler, adstock/weibull params, original_df (via global or passed)
        #              # Ensure original_df (st.session_state['data']) and df_engineered (st.session_state['df_engineered']) are accessible

        #              # Need to pass necessary data and parameters to run_optimization
        #              # You might need to adapt your run_optimization function's signature
        #
        #              optimization_results = run_optimization(
        #                  total_budget=total_volume_budget,
        #                  spend_min_pct=0.0, # Replace with widget values
        #                  spend_max_pct=float('inf') # Replace with widget values
        #                  # Pass model, scaler, adstock/weibull params if run_optimization expects them
        #              )
        #
        #              if optimization_results and optimization_results['success']:
        #                  st.subheader("Optimization Results")
        #                  st.write("Optimal Media Volume Allocation:")
        #                  st.dataframe(optimization_results['optimal_allocation_df'])
        #                  st.write(f"Predicted Total Sales at Optimal Allocation: {optimization_results['predicted_sales']:,.0f}")
        #              elif optimization_results:
        #                  st.error(f"Optimization failed: {optimization_results.get('message', 'Unknown error')}")
        #              else:
        #                  st.error("Optimization did not return results.")
        # else:
        #      st.info("Train the model first to enable optimization.")


    except Exception as e:
        st.error(f"An error occurred during data loading or initial processing: {e}")

else:
    st.info("Upload a CSV file to get started with the MMM analysis.")