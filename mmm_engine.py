# -*- coding: utf-8 -*-
"""MMM Engine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fp8Ag65Eeh1rs9VerXUkQ0LfZpZvSbLj
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# from datetime import datetime
# import warnings
# warnings.filterwarnings('ignore')
# 
# # Add other necessary imports
# from sklearn.model_selection import train_test_split, GridSearchCV
# from sklearn.linear_model import ElasticNet, Lasso, Ridge
# from sklearn.metrics import mean_squared_error, r2_score
# from sklearn.preprocessing import StandardScaler
# import statsmodels.api as sm
# from scipy.optimize import minimize
# import json
# 
# # Set page config
# st.set_page_config(page_title="Marketing Mix Model", layout="wide")
# 
# def main():
#     st.title("Marketing Mix Modeling Platform")
#     st.sidebar.title("Navigation")
# 
#     # Initialize session state
#     if 'data' not in st.session_state:
#         st.session_state.data = None
#     if 'model' not in st.session_state:
#         st.session_state.model = None
#     if 'results' not in st.session_state:
#         st.session_state.results = None
# 
#     # Navigation
#     pages = {
#         "Data Upload & Preprocessing": data_upload,
#         "EDA & Data Analysis": eda_analysis,
#         "Feature Engineering": feature_engineering,
#         "Model Training": model_training,
#         "Results & Interpretation": results_interpreter,
#         "Optimization & Scenario Planning": optimization
#     }
# 
#     page = st.sidebar.radio("Go to", list(pages.keys()))
#     pages[page]()
# 
# def data_upload():
#     st.header("Data Upload & Preprocessing")
# 
#     uploaded_file = st.file_uploader("Upload your CSV file", type=["csv"])
# 
#     if uploaded_file is not None:
#         try:
#             # Read data with Indian number format handling
#             df = pd.read_csv(uploaded_file, thousands=',', na_values=['-', 'NA', ''])
# 
#             # Convert date column
#             date_col = st.selectbox("Select date column", df.columns)
#             if date_col:
#                 df[date_col] = pd.to_datetime(df[date_col], format='%d-%m-%Y %H:%M', errors='coerce')
#                 df = df.sort_values(date_col).reset_index(drop=True)
# 
#             # Identify target variable
#             target_var = st.selectbox("Select target variable (sales)", df.columns)
# 
#             # Store in session state
#             st.session_state.data = df
#             st.session_state.date_col = date_col
#             st.session_state.target_var = target_var
# 
#             st.success("Data uploaded successfully!")
# 
#             # Check for data imbalance
#             check_data_imbalance(df, target_var)
# 
#             # Display basic info
#             st.subheader("Data Preview")
#             st.dataframe(df.head())
# 
#             st.subheader("Data Information")
#             col1, col2 = st.columns(2)
#             with col1:
#                 st.write("Shape:", df.shape)
#                 st.write("Date Range:", df[date_col].min(), "to", df[date_col].max())
#             with col2:
#                 st.write("Columns:", list(df.columns))
#                 st.write("Missing Values:", df.isnull().sum().sum())
# 
#         except Exception as e:
#             st.error(f"Error reading file: {str(e)}")
# 
# def check_data_imbalance(df, target_var):
#     """Check for data imbalance and suggest solutions"""
#     # Check for missing values
#     missing_vals = df.isnull().sum()
#     if missing_vals.sum() > 0:
#         st.warning(f"Data has {missing_vals.sum()} missing values. Consider imputation.")
#         if st.button("Show missing values details"):
#             st.write(missing_vals[missing_vals > 0])
# 
#     # Check for constant columns
#     constant_cols = [col for col in df.columns if df[col].nunique() <= 1]
#     if constant_cols:
#         st.warning(f"Constant columns detected: {constant_cols}. Consider removing them.")
# 
#     # Check for outliers in target variable
#     Q1 = df[target_var].quantile(0.25)
#     Q3 = df[target_var].quantile(0.75)
#     IQR = Q3 - Q1
#     outliers = ((df[target_var] < (Q1 - 1.5 * IQR)) | (df[target_var] > (Q3 + 1.5 * IQR))).sum()
#     if outliers > 0:
#         st.warning(f"Potential outliers detected in target variable: {outliers} values")
# 
# def eda_analysis():
#     st.header("Exploratory Data Analysis")
# 
#     if st.session_state.data is None:
#         st.warning("Please upload data first")
#         return
# 
#     df = st.session_state.data
#     target_var = st.session_state.target_var
#     date_col = st.session_state.date_col
# 
#     # Select variables for analysis
#     numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
#     selected_vars = st.multiselect("Select variables for analysis", numeric_cols, default=[target_var])
# 
#     if selected_vars:
#         # Time series plot
#         st.subheader("Time Series Analysis")
#         fig, ax = plt.subplots(figsize=(12, 6))
#         for var in selected_vars:
#             ax.plot(df[date_col], df[var], label=var)
#         ax.set_xlabel("Date")
#         ax.set_ylabel("Value")
#         ax.legend()
#         ax.tick_params(axis='x', rotation=45)
#         st.pyplot(fig)
# 
#         # Correlation matrix
#         st.subheader("Correlation Matrix")
#         corr_matrix = df[selected_vars].corr()
#         fig, ax = plt.subplots(figsize=(10, 8))
#         sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=ax)
#         st.pyplot(fig)
# 
#         # Distribution plots
#         st.subheader("Distribution of Variables")
#         for var in selected_vars:
#             fig, ax = plt.subplots(figsize=(8, 4))
#             sns.histplot(df[var], kde=True, ax=ax)
#             ax.set_title(f"Distribution of {var}")
#             st.pyplot(fig)
# 
# def feature_engineering():
#     st.header("Feature Engineering")
# 
#     if st.session_state.data is None:
#         st.warning("Please upload data first")
#         return
# 
#     df = st.session_state.data.copy()
#     date_col = st.session_state.date_col
# 
#     st.subheader("Date-Based Features")
#     # Extract date components
#     df['year'] = df[date_col].dt.year
#     df['month'] = df[date_col].dt.month
#     df['week'] = df[date_col].dt.isocalendar().week
#     df['quarter'] = df[date_col].dt.quarter
# 
#     # Create holiday dummies
#     st.write("### Add Holiday Dummies")
#     holiday_dates = st.text_area("Enter holiday dates (YYYY-MM-DD, one per line)")
# 
#     if holiday_dates:
#         holidays = [datetime.strptime(date.strip(), '%Y-%m-%d') for date in holiday_dates.split('\n') if date.strip()]
#         df['holiday'] = df[date_col].isin(holidays).astype(int)
#         st.success(f"Added {len(holidays)} holiday dummies")
# 
#     st.subheader("Media Transformation Options")
# 
#     # Select media variables
#     media_vars = st.multiselect("Select media variables", df.select_dtypes(include=[np.number]).columns.tolist())
# 
#     if media_vars:
#         col1, col2 = st.columns(2)
# 
#         with col1:
#             st.write("### Adstock Parameters")
#             adstock_decay = {}
#             for var in media_vars:
#                 adstock_decay[var] = st.slider(f"Adstock decay for {var}", 0.0, 1.0, 0.5, 0.05)
# 
#         with col2:
#             st.write("### Saturation Parameters")
#             saturation_params = {}
#             for var in media_vars:
#                 saturation_params[var] = st.slider(f"Saturation for {var}", 0.1, 1.0, 0.7, 0.05)
# 
#         # Apply transformations
#         for var in media_vars:
#             df[f"{var}_adstock"] = apply_adstock(df[var], adstock_decay[var])
#             df[f"{var}_saturated"] = apply_saturation(df[f"{var}_adstock"], saturation_params[var])
# 
#         st.success("Media transformations applied!")
# 
#     st.subheader("Super Campaigns")
#     create_super_campaigns = st.checkbox("Create super campaigns")
# 
#     if create_super_campaigns:
#         campaign_name = st.text_input("Super campaign name")
#         selected_media = st.multiselect("Select media for super campaign", media_vars)
# 
#         if campaign_name and selected_media:
#             df[f"super_{campaign_name}"] = df[selected_media].sum(axis=1)
#             st.success(f"Created super campaign: {campaign_name}")
# 
#     st.subheader("Variable Splitting")
#     split_var = st.selectbox("Select variable to split", [None] + df.columns.tolist())
# 
#     if split_var:
#         split_value = st.slider("Split threshold", float(df[split_var].min()), float(df[split_var].max()),
#                                float(df[split_var].median()))
#         df[f"{split_var}_low"] = (df[split_var] <= split_value).astype(int)
#         df[f"{split_var}_high"] = (df[split_var] > split_value).astype(int)
#         st.success(f"Split {split_var} at value {split_value}")
# 
#     # Update session state
#     st.session_state.processed_data = df
# 
#     st.subheader("Processed Data Preview")
#     st.dataframe(df.head())
# 
# def apply_adstock(series, decay):
#     """Apply adstock transformation to a series"""
#     adstock = np.zeros_like(series)
#     adstock[0] = series[0]
#     for i in range(1, len(series)):
#         adstock[i] = series[i] + decay * adstock[i-1]
#     return adstock
# 
# def apply_saturation(series, saturation):
#     """Apply saturation transformation to a series"""
#     return series ** saturation
# 
# def model_training():
#     st.header("Model Training")
# 
#     if 'processed_data' not in st.session_state:
#         st.warning("Please process data in Feature Engineering first")
#         return
# 
#     df = st.session_state.processed_data
#     target_var = st.session_state.target_var
# 
#     # Select features
#     all_features = [col for col in df.columns if col != target_var and not pd.api.types.is_datetime64_any_dtype(df[col])]
#     selected_features = st.multiselect("Select features for model", all_features, default=all_features)
# 
#     if not selected_features:
#         st.warning("Please select at least one feature")
#         return
# 
#     # Prepare data
#     X = df[selected_features]
#     y = df[target_var]
# 
#     # Handle missing values
#     if X.isnull().sum().sum() > 0:
#         st.warning("Data contains missing values. Using mean imputation.")
#         X = X.fillna(X.mean())
# 
#     # Train-test split
#     test_size = st.slider("Test set size", 0.1, 0.5, 0.2, 0.05)
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)
# 
#     # Scale features
#     scaler = StandardScaler()
#     X_train_scaled = scaler.fit_transform(X_train)
#     X_test_scaled = scaler.transform(X_test)
# 
#     # Model selection
#     model_type = st.selectbox("Select model type", ["ElasticNet", "Lasso", "Ridge"])
# 
#     # Hyperparameter tuning
#     if model_type == "ElasticNet":
#         param_grid = {
#             'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],
#             'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]
#         }
#         model = ElasticNet()
#     elif model_type == "Lasso":
#         param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]}
#         model = Lasso()
#     else:  # Ridge
#         param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]}
#         model = Ridge()
# 
#     # Grid search
#     grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')
#     grid_search.fit(X_train_scaled, y_train)
# 
#     # Best model
#     best_model = grid_search.best_estimator_
#     st.session_state.model = best_model
#     st.session_state.scaler = scaler
#     st.session_state.features = selected_features
# 
#     # Evaluate model
#     y_pred = best_model.predict(X_test_scaled)
#     mse = mean_squared_error(y_test, y_pred)
#     r2 = r2_score(y_test, y_pred)
# 
#     st.subheader("Model Performance")
#     col1, col2, col3 = st.columns(3)
#     col1.metric("Best Parameters", str(grid_search.best_params_))
#     col2.metric("MSE", f"{mse:.2f}")
#     col3.metric("R²", f"{r2:.4f}")
# 
#     # Plot predictions vs actual
#     fig, ax = plt.subplots(figsize=(10, 6))
#     ax.plot(y_test.values, label='Actual')
#     ax.plot(y_pred, label='Predicted')
#     ax.set_xlabel("Time")
#     ax.set_ylabel(target_var)
#     ax.legend()
#     ax.set_title("Actual vs Predicted Values")
#     st.pyplot(fig)
# 
#     # Feature importance
#     if hasattr(best_model, 'coef_'):
#         importance = pd.DataFrame({
#             'feature': selected_features,
#             'coefficient': best_model.coef_
#         }).sort_values('coefficient', key=abs, ascending=False)
# 
#         st.subheader("Feature Importance")
#         fig, ax = plt.subplots(figsize=(10, 8))
#         sns.barplot(x='coefficient', y='feature', data=importance, ax=ax)
#         ax.set_title("Feature Coefficients")
#         st.pyplot(fig)
# 
# def results_interpreter():
#     st.header("Results Interpretation")
# 
#     if st.session_state.model is None:
#         st.warning("Please train a model first")
#         return
# 
#     model = st.session_state.model
#     df = st.session_state.processed_data
#     features = st.session_state.features
#     target_var = st.session_state.target_var
# 
#     # Calculate contributions
#     X = df[features]
#     X_scaled = st.session_state.scaler.transform(X)
# 
#     if hasattr(model, 'coef_'):
#         contributions = X_scaled * model.coef_
#         contributions_df = pd.DataFrame(contributions, columns=[f"contribution_{f}" for f in features])
#         contributions_df['base'] = model.intercept_ if hasattr(model, 'intercept_') else 0
#         contributions_df['total'] = contributions_df.sum(axis=1)
#         contributions_df[target_var] = df[target_var].values
# 
#         # Plot contributions over time
#         st.subheader("Sales Decomposition")
#         fig, ax = plt.subplots(figsize=(12, 8))
#         ax.stackplot(df.index, contributions_df[[f"contribution_{f}" for f in features]].T,
#                     labels=features, alpha=0.7)
#         ax.plot(df.index, contributions_df['total'], label='Total Predicted', color='black', linewidth=2)
#         ax.plot(df.index, contributions_df[target_var], label='Actual', color='red', linestyle='--')
#         ax.legend(loc='upper left')
#         ax.set_xlabel("Time")
#         ax.set_ylabel(target_var)
#         ax.set_title("Sales Decomposition by Channel")
#         st.pyplot(fig)
# 
#         # ROI calculation (if spend data is available)
#         st.subheader("ROI Analysis")
#         # This would require actual spend data, which we don't have
#         # For demonstration, we'll use the coefficients as efficiency measures
# 
#         efficiency = pd.DataFrame({
#             'channel': features,
#             'efficiency': model.coef_
#         }).sort_values('efficiency', ascending=False)
# 
#         st.dataframe(efficiency)
# 
# def optimization():
#     st.header("Optimization & Scenario Planning")
# 
#     if st.session_state.model is None:
#         st.warning("Please train a model first")
#         return
# 
#     model = st.session_state.model
#     features = st.session_state.features
#     scaler = st.session_state.scaler
# 
#     st.subheader("Budget Optimization")
# 
#     # Get current media spend (approximate)
#     media_vars = [f for f in features if any(x in f for x in ['impressions', 'clicks', 'spend'])]
# 
#     if not media_vars:
#         st.warning("No media variables found for optimization")
#         return
# 
#     current_spend = {}
#     for var in media_vars:
#         current_spend[var] = st.number_input(f"Current spend for {var}", value=10000.0, step=1000.0)
# 
#     total_budget = st.number_input("Total marketing budget", value=sum(current_spend.values()), step=1000.0)
# 
#     # Define optimization function
#     def objective(x):
#         # x is the allocation to each media channel
#         # Create a base scenario with mean values for non-media variables
#         base_values = np.zeros(len(features))
#         for i, f in enumerate(features):
#             if f in media_vars:
#                 base_values[i] = x[media_vars.index(f)]
#             else:
#                 base_values[i] = np.mean(st.session_state.processed_data[f])
# 
#         # Scale and predict
#         base_values_scaled = scaler.transform(base_values.reshape(1, -1))
#         return -model.predict(base_values_scaled)[0]  # Negative for minimization
# 
#     # Constraints
#     constraints = (
#         {'type': 'eq', 'fun': lambda x: total_budget - sum(x)}  # Budget constraint
#     )
# 
#     # Bounds
#     bounds = [(0, total_budget) for _ in media_vars]
# 
#     # Initial guess
#     x0 = [current_spend[var] for var in media_vars]
# 
#     if st.button("Run Optimization"):
#         result = minimize(objective, x0, method='SLSQP', bounds=bounds, constraints=constraints)
# 
#         if result.success:
#             optimized_allocation = result.x
#             current_sales = -objective([current_spend[var] for var in media_vars])
#             optimized_sales = -result.fun
# 
#             st.subheader("Optimization Results")
#             col1, col2 = st.columns(2)
# 
#             with col1:
#                 st.metric("Current Sales", f"${current_sales:,.2f}")
#                 st.metric("Optimized Sales", f"${optimized_sales:,.2f}")
#                 st.metric("Improvement", f"${optimized_sales - current_sales:,.2f}")
# 
#             with col2:
#                 allocation_df = pd.DataFrame({
#                     'Channel': media_vars,
#                     'Current': [current_spend[var] for var in media_vars],
#                     'Optimized': optimized_allocation,
#                     'Change': optimized_allocation - [current_spend[var] for var in media_vars]
#                 })
#                 st.dataframe(allocation_df)
# 
#             # Plot allocation comparison
#             fig, ax = plt.subplots(figsize=(10, 6))
#             x = np.arange(len(media_vars))
#             width = 0.35
#             ax.bar(x - width/2, [current_spend[var] for var in media_vars], width, label='Current')
#             ax.bar(x + width/2, optimized_allocation, width, label='Optimized')
#             ax.set_xlabel('Channels')
#             ax.set_ylabel('Spend')
#             ax.set_title('Current vs Optimized Spend Allocation')
#             ax.set_xticks(x)
#             ax.set_xticklabels(media_vars, rotation=45)
#             ax.legend()
#             st.pyplot(fig)
# 
#         else:
#             st.error("Optimization failed: " + result.message)
# 
#     st.subheader("Scenario Planning")
#     scenario_name = st.text_input("Scenario name")
# 
#     if scenario_name:
#         scenario_changes = {}
#         for var in media_vars:
#             change = st.slider(f"Change for {var} (%)", -100, 100, 0, 5)
#             scenario_changes[var] = change / 100
# 
#         if st.button("Evaluate Scenario"):
#             # Calculate new values
#             new_values = {}
#             for var in media_vars:
#                 new_values[var] = current_spend[var] * (1 + scenario_changes[var])
# 
#             # Predict outcome
#             base_values = np.zeros(len(features))
#             for i, f in enumerate(features):
#                 if f in media_vars:
#                     base_values[i] = new_values[f]
#                 else:
#                     base_values[i] = np.mean(st.session_state.processed_data[f])
# 
#             base_values_scaled = scaler.transform(base_values.reshape(1, -1))
#             scenario_sales = model.predict(base_values_scaled)[0]
#             current_sales = -objective([current_spend[var] for var in media_vars])
# 
#             st.metric("Scenario Sales", f"${scenario_sales:,.2f}",
#                      delta=f"${scenario_sales - current_sales:,.2f}")
# 
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# %env STREAMLIT_GATHER_USAGE_STATS=false
!streamlit run app.py & npx localtunnel --port 8501