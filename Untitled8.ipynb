{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy7rieCZtPRat3d3UqzaC6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhav5086-lab/MMM-Platform/blob/main/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KRF445BKHKe"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose # Import seasonal_decompose\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from scipy.optimize import curve_fit, nnls, minimize, Bounds, LinearConstraint # Import for optimization\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import io # Import the io module\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import for tick formatting in plots\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "# Import display and HTML for rendering the media report table (though st.write(..., unsafe_allow_html=True) is preferred)\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n",
        "# --- Custom Functions (Copy these from your notebook or import them if in a separate file) ---\n",
        "\n",
        "# You MUST include the definitions for these functions in your final .py file,\n",
        "# either by copying them directly above this line or importing them from another local file.\n",
        "# Examples of functions you'll need to copy:\n",
        "# - assign_fiscal_year\n",
        "# - enhanced_adstock\n",
        "# - weibull_saturation\n",
        "# - apply_enhanced_transformations\n",
        "# - ConstrainedLinearRegression\n",
        "# - optimize_transformation_params\n",
        "# - enhanced_mmm_analysis\n",
        "# - calculate_roi_by_fiscal_year\n",
        "# - calculate_media_effectiveness\n",
        "# - plot_avp_with_holdout (Adapt to return Plotly figs)\n",
        "# - plot_contribution_by_bucket (Adapt to return Plotly figs)\n",
        "# - plot_fy_comparison (Adapt to return Plotly figs)\n",
        "# - plot_fy_pie_charts (Adapt to return Plotly figs)\n",
        "# - plot_media_response_curves (Adapt to return Plotly figs)\n",
        "# - run_scenario\n",
        "# - compare_scenarios\n",
        "# - objective_function\n",
        "# - run_optimization\n",
        "\n",
        "\n",
        "# --- Function to convert Indian number format to float (from your original code) ---\n",
        "# This function is included here as it was provided in previous interactions\n",
        "def convert_indian_number(value):\n",
        "    \"\"\"Convert Indian number format string to float\"\"\"\n",
        "    if isinstance(value, str):\n",
        "        cleaned_value = value.replace(',', '').strip()\n",
        "        if cleaned_value in ['-', ''] or cleaned_value.isspace():\n",
        "            return np.nan\n",
        "        try:\n",
        "            return float(cleaned_value)\n",
        "        except ValueError:\n",
        "            # In a Streamlit app, printing to console might not be seen easily.\n",
        "            # You might want to add a warning to the app itself if conversion fails.\n",
        "            # st.warning(f\"Could not convert value: '{value}' to numeric.\")\n",
        "            return np.nan\n",
        "    return value\n",
        "\n",
        "# --- Simplified EDA function for Streamlit display (from previous integration) ---\n",
        "# Adapting your original perform_comprehensive_eda to use Streamlit components\n",
        "def perform_streamlit_eda(data, target_var='Sales'):\n",
        "    \"\"\"Perform EDA and display results using Streamlit\"\"\"\n",
        "    st.subheader(\"Exploratory Data Analysis Results\")\n",
        "\n",
        "    # 1. Basic Information\n",
        "    st.write(\"### 1. Basic Dataset Information\")\n",
        "    st.write(f\"Shape: {data.shape}\")\n",
        "    st.write(f\"Columns: {list(data.columns)}\")\n",
        "    if 'Week_Ending' in data.index.name: # Check if index is the date column\n",
        "        st.write(f\"Date Range: {data.index.min()} to {data.index.max()}\") # Use index as date\n",
        "    st.write(f\"Missing Values: {data.isnull().sum().sum()}\")\n",
        "\n",
        "    # Check if target variable is numeric\n",
        "    if target_var in data.columns and not pd.api.types.is_numeric_dtype(data[target_var]):\n",
        "         st.warning(f\"WARNING: Target variable '{target_var}' is not numeric after preprocessing.\")\n",
        "\n",
        "\n",
        "    # 2. Summary Statistics\n",
        "    st.write(\"### 2. Summary Statistics\")\n",
        "    numeric_df = data.select_dtypes(include=[np.number])\n",
        "    st.write(\"Numeric Variables Summary:\")\n",
        "    st.write(numeric_df.describe())\n",
        "    st.write(\"Skewness and Kurtosis:\")\n",
        "    st.write(pd.concat([numeric_df.skew().to_frame('Skewness'), numeric_df.kurtosis().to_frame('Kurtosis')], axis=1))\n",
        "\n",
        "\n",
        "    # 3. Univariate Analysis (Histograms)\n",
        "    st.write(\"### 3. Univariate Analysis (Distributions)\")\n",
        "    numeric_cols = numeric_df.columns.tolist()\n",
        "    for col in numeric_cols:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6)) # Create figure and axes\n",
        "        sns.histplot(data=data, x=col, kde=True, bins=30, ax=ax)\n",
        "        ax.set_title(f\"Distribution of {col}\")\n",
        "        ax.set_ylabel(\"Frequency\")\n",
        "\n",
        "        # Add vertical lines for mean and median with improved formatting\n",
        "        mean_val = data[col].mean()\n",
        "        median_val = data[col].median()\n",
        "\n",
        "        # Conditional formatting for labels and axis ticks based on column name\n",
        "        if 'impressions' in col.lower() or 'clicks' in col.lower() or col == target_var:\n",
        "            ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))\n",
        "        elif 'discount' in col.lower():\n",
        "             ax.xaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0)) # xmax=1.0 because data is 0-1 range\n",
        "\n",
        "\n",
        "        ax.axvline(mean_val, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean_val:,.2f}')\n",
        "        ax.axvline(median_val, color='green', linestyle='dashed', linewidth=1, label=f'Median: {median_val:,.2f}')\n",
        "        ax.legend()\n",
        "        st.pyplot(fig) # Display the matplotlib figure in Streamlit\n",
        "        plt.close(fig) # Close the figure to free memory\n",
        "\n",
        "\n",
        "    # 4. Bivariate Analysis (Scatter plots vs Target)\n",
        "    st.write(\"### 4. Bivariate Analysis: Relationship with Target Variable\")\n",
        "    if target_var in numeric_cols:\n",
        "        numeric_cols_for_scatter = numeric_cols.copy()\n",
        "        numeric_cols_for_scatter.remove(target_var)\n",
        "\n",
        "        for col in numeric_cols_for_scatter:\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            sns.scatterplot(data=data, x=col, y=target_var, ax=ax)\n",
        "            ax.set_title(f\"{target_var} vs {col}\")\n",
        "\n",
        "            # Conditional formatting for axis ticks\n",
        "            if 'impressions' in col.lower() or 'clicks' in col.lower() or col == target_var:\n",
        "                ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))\n",
        "                ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))\n",
        "            elif 'discount' in col.lower():\n",
        "                ax.xaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0))\n",
        "\n",
        "\n",
        "            # Add correlation coefficient as text\n",
        "            correlation = data[col].corr(data[target_var])\n",
        "            ax.text(data[col].min() + (data[col].max() - data[col].min()) * 0.05,\n",
        "                     data[target_var].max() - (data[target_var].max() - data[target_var].min()) * 0.05,\n",
        "                     f\"r = {correlation:.3f}\",\n",
        "                     fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
        "            st.pyplot(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "    # 5. Time Series Analysis\n",
        "    st.write(\"### 5. Time Series Analysis\")\n",
        "    if 'Week_Ending' in data.index.name: # Check if index is the date column\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        sns.lineplot(data=data, x=data.index, y=target_var, ax=ax) # Use index for x\n",
        "        ax.set_title(f\"{target_var} Over Time\")\n",
        "        ax.set_xlabel(\"Date\")\n",
        "        ax.set_ylabel(target_var)\n",
        "        st.pyplot(fig)\n",
        "        plt.close(fig)\n",
        "\n",
        "        # Seasonal decomposition (simplified for app)\n",
        "        st.write(\"Seasonal Decomposition:\")\n",
        "        try:\n",
        "            # Ensure the data index is a datetime index for decomposition\n",
        "            if isinstance(data.index, pd.DatetimeIndex):\n",
        "                temp_series = data[target_var].copy()\n",
        "                if temp_series.isna().any():\n",
        "                    temp_series = temp_series.fillna(method='ffill').fillna(method='bfill')\n",
        "                # Use a reasonable default period or allow user input in sidebar\n",
        "                decomposition = seasonal_decompose(temp_series, period=st.session_state.get('seasonal_period', 52), model='additive', extrapolate_trend='freq') # Use session state for period\n",
        "\n",
        "                fig = decomposition.plot() # seasonal_decompose plot is a matplotlib figure\n",
        "                st.pyplot(fig)\n",
        "                plt.close(fig)\n",
        "            else:\n",
        "                 st.warning(\"Date index required for seasonal decomposition.\")\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Could not perform seasonal decomposition: {str(e)}\")\n",
        "\n",
        "\n",
        "    # 6. Correlation Analysis\n",
        "    st.write(\"### 6. Correlation Analysis\")\n",
        "    st.write(\"Full Correlation Matrix:\")\n",
        "    numeric_df = data.select_dtypes(include=[np.number]) # Re-calculate numeric_df in case data changed\n",
        "    corr = numeric_df.corr()\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=\".5\", ax=ax)\n",
        "    ax.set_title(\"Correlation Matrix - All Variables\")\n",
        "    st.pyplot(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # 6.1 Media Execution Share (Bar Chart)\n",
        "    st.write(\"### 6.1 Media Execution Share\")\n",
        "    media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video']\n",
        "    media_cols = [col for col in numeric_df.columns if any(keyword in col.lower() for keyword in media_keywords)]\n",
        "    media_cols = [col for col in media_cols if col in numeric_df.columns] # Ensure they are in numeric_df\n",
        "\n",
        "    if media_cols:\n",
        "        media_totals = numeric_df[media_cols].sum()\n",
        "        total_media = media_totals.sum()\n",
        "\n",
        "        if total_media > 0:\n",
        "            media_share = (media_totals / total_media) * 100\n",
        "            media_share = media_share.sort_values(ascending=False)\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 8))\n",
        "            sns.barplot(x=media_share.index, y=media_share.values, palette='husl', ax=ax)\n",
        "            ax.set_title(\"Media Execution Share by Channel\")\n",
        "            ax.set_xlabel(\"Channel\")\n",
        "            ax.set_ylabel(\"Share (%)\")\n",
        "            plt.xticks(rotation=-45, ha='left')\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "            st.write(\"Media Execution Share Percentage:\")\n",
        "            st.write(media_share.round(2).to_frame(\"Share (%)\"))\n",
        "        else:\n",
        "            st.info(\"Total media execution is 0. Cannot generate share chart.\")\n",
        "\n",
        "\n",
        "    # 7. Outlier Analysis\n",
        "    st.write(\"### 7. Outlier Analysis\")\n",
        "    if target_var in numeric_df.columns:\n",
        "        Q1 = data[target_var].quantile(0.25)\n",
        "        Q3 = data[target_var].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        outliers = data[(data[target_var] < lower_bound) | (data[target_var] > upper_bound)]\n",
        "        normal_data = data[~((data[target_var] < lower_bound) | (data[target_var] > upper_bound))]\n",
        "\n",
        "        st.write(f\"Number of potential outliers in {target_var}: {len(outliers)}\")\n",
        "\n",
        "        if len(outliers) > 0:\n",
        "            st.write(\"Outlier values:\")\n",
        "            st.write(outliers[[target_var]]) # Display relevant columns\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(12, 6))\n",
        "            sns.lineplot(data=normal_data, x=normal_data.index, y=target_var, label='Normal Values', color='darkgreen', marker='o', markersize=5, ax=ax)\n",
        "            sns.scatterplot(data=outliers, x=outliers.index, y=target_var, color='red', label='Outliers', s=100, marker='X', ax=ax)\n",
        "\n",
        "            ax.axhline(upper_bound, color='red', linestyle='dashed', linewidth=1, label='Upper Bound')\n",
        "            ax.axhline(lower_bound, color='red', linestyle='dashed', linewidth=1, label='Lower Bound')\n",
        "\n",
        "            ax.set_title(f\"Outlier Detection in {target_var}\")\n",
        "            ax.set_xlabel(\"Date\")\n",
        "            ax.set_ylabel(target_var)\n",
        "            ax.legend()\n",
        "            st.pyplot(fig)\n",
        "            plt.close(fig)\n",
        "    else:\n",
        "        st.warning(f\"Target variable '{target_var}' is not numeric. Cannot perform outlier analysis.\")\n",
        "\n",
        "\n",
        "# --- Feature Engineering Function (Adapted for Streamlit) ---\n",
        "# You'll need to adapt the logic from your original feature_engineering_and_reporting_module\n",
        "# This is a simplified placeholder to show the structure.\n",
        "def perform_streamlit_feature_engineering(data, seasonal_period, target_var='Sales'):\n",
        "    \"\"\"Perform Feature Engineering and display results using Streamlit\"\"\"\n",
        "    st.subheader(\"Feature Engineering & Media Reporting Results\")\n",
        "\n",
        "    df_engineered = data.copy()\n",
        "\n",
        "    # 1. Seasonal Index (SIndex)\n",
        "    st.write(\"### 1. Seasonal Decomposition (for SIndex)\")\n",
        "    try:\n",
        "        if isinstance(df_engineered.index, pd.DatetimeIndex) and target_var in df_engineered.columns:\n",
        "            temp_series = df_engineered[target_var].copy()\n",
        "            if temp_series.isna().any():\n",
        "                temp_series = temp_series.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "            # Ensure the seasonal_period is valid\n",
        "            if seasonal_period > 0 and seasonal_period <= len(temp_series):\n",
        "                 decomposition = seasonal_decompose(temp_series, period=seasonal_period, model='additive', extrapolate_trend='freq')\n",
        "                 df_engineered['SIndex'] = decomposition.seasonal.values\n",
        "                 st.write(f\"✅ Seasonal Index (SIndex) created using period {seasonal_period}.\")\n",
        "                 # Display seasonal decomposition plot\n",
        "                 fig = decomposition.plot()\n",
        "                 st.pyplot(fig)\n",
        "                 plt.close(fig)\n",
        "            else:\n",
        "                 st.warning(f\"⚠️ Invalid seasonal period ({seasonal_period}). Cannot create SIndex.\")\n",
        "        else:\n",
        "             st.warning(\"⚠️ Date index and target variable required for SIndex creation.\")\n",
        "    except Exception as e:\n",
        "        st.warning(f\"❌ Could not perform seasonal decomposition: {str(e)}. SIndex not created.\")\n",
        "\n",
        "\n",
        "    # 2. Custom Dummy Variables (Add logic here based on user input widgets)\n",
        "    st.write(\"### 2. Custom Dummy Variables\")\n",
        "    st.info(\"Add Streamlit widgets here for user to define custom dummy dates.\")\n",
        "    # Example: dummy_dates_input = st.text_input(\"Enter comma-separated dates (YYYY-MM-DD) for custom dummies:\")\n",
        "    # if dummy_dates_input:\n",
        "    #     # Parse dates and create dummy columns\n",
        "    #     pass # Add your dummy creation logic\n",
        "\n",
        "\n",
        "    # 3. Split Variable (Add logic here based on user input widgets)\n",
        "    st.write(\"### 3. Split Variable\")\n",
        "    st.info(\"Add Streamlit widgets here for user to select variable and split date.\")\n",
        "    # Example: split_var = st.selectbox(\"Select variable to split:\", data.columns.tolist())\n",
        "    # split_date = st.date_input(\"Select split date:\")\n",
        "    # if split_var and split_date:\n",
        "    #     # Add your splitting logic (for volume and spend if applicable)\n",
        "    #     pass\n",
        "\n",
        "    # 4. Super Campaign (Add logic here based on user input widgets)\n",
        "    st.write(\"### 4. Super Campaign\")\n",
        "    st.info(\"Add Streamlit widgets here for user to select variables for super campaign.\")\n",
        "    # Example: vars_to_combine = st.multiselect(\"Select variables for Super Campaign:\", data.columns.tolist())\n",
        "    # if vars_to_combine:\n",
        "    #     # Add your super campaign creation logic (for volume and spend)\n",
        "    #     pass\n",
        "\n",
        "\n",
        "    # --- Media Spend Estimation & Reporting (Simplified) ---\n",
        "    st.write(\"### 5. Media Spend Estimation & Reporting\")\n",
        "    st.info(\"Integrate your media spend estimation and reporting logic here.\")\n",
        "    st.info(\"You'll need widgets for users to input CPM/CPC rates by year.\")\n",
        "    # Example:\n",
        "    # media_channels = ['Paid Social Impressions', 'Email Clicks', ...] # Get from data/config\n",
        "    # estimated_rates = {}\n",
        "    # for channel in media_channels:\n",
        "    #     rate = st.number_input(f\"Estimated rate for {channel}:\", value=0.0) # Add year logic\n",
        "    #     estimated_rates[channel] = rate\n",
        "    #\n",
        "    # # Calculate estimated spend and generate report table\n",
        "    # if st.button(\"Generate Media Report\"):\n",
        "    #    media_report_df = calculate_media_report(df_engineered, estimated_rates) # Your function\n",
        "    #    st.write(\"Media Performance Report:\")\n",
        "    #    st.dataframe(media_report_df) # Use st.dataframe or st.write(..., unsafe_allow_html=True) for formatted table\n",
        "\n",
        "\n",
        "    # Return the engineered dataframe (including SIndex, dummies, splits, supers)\n",
        "    return df_engineered\n",
        "\n",
        "\n",
        "# --- Streamlit App Structure ---\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "st.title(\"Marketing Mix Modeling (MMM) App\")\n",
        "\n",
        "# --- Sidebar for Inputs ---\n",
        "st.sidebar.header(\"Upload Data\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
        "\n",
        "# Initialize session state variables\n",
        "if 'data' not in st.session_state:\n",
        "    st.session_state['data'] = None\n",
        "if 'df_engineered' not in st.session_state:\n",
        "    st.session_state['df_engineered'] = None\n",
        "if 'seasonal_period' not in st.session_state:\n",
        "    st.session_state['seasonal_period'] = 52 # Default seasonal period\n",
        "if 'model_results' not in st.session_state:\n",
        "    st.session_state['model_results'] = None\n",
        "if 'scaler' not in st.session_state:\n",
        "    st.session_state['scaler'] = None\n",
        "if 'modeling_buckets' not in st.session_state:\n",
        "     st.session_state['modeling_buckets'] = None # Store modeling buckets for later use\n",
        "\n",
        "\n",
        "# --- Main Content Area ---\n",
        "if uploaded_file is not None:\n",
        "    # Load and preprocess data from the uploaded file\n",
        "    try:\n",
        "        # Simple loading with pandas, assuming similar preprocessing as your function\n",
        "        data = pd.read_csv(uploaded_file)\n",
        "\n",
        "        # Assuming 'Week_Ending' is the date column and 'Sales' is the target\n",
        "        date_column = 'Week_Ending'\n",
        "        target_variable = 'Sales'\n",
        "\n",
        "        # Apply Indian number conversion to relevant columns if needed\n",
        "        all_columns = data.columns.tolist()\n",
        "        if date_column in all_columns:\n",
        "            all_columns.remove(date_column)\n",
        "\n",
        "        for col in all_columns:\n",
        "             if data[col].dtype == 'object':\n",
        "                 try:\n",
        "                      data[col] = data[col].apply(convert_indian_number)\n",
        "                 except Exception as e:\n",
        "                      st.warning(f\"Could not apply number conversion to column '{col}': {e}\")\n",
        "\n",
        "\n",
        "        # Convert date column and set index\n",
        "        if date_column in data.columns:\n",
        "            data[date_column] = pd.to_datetime(data[date_column], format='%d-%m-%Y %H:%M', errors='coerce', infer_datetime_format=True, cache=True)\n",
        "            data = data.dropna(subset=[date_column]) # Drop rows where date conversion failed\n",
        "            data = data.sort_values(date_column).set_index(date_column)\n",
        "            data.index.name = 'Week_Ending' # Ensure index name is set\n",
        "            st.sidebar.success(\"Data loaded and date index set successfully!\")\n",
        "            st.subheader(\"Raw Data Preview (with Date Index)\")\n",
        "            st.write(data.head())\n",
        "\n",
        "            # Store loaded data in session state\n",
        "            st.session_state['data'] = data.copy() # Store a copy\n",
        "            st.session_state['target_variable'] = target_variable # Store target variable\n",
        "\n",
        "        else:\n",
        "            st.error(f\"Date column '{date_column}' not found. Cannot proceed with analysis.\")\n",
        "            st.stop() # Stop the app if date column is missing\n",
        "\n",
        "\n",
        "        # --- EDA Section ---\n",
        "        st.sidebar.header(\"Exploratory Data Analysis (EDA)\")\n",
        "        if st.sidebar.button(\"Run EDA\"):\n",
        "            if st.session_state['data'] is not None:\n",
        "                 # Assuming perform_streamlit_eda is defined (copied/imported) above\n",
        "                 perform_streamlit_eda(st.session_state['data'].copy(), target_var=st.session_state['target_variable']) # Use data from session state\n",
        "            else:\n",
        "                 st.warning(\"Please upload data first.\")\n",
        "\n",
        "\n",
        "        # --- Feature Engineering Section ---\n",
        "        st.sidebar.header(\"Feature Engineering\")\n",
        "        # Add controls for feature engineering options\n",
        "        st.session_state['seasonal_period'] = st.sidebar.selectbox(\n",
        "            \"Seasonal Decomposition Period\",\n",
        "            [4, 13, 26, 52],\n",
        "            index=[4, 13, 26, 52].index(st.session_state['seasonal_period']), # Set default based on session state\n",
        "            key='seasonal_period_selectbox' # Add a unique key\n",
        "        )\n",
        "\n",
        "        # Placeholder for other FE controls (dummies, splits, supers) - Add Streamlit widgets for these\n",
        "        st.sidebar.info(\"Add controls for Custom Dummies, Variable Splitting, Super Campaigns here.\")\n",
        "        # Example for dummy dates:\n",
        "        # dummy_dates_input = st.sidebar.text_input(\"Custom Dummy Dates (YYYY-MM-DD, comma-separated):\")\n",
        "\n",
        "\n",
        "        if st.sidebar.button(\"Run Feature Engineering\"):\n",
        "             if st.session_state['data'] is not None:\n",
        "                 with st.spinner(\"Running Feature Engineering...\"):\n",
        "                     # Assuming perform_streamlit_feature_engineering is defined (copied/imported) above\n",
        "                     # You'll need to pass other FE options (dummies, splits, supers) to this function\n",
        "                     st.session_state['df_engineered'] = perform_streamlit_feature_engineering(\n",
        "                         st.session_state['data'].copy(), # Use data from session state\n",
        "                         st.session_state['seasonal_period'],\n",
        "                         target_var=st.session_state['target_variable']\n",
        "                         # Pass other FE parameters here\n",
        "                     )\n",
        "                     # The perform_streamlit_feature_engineering function should also return modeling_buckets\n",
        "                     # For now, we'll just assume df_engineered is returned.\n",
        "                     # You might need to adapt your function to return both.\n",
        "                     # Example adaptation: return df_engineered, modeling_buckets\n",
        "\n",
        "                 if st.session_state['df_engineered'] is not None:\n",
        "                     st.subheader(\"Engineered Features Preview:\")\n",
        "                     st.write(st.session_state['df_engineered'].head())\n",
        "                     st.success(\"Feature Engineering complete!\")\n",
        "                     # Display Media Performance Report if perform_streamlit_feature_engineering generates it\n",
        "\n",
        "             else:\n",
        "                 st.warning(\"Please upload data first.\")\n",
        "\n",
        "\n",
        "        # --- Model Training Section ---\n",
        "        st.sidebar.header(\"Model Training\")\n",
        "        # Add controls for Model Training\n",
        "        model_type = st.sidebar.selectbox(\"Select Model\", [\"Ridge\", \"Lasso\", \"ElasticNet\"], key='model_type_selectbox')\n",
        "        holdout_weeks = st.sidebar.number_input(\"Holdout Weeks\", min_value=1, value=12, key='holdout_weeks_input')\n",
        "        enforce_positive = st.sidebar.checkbox(\"Enforce Positive Media Coefficients\", value=True, key='enforce_positive_checkbox')\n",
        "\n",
        "        if st.sidebar.button(\"Train Model\"):\n",
        "           if st.session_state['df_engineered'] is not None:\n",
        "               with st.spinner(\"Training Model...\"):\n",
        "                   # Prepare model_choice dictionary\n",
        "                   model_choice = {\n",
        "                       'model': model_type.lower(),\n",
        "                       'grid': {}, # Define default grids or add widgets for hyperparameter tuning\n",
        "                       'reasons': [], # Add widgets for reasons if needed\n",
        "                       'notes': '',\n",
        "                       'timestamp': pd.Timestamp.utcnow().isoformat() + 'Z',\n",
        "                       'ready_to_train': True\n",
        "                   }\n",
        "\n",
        "                   # Assuming enhanced_mmm_analysis is defined (copied/imported) above\n",
        "                   # enhanced_mmm_analysis function expects df_features (which is df_engineered here)\n",
        "                   # and MODEL_CHOICE, HOLDOUT_WEEKS, enforce_positive\n",
        "                   # It should return results dictionary and scaler\n",
        "                   # It should also return modeling_buckets or ensure it's set in global scope if needed\n",
        "\n",
        "                   # Need to get modeling_buckets here if enhanced_mmm_analysis doesn't return it\n",
        "                   # Or adapt enhanced_mmm_analysis to take modeling_buckets as input\n",
        "\n",
        "                   # For now, let's assume modeling_buckets is generated/available after FE or passed\n",
        "                   # If your FE function returns modeling_buckets, update the FE section call.\n",
        "                   # Example: st.session_state['df_engineered'], st.session_state['modeling_buckets'] = perform_streamlit_feature_engineering(...)\n",
        "\n",
        "                   # If modeling_buckets is not returned by FE, you might need to reconstruct it\n",
        "                   # or ensure your enhanced_mmm_analysis function can handle it.\n",
        "\n",
        "                   # Temporary placeholder for modeling_buckets if not from FE:\n",
        "                   if st.session_state['modeling_buckets'] is None:\n",
        "                        st.warning(\"Modeling buckets not found in session state. Attempting to reconstruct basic buckets.\")\n",
        "                        # Basic reconstruction (may not match exact buckets from notebook FE)\n",
        "                        numeric_cols = st.session_state['df_engineered'].select_dtypes(include=[np.number]).columns.tolist()\n",
        "                        target_var = st.session_state['target_variable']\n",
        "                        if target_var in numeric_cols:\n",
        "                             numeric_cols.remove(target_var)\n",
        "\n",
        "                        # Simple heuristic for buckets (needs refinement based on your actual data/FE)\n",
        "                        base_vars = [col for col in numeric_cols if col not in ['Discount1', 'Discount2'] and 'impression' not in col.lower() and 'click' not in col.lower() and '_Spend' not in col]\n",
        "                        promo_vars = [col for col in numeric_cols if 'Discount' in col]\n",
        "                        media_vars = [col for col in numeric_cols if ('impression' in col.lower() or 'click' in col.lower()) and '_Spend' not in col]\n",
        "\n",
        "                        st.session_state['modeling_buckets'] = {\n",
        "                           'base_vars': base_vars,\n",
        "                           'promo_vars': promo_vars,\n",
        "                           'media_vars': media_vars,\n",
        "                           'target_var': target_var\n",
        "                       }\n",
        "                        st.write(\"Reconstructed Basic Modeling Buckets:\", st.session_state['modeling_buckets'])\n",
        "\n",
        "\n",
        "                   if st.session_state['modeling_buckets'] is not None:\n",
        "                       # Pass modeling_buckets to enhanced_mmm_analysis if needed\n",
        "                       results, scaler = enhanced_mmm_analysis(\n",
        "                           st.session_state['df_engineered'].copy(), # Use engineered data\n",
        "                           model_choice,\n",
        "                           holdout_weeks,\n",
        "                           enforce_positive\n",
        "                           # Pass modeling_buckets here if your function expects it\n",
        "                       )\n",
        "\n",
        "                       if results:\n",
        "                           st.session_state['model_results'] = results\n",
        "                           st.session_state['scaler'] = scaler # Store scaler\n",
        "\n",
        "                           st.subheader(\"Model Training & Analysis Results\")\n",
        "                           st.write(\"Model Performance Metrics:\", results['metrics'])\n",
        "\n",
        "                           # Display visualizations (assuming Plotly figures are returned)\n",
        "                           if results['visualizations'].get('avp_chart'):\n",
        "                               st.plotly_chart(results['visualizations']['avp_chart'])\n",
        "                           if results['visualizations'].get('contribution_timeline'):\n",
        "                               st.plotly_chart(results['visualizations']['contribution_timeline'])\n",
        "                           if results['visualizations'].get('fy_contribution_bars'):\n",
        "                               st.plotly_chart(results['visualizations']['fy_contribution_bars'])\n",
        "                           if results['visualizations'].get('fy_contribution_pies'):\n",
        "                                # Check if pie charts were generated (can be None)\n",
        "                                if results['visualizations']['fy_contribution_pies']:\n",
        "                                    st.plotly_chart(results['visualizations']['fy_contribution_pies'])\n",
        "                                else:\n",
        "                                    st.info(\"Fiscal Year Pie Charts not generated (likely insufficient data).\")\n",
        "                           if results['visualizations'].get('response_curves'):\n",
        "                                # Check if response curves were generated (can be None)\n",
        "                                if results['visualizations']['response_curves']:\n",
        "                                    st.plotly_chart(results['visualizations']['response_curves'])\n",
        "                                else:\n",
        "                                     st.info(\"Media Response Curves not generated (likely no paid media).\")\n",
        "\n",
        "\n",
        "                           # Display analysis tables\n",
        "                           if 'fy_roi' in results['analyses'] and not results['analyses']['fy_roi'].empty:\n",
        "                                st.write(\"ROI by Fiscal Year:\")\n",
        "                                # Display formatted HTML table (assuming your function returns HTML or format here)\n",
        "                                # If fy_roi is a DataFrame, display it:\n",
        "                                st.dataframe(results['analyses']['fy_roi'])\n",
        "                           else:\n",
        "                                st.info(\"No Fiscal Year ROI data to display.\")\n",
        "\n",
        "                           if 'media_effectiveness' in results['analyses'] and not results['analyses']['media_effectiveness'].empty:\n",
        "                                st.write(\"Media Effectiveness Metrics:\")\n",
        "                                st.dataframe(results['analyses']['media_effectiveness'])\n",
        "                           else:\n",
        "                                st.info(\"No Media Effectiveness data to display.\")\n",
        "\n",
        "                           if 'fy_contributions' in results['analyses'] and not results['analyses']['fy_contributions'].empty:\n",
        "                                st.write(\"Contributions by Fiscal Year:\")\n",
        "                                st.dataframe(results['analyses']['fy_contributions'])\n",
        "                           else:\n",
        "                                st.info(\"No Fiscal Year Contribution data to display.\")\n",
        "\n",
        "\n",
        "                           st.success(\"Model Training complete!\")\n",
        "\n",
        "                       else:\n",
        "                           st.error(\"Model training failed.\")\n",
        "\n",
        "           else:\n",
        "              st.warning(\"Please run Feature Engineering first.\")\n",
        "\n",
        "\n",
        "        # --- Scenario Analysis Section ---\n",
        "        st.sidebar.header(\"Scenario Analysis\")\n",
        "        # Add controls for Scenario Analysis\n",
        "        st.sidebar.info(\"Scenario analysis controls will be here. Requires a trained model.\")\n",
        "\n",
        "        # Example Scenario Controls (will need to be integrated):\n",
        "        # if st.session_state.get('model_results') is not None and st.session_state.get('df_engineered') is not None:\n",
        "        #     st.sidebar.subheader(\"Define Scenario\")\n",
        "        #     media_channels_for_scenario = st.session_state['modeling_buckets'].get('media_vars', [])\n",
        "        #     if media_channels_for_scenario:\n",
        "        #         scenario_channel = st.sidebar.selectbox(\"Select channel to change:\", [''] + media_channels_for_scenario)\n",
        "        #         if scenario_channel:\n",
        "        #             change_type = st.sidebar.radio(\"Change type:\", [\"Percentage change\", \"Fixed spend value\"])\n",
        "        #             if change_type == \"Percentage change\":\n",
        "        #                 spend_change_pct = st.sidebar.number_input(\"Percentage change (+/-):\", value=0.0) / 100.0\n",
        "        #                 fixed_spend_value = None\n",
        "        #             else:\n",
        "        #                 fixed_spend_value = st.sidebar.number_input(\"Fixed spend value:\")\n",
        "        #                 spend_change_pct = 0.0\n",
        "        #\n",
        "        #             if st.sidebar.button(\"Run Scenario\"):\n",
        "        #                 with st.spinner(f\"Running Scenario: {scenario_channel} change...\"):\n",
        "        #                     # Assuming run_scenario function is defined (copied/imported)\n",
        "        #                     # run_scenario expects base_df, scenario_name, media_channel_to_change, spend_change_pct, fixed_spend_value\n",
        "        #                     # It also needs access to the trained model, scaler, and transformation parameters (via global scope or passed)\n",
        "        #                     # You'll need to ensure these are accessible to the run_scenario function in your .py file\n",
        "        #\n",
        "        #                     # Need to ensure adstock/weibull params are available to run_scenario\n",
        "        #                     # If your enhanced_mmm_analysis returns them in results['params'], you can store them\n",
        "        #                     # st.session_state['adstock_params_optimized'] = results['params']['theta']\n",
        "        #                     # st.session_state['weibull_params_optimized'] = results['params']['weibull']\n",
        "        #\n",
        "        #                     scenario_result = run_scenario(\n",
        "        #                         st.session_state['df_engineered'].copy(), # Base data for scenario\n",
        "        #                         scenario_name=f\"{scenario_channel} Change\",\n",
        "        #                         media_channel_to_change=scenario_channel,\n",
        "        #                         spend_change_pct=spend_change_pct,\n",
        "        #                         fixed_spend_value=fixed_spend_value\n",
        "        #                         # Pass model, scaler, adstock/weibull params if run_scenario expects them\n",
        "        #                     )\n",
        "        #\n",
        "        #                     if scenario_result:\n",
        "        #                         st.subheader(f\"Scenario: {scenario_result['scenario_name']} Results\")\n",
        "        #                         st.write(f\"Total Predicted Sales: {scenario_result['predicted_sales'].sum():,.0f}\")\n",
        "        #                         st.write(f\"Total Incremental Sales (vs Base): {scenario_result['incremental_sales'].sum():,.0f}\")\n",
        "        #                         st.write(f\"Total Scenario Spend: {scenario_result['total_spend']:,.0f}\")\n",
        "        #                         st.write(f\"Total Incremental Spend (vs Base): {scenario_result['incremental_spend']:,.0f}\")\n",
        "        #                         # Format and display ROI\n",
        "        #                         roi_formatted = f\"{scenario_result['scenario_roi']:.2%}\" if not np.isinf(scenario_result['scenario_roi']) and not np.isnan(scenario_result['scenario_roi']) else str(scenario_result['scenario_roi'])\n",
        "        #                         st.write(f\"Scenario ROI: {roi_formatted}\")\n",
        "        #                     else:\n",
        "        #                         st.error(\"Scenario analysis failed.\")\n",
        "        #             else:\n",
        "        #                 st.info(\"Select a channel to define a scenario.\")\n",
        "        #     else:\n",
        "        #         st.info(\"No media channels identified for scenario analysis.\")\n",
        "        # else:\n",
        "        #    st.info(\"Train the model first to enable scenario analysis.\")\n",
        "\n",
        "\n",
        "        # --- Optimization Section ---\n",
        "        st.sidebar.header(\"Optimization\")\n",
        "        # Add controls for Optimization\n",
        "        st.sidebar.info(\"Optimization controls will be here. Requires a trained model.\")\n",
        "\n",
        "        # Example Optimization Controls (will need to be integrated):\n",
        "        # if st.session_state.get('model_results') is not None and st.session_state.get('df_engineered') is not None:\n",
        "        #      st.sidebar.subheader(\"Run Optimization\")\n",
        "        #      total_volume_budget = st.sidebar.number_input(\n",
        "        #          \"Total Volume Budget\",\n",
        "        #          value=st.session_state['df_engineered'][st.session_state['modeling_buckets'].get('media_vars', [])].sum().sum() if st.session_state.get('modeling_buckets') else 0.0\n",
        "        #      )\n",
        "        #      # Add controls for min/max percentage constraints per channel\n",
        "        #      # min_pct = st.sidebar.number_input(\"Min % of historical volume:\", value=0.0)\n",
        "        #      # max_pct = st.sidebar.number_input(\"Max % of historical volume:\", value=1.0)\n",
        "        #      st.sidebar.info(\"Add detailed channel constraints here.\")\n",
        "        #\n",
        "        #      if st.sidebar.button(\"Run Optimization\"):\n",
        "        #          with st.spinner(\"Running Optimization...\"):\n",
        "        #              # Assuming run_optimization function is defined (copied/imported)\n",
        "        #              # run_optimization expects total_budget, spend_min_pct, spend_max_pct, channel_min_spend, channel_max_spend\n",
        "        #              # It also needs access to the trained model, scaler, adstock/weibull params, original_df (via global or passed)\n",
        "        #              # Ensure original_df (st.session_state['data']) and df_engineered (st.session_state['df_engineered']) are accessible\n",
        "\n",
        "        #              # Need to pass necessary data and parameters to run_optimization\n",
        "        #              # You might need to adapt your run_optimization function's signature\n",
        "        #\n",
        "        #              optimization_results = run_optimization(\n",
        "        #                  total_budget=total_volume_budget,\n",
        "        #                  spend_min_pct=0.0, # Replace with widget values\n",
        "        #                  spend_max_pct=float('inf') # Replace with widget values\n",
        "        #                  # Pass model, scaler, adstock/weibull params if run_optimization expects them\n",
        "        #              )\n",
        "        #\n",
        "        #              if optimization_results and optimization_results['success']:\n",
        "        #                  st.subheader(\"Optimization Results\")\n",
        "        #                  st.write(\"Optimal Media Volume Allocation:\")\n",
        "        #                  st.dataframe(optimization_results['optimal_allocation_df'])\n",
        "        #                  st.write(f\"Predicted Total Sales at Optimal Allocation: {optimization_results['predicted_sales']:,.0f}\")\n",
        "        #              elif optimization_results:\n",
        "        #                  st.error(f\"Optimization failed: {optimization_results.get('message', 'Unknown error')}\")\n",
        "        #              else:\n",
        "        #                  st.error(\"Optimization did not return results.\")\n",
        "        # else:\n",
        "        #      st.info(\"Train the model first to enable optimization.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred during data loading or initial processing: {e}\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Upload a CSV file to get started with the MMM analysis.\")"
      ]
    }
  ]
}