# -*- coding: utf-8 -*-
"""Beta_MMM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14klC7g67_QAaPRaWFD-MODrI2GzTqHA8
"""

# Marketing Mix Modeling - Colab Testing Version (Fixed)
import pandas as pd
import numpy as np
# import matplotlib.pyplot as plt # Commented out as not used for main plots
# import seaborn as sns # Commented out as not used for main plots
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Add other necessary imports
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from scipy.optimize import minimize
from statsmodels.tsa.seasonal import seasonal_decompose

# Plotly imports
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots # Import make_subplots
from IPython.display import display

# Set style for plots (not needed for Plotly)
# plt.style.use('default')
# sns.set_palette("husl")


# Function to convert Indian number format to float
def convert_indian_number(value):
    """Convert Indian number format string to float"""
    if isinstance(value, str):
        # Remove commas and strip whitespace
        cleaned_value = value.replace(',', '').strip()

        # Handle special cases like ' -   ' which should be treated as NaN
        if cleaned_value in ['-', ''] or cleaned_value.isspace():
            return np.nan

        try:
            return float(cleaned_value)
        except ValueError:
            print(f"Could not convert value: '{value}'")
            return np.nan
    return value

# Load and preprocess data
def load_and_preprocess_data(file_path):
    """Load and preprocess the marketing mix data"""
    # Load the data
    data = pd.read_csv(file_path)

    # First, identify all columns that might contain Indian number format
    # These are all columns except the date column
    all_columns = data.columns.tolist()
    date_column = 'Week_Ending'

    if date_column in all_columns:
        all_columns.remove(date_column)

    # Convert all numeric columns (including Sales)
    for col in all_columns:
        # Check if the column contains string values with commas (Indian number format)
        if data[col].dtype == 'object' and data[col].str.contains(',').any():
            data[col] = data[col].apply(convert_indian_number)
        # Also convert columns that might be objects but don't have commas (like ' - ')
        elif data[col].dtype == 'object':
             data[col] = data[col].apply(convert_indian_number)


    # Handle missing values in Paid Search Impressions
    if 'Paid Search Impressions' in data.columns:
        missing_count = data['Paid Search Impressions'].isna().sum()
        if missing_count > 0:
            print(f"Found {missing_count} missing values in 'Paid Search Impressions'. Imputing with 0.")
            data['Paid Search Impressions'] = data['Paid Search Impressions'].fillna(0)

    # Convert date column
    if 'Week_Ending' in data.columns:
        data['Week_Ending'] = pd.to_datetime(data['Week_Ending'], format='%d-%m-%Y %H:%M', errors='coerce')
        data = data.sort_values('Week_Ending').reset_index(drop=True)

    return data

# Format numbers in millions (not needed for Plotly tickformat)
# def format_millions(x, pos):
#     """Format numbers in millions for plot axes"""
#     return f'{x/1e6:.1f}M'


# Perform comprehensive EDA
def perform_comprehensive_eda(data, target_var='Sales'):
    """Perform comprehensive exploratory data analysis"""
    print("="*60)
    print("COMPREHENSIVE EXPLORATORY DATA ANALYSIS")
    print("="*60)

    # 1. Basic Information
    print("\n1. BASIC DATASET INFORMATION")
    print("="*40)
    print(f"Shape: {data.shape}")
    print(f"Columns: {list(data.columns)}")
    if 'Week_Ending' in data.columns:
        print(f"Date Range: {data['Week_Ending'].min()} to {data['Week_Ending'].max()}")
    print(f"Missing Values: {data.isnull().sum().sum()}")

    # Check if target variable is numeric - should be handled in load_and_preprocess_data now
    if data[target_var].dtype == 'object':
         print(f"\nWARNING: Target variable '{target_var}' is not numeric after preprocessing.")


    # 2. Summary Statistics
    print("\n\n2. SUMMARY STATISTICS")
    print("="*40)

    # Numeric variables summary
    numeric_df = data.select_dtypes(include=[np.number])
    print("Numeric Variables Summary:")
    display(numeric_df.describe())

    # Add skewness and kurtosis
    skewness = numeric_df.skew().to_frame('Skewness')
    kurtosis = numeric_df.kurtosis().to_frame('Kurtosis')
    stats_df = pd.concat([skewness, kurtosis], axis=1)
    print("\nSkewness and Kurtosis:")
    display(stats_df)

    # 3. Univariate Analysis
    print("\n\n3. UNIVARIATE ANALYSIS")
    print("="*40)

    # Create distribution plots for all numeric variables using Plotly
    numeric_cols = numeric_df.columns.tolist()
    for col in numeric_cols:
        fig = px.histogram(data, x=col, nbins=30, title=f"Distribution of {col}")
        fig.update_layout(
            xaxis_title=col,
            yaxis_title="Frequency",
            bargap=0.1 # Add gap between bars
        )
        # Add vertical lines for mean and median
        mean_val = data[col].mean()
        median_val = data[col].median()
        fig.add_vline(x=mean_val, line_dash="dash", line_color="red", annotation_text=f"Mean: {mean_val:.2f}", annotation_position="top right")
        fig.add_vline(x=median_val, line_dash="dash", line_color="green", annotation_text=f"Median: {median_val:.2f}", annotation_position="top left")

        fig.show()


    # 4. Bivariate Analysis
    print("\n\n4. BIVARIATE ANALYSIS: RELATIONSHIP WITH TARGET VARIABLE")
    print("="*40)

    # Create scatter plots against target variable using Plotly
    if target_var in numeric_cols:
        numeric_cols_for_scatter = numeric_cols.copy()
        numeric_cols_for_scatter.remove(target_var)

    for col in numeric_cols_for_scatter:
        fig = px.scatter(data, x=col, y=target_var, title=f"{target_var} vs {col}")
        fig.update_layout(
            xaxis_title=col,
            yaxis_title=target_var
        )
        # Add correlation coefficient
        correlation = data[col].corr(data[target_var])
        fig.add_annotation(
            x=data[col].min() + (data[col].max() - data[col].min()) * 0.05,
            y=data[target_var].max() - (data[target_var].max() - data[target_var].min()) * 0.05,
            text=f"r = {correlation:.3f}",
            showarrow=False,
            bgcolor="white",
            opacity=0.8
        )
        fig.show()


    # 5. Time Series Analysis
    print("\n\n5. TIME SERIES ANALYSIS")
    print("="*40)

    if 'Week_Ending' in data.columns:
        # Plot target variable over time using Plotly
        fig = px.line(data, x='Week_Ending', y=target_var, title=f"{target_var} Over Time (in Millions)")
        fig.update_layout(
            xaxis_title="Date",
            yaxis_title="Sales (in Millions)",
            yaxis_tickformat=".1s" # Format y-axis to show values in millions
        )
        fig.show()


        # Add seasonal decomposition using Plotly
        print("Seasonal Decomposition:")
        try:
            # Ensure the data is sorted by date and set as index
            temp_df = data.set_index('Week_Ending').sort_index()
            decomposition = seasonal_decompose(temp_df[target_var], period=4, model='additive', extrapolate_trend='freq')

            fig = make_subplots(rows=4, cols=1,
                                subplot_titles=('Observed', 'Trend', 'Seasonal', 'Residuals'),
                                shared_xaxes=True)

            fig.add_trace(go.Scatter(x=temp_df.index, y=decomposition.observed, mode='lines', name='Observed'), row=1, col=1)
            fig.add_trace(go.Scatter(x=temp_df.index, y=decomposition.trend, mode='lines', name='Trend'), row=2, col=1)
            fig.add_trace(go.Scatter(x=temp_df.index, y=decomposition.seasonal, mode='lines', name='Seasonal'), row=3, col=1)
            fig.add_trace(go.Scatter(x=temp_df.index, y=decomposition.resid, mode='lines', name='Residuals'), row=4, col=1)


            fig.update_layout(height=800, title_text="Seasonal Decomposition")
            # Format y-axis to show values in millions
            fig.update_yaxes(title_text="Sales (M)", tickformat=".1s", row=1, col=1)
            fig.update_yaxes(title_text="Trend (M)", tickformat=".1s", row=2, col=1)
            fig.update_yaxes(title_text="Seasonal (M)", tickformat=".1s", row=3, col=1)
            fig.update_yaxes(title_text="Residual (M)", tickformat=".1s", row=4, col=1)
            fig.show()
        except Exception as e:
            print(f"Could not perform seasonal decomposition: {str(e)}")

    # 6. Correlation Analysis
    print("\n\n6. CORRELATION ANALYSIS")
    print("="*40)

    # Full correlation matrix using Plotly
    print("Full Correlation Matrix:")
    corr = numeric_df.corr()
    # Corrected heatmap using go.Heatmap
    fig = go.Figure(data=go.Heatmap(
                        z=corr.values,
                        x=corr.columns,
                        y=corr.index,
                        colorscale='RdBu', # Changed colorscale to a valid one
                        zmid=0, # Center the color scale at 0
                        colorbar=dict(title="Correlation"),
                        text=corr.values.round(2), # Add text labels
                        texttemplate="%{text}" # Format text labels
                      ))

    fig.update_layout(title="Correlation Matrix - All Variables")
    fig.show()

    # Media variables correlation
    media_keywords = ['impressions', 'clicks', 'social', 'search', 'email', 'video']
    media_cols = [col for col in numeric_df.columns if any(keyword in col.lower() for keyword in media_keywords)]

    if media_cols and target_var in numeric_df.columns:
        print("Media Variables Correlation with Target:")
        media_corr = numeric_df[media_cols + [target_var]].corr()
        # Extract only correlations with target variable
        target_corr = media_corr[target_var].drop(target_var).sort_values(ascending=False)

        # Plot correlations as vertical bars using Plotly
        fig = px.bar(x=target_corr.index, y=target_corr.values * 100, title=f"Correlation of Media Variables with {target_var} (%)")
        fig.update_layout(
            xaxis_title="Media Variables",
            yaxis_title="Correlation Coefficient (%)",
            yaxis_ticksuffix="%", # Add percentage suffix
            xaxis_tickangle=-45
        )
        fig.show()


        # Also show as a table
        print((target_corr * 100).round(2).to_frame("Correlation (%)"))

    # 7. Outlier Analysis
    print("\n\n7. OUTLIER ANALYSIS")
    print("="*40)

    # Check for outliers in target variable
    Q1 = data[target_var].quantile(0.25)
    Q3 = data[target_var].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identify outliers
    outliers = data[(data[target_var] < lower_bound) | (data[target_var] > upper_bound)]
    normal_data = data[~((data[target_var] < lower_bound) | (data[target_var] > upper_bound))]
    print(f"Number of potential outliers in {target_var}: {len(outliers)}")

    if len(outliers) > 0:
        print("Outlier values:")
        display(outliers[['Week_Ending', target_var]])

        # Plot with outliers highlighted using Plotly
        fig = go.Figure()

        # Add normal values
        fig.add_trace(go.Scattergl(
            x=normal_data['Week_Ending'],
            y=normal_data[target_var],
            mode='markers',
            name='Normal Values',
            marker=dict(color='darkgreen', opacity=0.7, size=8)
        ))

        # Add outliers
        fig.add_trace(go.Scattergl(
            x=outliers['Week_Ending'],
            y=outliers[target_var],
            mode='markers',
            name='Outliers',
            marker=dict(color='red', opacity=0.9, size=10)
        ))

        # Add bounds lines
        fig.add_shape(type="line",
            x0=data['Week_Ending'].min(), y0=upper_bound, x1=data['Week_Ending'].max(), y1=upper_bound,
            line=dict(color="red", width=2, dash="dash"),
            name='Upper Bound'
        )
        fig.add_shape(type="line",
            x0=data['Week_Ending'].min(), y0=lower_bound, x1=data['Week_Ending'].max(), y1=lower_bound,
            line=dict(color="red", width=2, dash="dash"),
            name='Lower Bound'
        )


        fig.update_layout(
            title=f"Outlier Detection in {target_var}",
            xaxis_title="Date",
            yaxis_title=target_var,
            yaxis_tickformat=".1s" # Format y-axis to show values in millions
        )
        fig.show()


    # Return numeric_df and correlation matrix for interactive use
    return numeric_df, corr


# Main execution
if __name__ == "__main__":
    # Load and preprocess your data
    file_path = "/content/Book1.csv"  # Update this path to your file location
    data = load_and_preprocess_data(file_path)

    # Define the target variable here
    target_var = 'Sales'

    # Perform comprehensive EDA and get the numeric dataframe and correlation matrix
    numeric_df, correlations = perform_comprehensive_eda(data, target_var=target_var)

import pandas as pd
import numpy as np
from statsmodels.tsa.seasonal import seasonal_decompose
from IPython.display import display

def feature_engineering_module(df):
    """
    Feature engineering module for time series data
    """
    print("\n===== FEATURE ENGG MODULE =====")

    # Check for required columns
    if 'Week_Ending' not in df.columns:
        raise ValueError("DataFrame must contain a 'Week_Ending' column")
    if 'Sales' not in df.columns:
        raise ValueError("DataFrame must contain a 'Sales' column")

    # Ensure we're working with a copy to avoid modifying original data
    df = df.copy()

    # Track original columns to identify new features later
    original_columns = set(df.columns)

    # Ensure datetime index
    df['Week_Ending'] = pd.to_datetime(df['Week_Ending'])
    df = df.set_index('Week_Ending').sort_index()

    # 1. Seasonal Index (SIndex) - Always created
    try:
        period = 52  # Default period for weekly data
        if len(df) >= 2 * period:
            decomp = seasonal_decompose(df['Sales'], period=period, model='additive', extrapolate_trend='freq')
            df['SIndex'] = decomp.seasonal
            print(f"‚úÖ Seasonal Index created with period={period}")
        else:
            print(f"‚ö†Ô∏è Not enough data points ({len(df)}) for seasonality period {period}")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not create Seasonal Index: {str(e)}")

    # 2. Holiday Dummies
    try:
        add_dummies = input("\nDo you want to add holiday dummies? (Y/N): ").strip().lower()
        if add_dummies == 'y':
            dates = input("Enter holiday dates (comma separated, format YYYY-MM-DD): ")
            if dates.strip():
                dates = [d.strip() for d in dates.split(",")]
                for i, d in enumerate(dates, start=1):
                    try:
                        holiday_date = pd.to_datetime(d)
                        col_name = f"Holiday_{i}"
                        df[col_name] = (df.index == holiday_date).astype(int)
                        print(f"‚úÖ Added dummy: {col_name} for {d}")
                    except:
                        print(f"‚ö†Ô∏è Could not parse date: {d}")
    except:
        print("‚ö†Ô∏è Skipping holiday dummies due to input issue")

    # 3. Split Variable
    try:
        split_choice = input("\nDo you want to split a variable at a date? (Y/N): ").strip().lower()
        if split_choice == 'y':
            print("\nAvailable variables:")
            all_vars = [col for col in df.columns if col not in ['SIndex']]
            for i, var in enumerate(all_vars, 1):
                print(f"{i}. {var}")

            try:
                var_idx = int(input("Select variable number to split: ")) - 1
                if 0 <= var_idx < len(all_vars):
                    var_name = all_vars[var_idx]
                    split_date = input("Enter split date (YYYY-MM-DD): ").strip()

                    try:
                        split_dt = pd.to_datetime(split_date)
                        df[f"{var_name}_pre"] = np.where(df.index <= split_dt, df[var_name], 0)
                        df[f"{var_name}_post"] = np.where(df.index > split_dt, df[var_name], 0)

                        # Drop the original variable as requested
                        df.drop(columns=[var_name], inplace=True)
                        print(f"‚úÖ Split {var_name} into {var_name}_pre and {var_name}_post at {split_dt.date()}")
                        print(f"‚úÖ Dropped original variable: {var_name}")
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error splitting variable: {str(e)}")
                else:
                    print("‚ö†Ô∏è Invalid selection")
            except:
                print("‚ö†Ô∏è Invalid input")
    except:
        print("‚ö†Ô∏è Skipping variable splitting due to input issue")

    # 4. Super Campaign - Enhanced with variable selection by number
    try:
        super_choice = input("\nDo you want to create a super campaign? (Y/N): ").strip().lower()
        if super_choice == 'y':
            print("\nAvailable variables:")
            all_vars = [col for col in df.columns if col not in ['SIndex', 'Sales'] and not col.endswith(('_pre', '_post'))]
            for i, var in enumerate(all_vars, 1):
                print(f"{i}. {var}")

            try:
                selected = input("Enter variable numbers to combine (comma separated): ")
                var_indices = [int(x.strip())-1 for x in selected.split(",") if x.strip().isdigit()]
                vars_to_combine = [all_vars[i] for i in var_indices if 0 <= i < len(all_vars)]

                if vars_to_combine:
                    # Ask for custom name
                    name_choice = input("Do you want to provide a custom name? (Y/N): ").strip().lower()
                    if name_choice == 'y':
                        super_col = input("Enter the custom name: ").strip()
                    else:
                        super_col = "combined_var"

                    # Create the super campaign
                    df[super_col] = df[vars_to_combine].sum(axis=1)

                    # Display verification of sums
                    print(f"\nVerification of sums for {super_col}:")
                    for var in vars_to_combine:
                        print(f"{var}: {df[var].sum():.2f}")
                    print(f"{super_col}: {df[super_col].sum():.2f}")

                    # Drop the original variables as requested
                    df.drop(columns=vars_to_combine, inplace=True)
                    print(f"‚úÖ Created Super Campaign: {super_col} combining {vars_to_combine}")
                    print(f"‚úÖ Dropped original variables: {vars_to_combine}")
                else:
                    print("‚ö†Ô∏è No valid variables selected")
            except:
                print("‚ö†Ô∏è Invalid input")
    except:
        print("‚ö†Ô∏è Skipping super campaign creation due to input issue")

    # Show extra features created
    new_features = sorted(set(df.columns) - original_columns)

    if new_features:
        print("\nüìä EXTRA FEATURES CREATED:")
        print("=" * 50)

        # Create a summary table
        feature_info = []
        for col in new_features:
            if col == "SIndex":
                feature_type = "Seasonal Index"
            elif col.startswith("Holiday_"):
                feature_type = "Holiday Dummy"
            elif col.endswith(("_pre", "_post")):
                feature_type = "Split Variable"
            elif col == "combined_var" or col == "Super_Campaign":
                feature_type = "Super Campaign"
            else:
                feature_type = "Other"

            feature_info.append({
                "Feature Name": col,
                "Type": feature_type,
                "Data Type": str(df[col].dtype),
                "Non-Zero Values": f"{(df[col] != 0).sum()} / {len(df)}"
            })

        # Display as table
        display(pd.DataFrame(feature_info))

        # Show first few rows of new features
        print("\nüìã SAMPLE OF NEW FEATURES:")
        display(df[new_features].head())
    else:
        print("\n‚ÑπÔ∏è No additional features were created.")

    return df

def interactive_data_exploration(df_features):
    """
    Interactive data exploration function
    """
    print("\n\n===== INTERACTIVE DATA EXPLORATION =====")
    print("-" * 50)
    print("Ask questions about any variable in the dataset")
    print("Type 'N', 'NO', or 'no' to finish, 'variables' to see all available variables, or 'help' for examples")

    # Get all available variables for reference
    all_variables = list(df_features.columns)
    numeric_variables = list(df_features.select_dtypes(include=[np.number]).columns)

    # Recompute correlations with new features
    numeric_df_features = df_features.select_dtypes(include=[np.number])
    correlations = numeric_df_features.corr()

    # Simple question-answering mechanism
    while True:
        try:
            question = input("\nWhat would you like to know about your data? ").strip().lower()

            # Check for exit condition
            if question in ['n', 'no']:
                print("Exiting interactive mode...")
                break

        except:
            print("\nInput error. Exiting interactive mode.")
            break

        if question == 'variables':
            print("Available variables:")
            for var in all_variables:
                print(f"‚Ä¢ {var}")
            continue
        elif question == 'help':
            print("Examples of questions I can answer:")
            print("‚Ä¢ 'paid search' - Get comprehensive analysis of paid search metrics")
            print("‚Ä¢ 'discount1' - Analyze discount1 performance and relationship with sales")
            print("‚Ä¢ 'social media' - Explore all social media related metrics")
            print("‚Ä¢ 'trends' - View sales trends over time")
            print("‚Ä¢ 'correlations' - See strongest correlations with sales")
            print("‚Ä¢ 'seasonality' - Analyze seasonal patterns in sales")
            print("‚Ä¢ 'holidays' - Analyze holiday impacts on sales")
            print("‚Ä¢ 'split variables' - Show information about split variables")
            print("‚Ä¢ 'super campaign' - Show information about super campaign")
            print("‚Ä¢ 'sales correlation with paid search' - Get correlation between sales and paid search")
            continue

        # Check for special queries about feature engineering elements
        if 'seasonal' in question or 'sindex' in question:
            if 'SIndex' in df_features.columns:
                print("\nSEASONALITY ANALYSIS:")
                print("=" * 50)

                # Calculate seasonal strength
                seasonal_strength = df_features['SIndex'].std() / df_features['Sales'].std() * 100
                print(f"Seasonal strength: {seasonal_strength:.1f}% of sales variation")

                # Find strongest seasonal periods
                seasonal_peaks = df_features['SIndex'].nlargest(5)
                seasonal_troughs = df_features['SIndex'].nsmallest(5)

                print("\nStrongest seasonal peaks:")
                for date, value in seasonal_peaks.items():
                    print(f"‚Ä¢ {date.strftime('%Y-%m-%d')}: {value:,.0f}")

                print("\nStrongest seasonal troughs:")
                for date, value in seasonal_troughs.items():
                    print(f"‚Ä¢ {date.strftime('%Y-%m-%d')}: {value:,.0f}")
            else:
                print("No seasonal index found. Run feature engineering to create SIndex.")
            continue

        if 'holiday' in question:
            holiday_cols = [col for col in df_features.columns if col.startswith('Holiday_')]
            if holiday_cols:
                print("\nHOLIDAY ANALYSIS:")
                print("=" * 50)

                for col in holiday_cols:
                    holiday_dates = df_features[df_features[col] == 1].index
                    if not holiday_dates.empty:
                        print(f"\n{col}:")
                        for date in holiday_dates:
                            # Get sales on holiday vs average
                            holiday_sales = df_features.loc[date, 'Sales']
                            avg_sales = df_features['Sales'].mean()
                            pct_diff = (holiday_sales - avg_sales) / avg_sales * 100
                            print(f"‚Ä¢ {date.strftime('%Y-%m-%d')}: Sales ‚Çπ{holiday_sales:,.0f} ({pct_diff:+.1f}% vs average)")
            else:
                print("No holiday dummies found. You can add them in the feature engineering module.")
            continue

        if 'split' in question:
            split_cols = [col for col in df_features.columns if col.endswith(('_pre', '_post'))]
            if split_cols:
                print("\nSPLIT VARIABLES ANALYSIS:")
                print("=" * 50)

                # Group by root variable
                split_roots = {}
                for col in split_cols:
                    root = col.rsplit('_', 1)[0]
                    if root not in split_roots:
                        split_roots[root] = []
                    split_roots[root].append(col)

                for root, cols in split_roots.items():
                    print(f"\n{root}:")
                    for col in cols:
                        period = "before" if col.endswith('_pre') else "after"
                        avg_val = df_features[col].mean()
                        print(f"‚Ä¢ {col}: Average value {period} split: {avg_val:,.2f}")
            else:
                print("No split variables found. You can create them in the feature engineering module.")
            continue

        if 'super campaign' in question or 'combined' in question:
            super_cols = [col for col in df_features.columns if col in ['Super_Campaign', 'combined_var'] or 'super' in col.lower()]
            if super_cols:
                for super_col in super_cols:
                    print("\nSUPER CAMPAIGN ANALYSIS:")
                    print("=" * 50)

                    # Calculate correlation with sales
                    if 'Sales' in df_features.columns:
                        corr = df_features[super_col].corr(df_features['Sales'])
                        print(f"Correlation with sales: {corr:.3f}")

                    # Summary stats
                    avg_campaign = df_features[super_col].mean()
                    max_campaign = df_features[super_col].max()
                    min_campaign = df_features[super_col].min()

                    print(f"Average {super_col} value: {avg_campaign:,.2f}")
                    print(f"Maximum {super_col} value: {max_campaign:,.2f}")
                    print(f"Minimum {super_col} value: {min_campaign:,.2f}")

                    # Show contribution of original variables if available
                    print(f"Total {super_col} spend: {df_features[super_col].sum():,.2f}")
            else:
                print("No super campaign found. You can create one in the feature engineering module.")
            continue

        # Check for correlation questions
        if 'correlation' in question or 'relationship' in question:
            # Extract variable names from question
            mentioned_vars = [var for var in all_variables if var.lower() in question]

            if 'sales' in question and len(mentioned_vars) > 1:
                # Show correlation between sales and mentioned variables
                for var in mentioned_vars:
                    if var != 'Sales' and var in numeric_variables:
                        corr = df_features['Sales'].corr(df_features[var])
                        direction = "positive" if corr > 0 else "negative"
                        strength = "strong" if abs(corr) > 0.7 else "moderate" if abs(corr) > 0.3 else "weak"
                        print(f"Correlation between Sales and {var}: {corr:.3f} ({strength} {direction} relationship)")
            else:
                # Show top correlations with sales
                if 'Sales' in correlations.index:
                    strong_corrs = correlations['Sales'].drop('Sales').abs().sort_values(ascending=False)
                    # Filter for strong correlations (e.g., abs value > 0.4) and get original correlation values
                    strong_corrs = correlations['Sales'].loc[strong_corrs[strong_corrs > 0.4].index]

                    if not strong_corrs.empty:
                        print("Strongest relationships with sales:")
                        for var, corr in strong_corrs.items():
                            direction = "positive" if corr > 0 else "negative"
                            print(f"‚Ä¢ {var}: {corr:.3f} ({direction})")
                    else:
                        print("No strong correlations found (|r| > 0.4) with Sales.")
                else:
                    print("Target variable 'Sales' not found in correlation matrix.")
            continue

        # Check for other special queries
        if 'trend' in question:
            # Show sales trends
            trend_data = df_features[['Sales']].copy()
            overall_trend = "increasing" if trend_data['Sales'].iloc[-1] > trend_data['Sales'].iloc[0] else "decreasing"

            # Handle potential division by zero if the first value is 0
            if trend_data['Sales'].iloc[0] != 0:
                change_pct = (trend_data['Sales'].iloc[-1] - trend_data['Sales'].iloc[0]) / trend_data['Sales'].iloc[0] * 100
                print(f"Sales show an {overall_trend} trend overall ({change_pct:+.1f}% change)")
            else:
                print(f"Sales show an {overall_trend} trend overall (starting from zero)")

            # Show monthly trends if available
            if len(trend_data) >= 12:
                monthly_avg = trend_data.resample('M').mean()
                if not monthly_avg.empty:
                    best_month = monthly_avg.idxmax()[0].strftime('%B')
                    worst_month = monthly_avg.idxmin()[0].strftime('%B')
                    print(f"Best month: {best_month}, Worst month: {worst_month}")
                else:
                    print("Not enough data for monthly trend analysis.")
            continue

        # Find matching variables
        matching_vars = [var for var in all_variables if question in var.lower()]

        if not matching_vars:
            # Try to understand what the user might be asking about
            if any(word in question for word in ['sale', 'revenue', 'income']):
                print(f"Analysis for Sales:")
                print(f"Range: ‚Çπ{df_features['Sales'].min():,.0f} to ‚Çπ{df_features['Sales'].max():,.0f}")
                print(f"Average: ‚Çπ{df_features['Sales'].mean():,.0f}")
                print(f"Standard Deviation: ‚Çπ{df_features['Sales'].std():,.0f}")

                # Calculate growth rate
                first_value = df_features['Sales'].iloc[0]
                last_value = df_features['Sales'].iloc[-1]
                # Handle potential division by zero if the first value is 0
                if first_value != 0:
                    growth_pct = (last_value - first_value) / first_value * 100
                    print(f"Growth over period: {growth_pct:+.1f}%")
                else:
                    print(f"Growth over period: Infinite (starting from zero)")
            else:
                # Try fuzzy matching for similar variable names
                import difflib
                suggestions = difflib.get_close_matches(question, all_variables, n=3, cutoff=0.3)
                if suggestions:
                    print(f"Variable '{question}' not found. Did you mean: {', '.join(suggestions)}?")
                else:
                    print(f"Variable '{question}' not found. Type 'variables' to see all available variables.")
            continue

        # Process each matching variable
        for var in matching_vars:
            print(f"\n{'='*50}")
            print(f"COMPREHENSIVE ANALYSIS FOR: {var}")
            print(f"{'='*50}")

            # Basic variable information
            if var in numeric_variables:
                print(f"Type: Numeric")
                print(f"Range: {df_features[var].min():,.2f} to {df_features[var].max():,.2f}")
                print(f"Mean: {df_features[var].mean():,.2f}")
                print(f"Standard Deviation: {df_features[var].std():,.2f}")
                print(f"Missing Values: {df_features[var].isnull().sum()} ({df_features[var].isnull().sum()/len(df_features)*100:.1f}%)")

                # Distribution shape
                skewness = df_features[var].skew()
                if abs(skewness) > 1:
                    shape = "highly skewed"
                elif abs(skewness) > 0.5:
                    shape = "moderately skewed"
                else:
                    shape = "approximately symmetric"
                print(f"Distribution: {shape} (skewness: {skewness:.2f})")
            else:
                print(f"Type: Categorical")
                print(f"Unique values: {df_features[var].nunique()}")
                if df_features[var].nunique() <= 10:
                    print("Value counts:")
                    print(df_features[var].value_counts())

            # Relationship with target variable
            if var in numeric_variables and var != 'Sales' and 'Sales' in df_features.columns:
                corr = df_features[var].corr(df_features['Sales'])
                print(f"\nRELATIONSHIP WITH Sales:")
                print(f"Correlation coefficient: {corr:.3f}")

                if abs(corr) > 0.7:
                    strength = "very strong"
                elif abs(corr) > 0.5:
                    strength = "strong"
                elif abs(corr) > 0.3:
                    strength = "moderate"
                elif abs(corr) > 0.1:
                    strength = "weak"
                else:
                    strength = "very weak or no"

                direction = "positive" if corr > 0 else "negative"
                print(f"‚Üí {strength} {direction} relationship with sales")

                # Estimate impact
                if df_features[var].std() > 0:
                    # Calculate the estimated change in Sales for a one standard deviation change in the current variable
                    estimated_sales_change_per_std = corr * df_features['Sales'].std()
                    print(f"‚Üí A one standard deviation increase in {var} is associated with an estimated ‚Çπ{estimated_sales_change_per_std:,.0f} change in sales.")

            # Time trend analysis
            # Calculate trend
            trend_data = df_features[[var]].copy()
            if len(trend_data) > 1:
                overall_trend = "increasing" if trend_data[var].iloc[-1] > trend_data[var].iloc[0] else "decreasing"
                # Handle potential division by zero if the first value is 0
                change_pct = (trend_data[var].iloc[-1] - trend_data[var].iloc[0]) / trend_data[var].iloc[0] * 100 if trend_data[var].iloc[0] != 0 else float('inf') if trend_data[var].iloc[-1] > 0 else 0

                print(f"\nTIME TREND ANALYSIS:")
                if change_pct == float('inf'):
                    print(f"‚Üí Overall {overall_trend} trend (infinite percentage change from zero start)")
                else:
                    print(f"‚Üí Overall {overall_trend} trend ({change_pct:+.1f}% change)")

                # Check for seasonality
                if len(trend_data) >= 12:
                    monthly_avg = trend_data.resample('M').mean()
                    if len(monthly_avg) > 3 and monthly_avg[var].mean() != 0:
                        seasonal_variation = (monthly_avg[var].max() - monthly_avg[var].min()) / monthly_avg[var].mean() * 100
                        print(f"‚Üí Seasonal variation: {seasonal_variation:.1f}%")
                    elif monthly_avg[var].mean() == 0:
                        print(f"‚Üí Seasonal variation: Infinite (average is zero)")
                    else:
                        print("‚Üí Not enough data points for meaningful seasonal variation analysis.")

            # Compare with other variables in the same category
            if any(kw in var.lower() for kw in ['discount', 'promo']):
                discount_vars = [v for v in numeric_variables if any(kw in v.lower() for kw in ['discount', 'promo'])]
                if len(discount_vars) > 1:
                    print(f"\nCOMPARISON WITH OTHER DISCOUNT VARIABLES:")
                    for d_var in discount_vars:
                        if d_var != var:
                            d_corr = df_features[d_var].corr(df_features['Sales'])
                            print(f"‚Üí {d_var}: r = {d_corr:.3f}")

            if any(kw in var.lower() for kw in ['search', 'impression', 'click', 'social', 'email', 'video']):
                media_vars = [v for v in numeric_variables if any(kw in v.lower() for kw in ['search', 'impression', 'click', 'social', 'email', 'video'])]
                if len(media_vars) > 1:
                    print(f"\nCOMPARISON WITH OTHER MEDIA VARIABLES:")
                    for m_var in media_vars:
                        if m_var != var:
                            m_corr = df_features[m_var].corr(df_features['Sales'])
                            print(f"‚Üí {m_var}: r = {m_corr:.3f}")

            print(f"\nRECOMMENDATION FOR {var}:")
            if var in numeric_variables and var != 'Sales' and 'Sales' in df_features.columns:
                corr = df_features[var].corr(df_features['Sales'])
                if corr > 0.4:
                    print("‚Üí Consider increasing investment in this area as it strongly correlates with sales")
                elif corr > 0.2:
                    print("‚Üí This area shows positive correlation with sales, consider testing increased investment")
                elif corr < -0.2:
                    print("‚Üí This area shows negative correlation with sales, consider investigating further")
                else:
                    print("‚Üí No strong relationship with sales detected")

    # Show finalized list of variables by bucket after interactive session
    print("\n\n===== FINALIZED VARIABLES BY BUCKET =====")
    print("-" * 50)

    # Categorize variables
    base_vars = [col for col in df_features.columns if not col.startswith(('Holiday_', 'SIndex')) and not col.endswith(('_pre', '_post')) and col != 'Sales' and col not in ['combined_var', 'Super_Campaign']]
    holiday_vars = [col for col in df_features.columns if col.startswith('Holiday_')]
    split_vars = [col for col in df_features.columns if col.endswith(('_pre', '_post'))]
    seasonal_vars = [col for col in df_features.columns if col == 'SIndex']
    super_vars = [col for col in df_features.columns if col in ['combined_var', 'Super_Campaign']]

    print("TARGET VARIABLE:")
    print(f"‚Ä¢ Sales")

    print("\nINDEPENDENT VARIABLES:")
    print("\nBase Variables:")
    for var in base_vars:
        print(f"‚Ä¢ {var}")

    print("\nHoliday Dummies:")
    for var in holiday_vars:
        print(f"‚Ä¢ {var}")

    print("\nSplit Variables:")
    for var in split_vars:
        print(f"‚Ä¢ {var}")

    print("\nSeasonal Index:")
    for var in seasonal_vars:
        print(f"‚Ä¢ {var}")

    print("\nSuper Campaign:")
    for var in super_vars:
        print(f"‚Ä¢ {var}")

# Run feature engineering
df_features = feature_engineering_module(data)

# Run interactive data exploration
interactive_data_exploration(df_features)

# ================================================
# MODELING ENGINE - VARIABLE BUCKETING
# ================================================
def variable_bucketing(df):
    """
    Categorizes variables into different buckets for modeling

    Parameters:
    df (DataFrame): Input dataframe with all variables

    Returns:
    dict: Dictionary with variables categorized into buckets
    """
    print("\n===== VARIABLE BUCKETING =====")

    # Get all column names
    all_columns = df.columns.tolist()

    # Define buckets
    buckets = {
        'Media Variables': [],
        'Base Variables': [],
        'Promo Variables': [],
        'Extra Features': [],
        'Other Variables': []
    }

    # Media variables (keywords to identify media variables)
    media_keywords = ['impression', 'click', 'social', 'search', 'email', 'video', 'media', 'campaign', 'ad', 'spend']
    for col in all_columns:
        if any(keyword in col.lower() for keyword in media_keywords):
            buckets['Media Variables'].append(col)

    # Promo variables (discount-related)
    promo_keywords = ['discount', 'promo', 'promotion', 'offer']
    for col in all_columns:
        if any(keyword in col.lower() for keyword in promo_keywords):
            buckets['Promo Variables'].append(col)

    # Extra features (created during feature engineering)
    extra_keywords = ['sindex', 'holiday', '_pre', '_post', 'super_campaign']
    for col in all_columns:
        if any(keyword in col.lower() for keyword in extra_keywords):
            buckets['Extra Features'].append(col)

    # Base variables (everything else excluding Sales and date index)
    base_exclusions = ['sales', 'week_ending'] + buckets['Media Variables'] + buckets['Promo Variables'] + buckets['Extra Features']
    for col in all_columns:
        if col.lower() not in [excl.lower() for excl in base_exclusions]:
            buckets['Base Variables'].append(col)

    # Remove duplicates (if any variable was categorized in multiple buckets)
    for bucket_name in buckets:
        buckets[bucket_name] = list(set(buckets[bucket_name]))

    # Display the bucketing results
    print("üìä VARIABLE BUCKETS:")
    print("=" * 50)

    for bucket_name, variables in buckets.items():
        print(f"\n{bucket_name.upper()} ({len(variables)} variables):")
        if variables:
            for var in sorted(variables):
                print(f"  - {var}")
        else:
            print("  (None)")

    return buckets

# Apply variable bucketing to your data
variable_buckets = variable_bucketing(df_features)

# ================================================
# FINAL VARIABLE SELECTION FOR MODELING
# ================================================
def create_final_variable_list(variable_buckets):
    """
    Creates final list of variables for modeling by handling splits and super campaigns

    Parameters:
    variable_buckets (dict): Dictionary with variables categorized into buckets

    Returns:
    list: Final list of variables to use in modeling
    """
    print("\n===== FINAL VARIABLE SELECTION =====")

    # Create a copy of the buckets to work with
    final_vars = {}
    for bucket, vars_list in variable_buckets.items():
        final_vars[bucket] = vars_list.copy()

    # Identify split variables and their roots
    split_vars = []
    split_roots = set()

    for var in final_vars['Extra Features']:
        if var.endswith('_pre') or var.endswith('_post'):
            split_vars.append(var)
            # Extract the root variable name (remove _pre or _post)
            root_var = var.rsplit('_', 1)[0]
            split_roots.add(root_var)

    # Remove original variables that were split
    for bucket in ['Media Variables', 'Base Variables', 'Promo Variables']:
        for root_var in split_roots:
            if root_var in final_vars[bucket]:
                print(f"üóëÔ∏è  Removing original variable '{root_var}' (replaced with split versions)")
                final_vars[bucket].remove(root_var)

    # Check if super campaign was created
    super_campaign_vars = []
    for var in final_vars['Extra Features']:
        if 'Super_Campaign' in var:
            super_campaign_vars.append(var)

    # If super campaign was created, remove the individual media variables that were combined
    if super_campaign_vars:
        print("üîç Super campaign detected, identifying components...")

        # For each super campaign, we need to know which variables were combined
        # This would typically be stored during feature engineering, but we'll infer from names
        for super_var in super_campaign_vars:
            # Extract the variables that were combined (this is a simplification)
            # In a real scenario, you'd have stored this information during feature engineering
            if 'Super_Campaign' in super_var:
                # This is a simple approach - in practice you'd need to know exactly which variables were combined
                print(f"‚ö†Ô∏è  Need to manually specify which variables were combined into '{super_var}'")
                print("Available media variables:", final_vars['Media Variables'])

                # Ask user to specify which variables were combined
                combined_vars_input = input(
                    f"Enter the media variables that were combined into '{super_var}' (comma-separated): "
                )

                if combined_vars_input.strip():
                    combined_vars = [v.strip() for v in combined_vars_input.split(',')]
                    for var_to_remove in combined_vars:
                        if var_to_remove in final_vars['Media Variables']:
                            print(f"üóëÔ∏è  Removing '{var_to_remove}' (included in super campaign)")
                            final_vars['Media Variables'].remove(var_to_remove)

    # Create the final list of all variables
    all_final_vars = []
    for bucket_vars in final_vars.values():
        all_final_vars.extend(bucket_vars)

    # Remove duplicates
    all_final_vars = list(set(all_final_vars))

    # Display the final variable list
    print("\nüìã FINAL VARIABLES FOR MODELING:")
    print("=" * 50)

    for var in sorted(all_final_vars):
        # Categorize each variable
        if var in final_vars['Media Variables']:
            category = "Media"
        elif var in final_vars['Base Variables']:
            category = "Base"
        elif var in final_vars['Promo Variables']:
            category = "Promo"
        elif var in final_vars['Extra Features']:
            category = "Extra"
        else:
            category = "Other"

        print(f"{var} ({category})")

    print(f"\nTotal variables: {len(all_final_vars)}")

    return all_final_vars

# Create the final variable list
final_variables = create_final_variable_list(variable_buckets)

def preview_final_variables(df, final_variables, target_var='Sales'):
    """
    Display a preview of the final variables and target variable
    """
    print("\n===== FINAL VARIABLES PREVIEW =====")

    # Ensure target variable is included in the preview
    preview_columns = final_variables.copy()
    if target_var not in preview_columns and target_var in df.columns:
        preview_columns.append(target_var)

    # Display the first 10 rows
    print(f"Preview of first 10 rows ({len(preview_columns)} variables):")
    print("=" * 80)

    # Create a preview dataframe
    preview_df = df[preview_columns].head(10).copy()

    # Format numbers to avoid scientific notation
    pd.set_option('display.float_format', '{:.2f}'.format)

    # Display the preview
    display(preview_df)

    # Reset the display option
    pd.reset_option('display.float_format')

    # Show summary statistics
    print("\nSummary Statistics:")
    print("=" * 80)

    # Get summary stats for numeric columns only
    numeric_cols = df[preview_columns].select_dtypes(include=[np.number]).columns.tolist()
    if numeric_cols:
        stats_df = df[numeric_cols].describe().T
        stats_df = stats_df[['count', 'mean', 'std', 'min', 'max']]
        display(stats_df)
    else:
        print("No numeric variables to display statistics for.")

# Preview the final variables
preview_final_variables(df_features, final_variables)

# ================================================
# MODEL PREPROCESSING MODULE
# ================================================
from sklearn.preprocessing import StandardScaler

def model_preprocessing(df, feature_columns, target_column='Sales', test_size=0.2):
    """
    Preprocess data for modeling: scaling and time-based train-test split

    Parameters:
    df (DataFrame): Input dataframe with all variables
    feature_columns (list): List of independent variables
    target_column (str): Name of the target variable (default: 'Sales')
    test_size (float): Proportion of data to use for testing (default: 0.2)

    Returns:
    tuple: X_train_scaled, X_test_scaled, y_train, y_test, scaler
    """
    print("\n===== MODEL PREPROCESSING =====")

    # Separate features and target
    X = df[feature_columns].copy()
    y = df[target_column].copy()

    print(f"Original feature shapes: X={X.shape}, y={y.shape}")

    # Time-based train-test split (not random for time series)
    split_idx = int(len(X) * (1 - test_size))

    X_train = X.iloc[:split_idx]
    X_test = X.iloc[split_idx:]
    y_train = y.iloc[:split_idx]
    y_test = y.iloc[split_idx:]

    print(f"After time-based split:")
    print(f"X_train: {X_train.shape}, X_test: {X_test.shape}")
    print(f"y_train: {y_train.shape}, y_test: {y_test.shape}")

    # Scale the features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Convert back to DataFrames with original column names
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns, index=X_train.index)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns, index=X_test.index)

    print("‚úÖ Features scaled using StandardScaler")
    print("‚úÖ Time-based train-test split completed")

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler

# Apply preprocessing to your data
X_train_scaled, X_test_scaled, y_train, y_test, scaler = model_preprocessing(
    df_features,
    final_variables,
    target_column='Sales',
    test_size=0.2
)

# Display preview of scaled training data
print("\nüìä PREVIEW OF SCALED TRAINING DATA (first 5 rows):")
display(X_train_scaled.head())

print("\nüìä PREVIEW OF TARGET VARIABLE (first 5 rows):")
display(y_train.head())

# ================================================
# COMPLETE MARKETING MIX MODELING PIPELINE
# ================================================
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from statsmodels.stats.outliers_influence import variance_inflation_factor
from IPython.display import display

# First, let's make sure we have the data
try:
    # Check if data is available
    data
except NameError:
    print("‚ùå ERROR: 'data' is not defined. Please run your data loading code first.")
    # You need to run your data loading code here
    # For example: data = load_and_preprocess_data(file_path)
    raise

# If you've already run feature engineering, use df_features
# Otherwise, we need to run feature engineering first
if 'df_features' not in globals():
    print("‚ö†Ô∏è  df_features not found. Running feature engineering...")

    # Run your feature engineering code here
    # This should be the feature_engineering_module function you created earlier
    df_features = feature_engineering_module(data)
else:
    print("‚úÖ df_features found. Proceeding with modeling preparation.")

# Make sure final_variables is defined
if 'final_variables' not in globals():
    print("‚ö†Ô∏è  final_variables not found. Creating from df_features columns...")
    # Create final_variables by excluding non-feature columns
    exclude_cols = ['Week_Ending', 'Sales']  # Add any other columns to exclude
    final_variables = [col for col in df_features.columns if col not in exclude_cols]
    print(f"Created final_variables: {final_variables}")

# Now run the complete model preparation
def complete_model_preparation(df, feature_columns, target_column='Sales', test_size=0.2):
    """
    Complete model preparation with preprocessing and multicollinearity analysis
    """
    print("\n===== COMPLETE MODEL PREPARATION =====")

    # 1. Preprocessing
    print("\n1. DATA PREPROCESSING")
    print("=" * 30)

    # Separate features and target
    X = df[feature_columns].copy()
    y = df[target_column].copy()

    # Time-based train-test split
    split_idx = int(len(X) * (1 - test_size))
    X_train = X.iloc[:split_idx]
    X_test = X.iloc[split_idx:]
    y_train = y.iloc[:split_idx]
    y_test = y.iloc[split_idx:]

    # Scale the features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Convert back to DataFrames
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns, index=X_train.index)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns, index=X_test.index)

    print("‚úÖ Preprocessing completed")
    print(f"Training set: {X_train_scaled.shape}")
    print(f"Testing set: {X_test_scaled.shape}")

    # 2. Multicollinearity Analysis
    print("\n2. MULTICOLLINEARITY ANALYSIS")
    print("=" * 30)

    # Calculate VIF for each feature
    vif_data = pd.DataFrame()
    vif_data["Feature"] = X_train_scaled.columns
    vif_data["VIF"] = [variance_inflation_factor(X_train_scaled.values, i)
                       for i in range(len(X_train_scaled.columns))]
    vif_data = vif_data.sort_values("VIF", ascending=False)

    # Display VIF results
    print("VIF Results:")
    display(vif_data)

    # 3. Analyze multicollinearity pattern
    print("\n3. MULTICOLLINEARITY PATTERN ANALYSIS")
    print("=" * 30)

    high_vif_features = vif_data[vif_data["VIF"] > 10]["Feature"].tolist()
    if high_vif_features:
        print(f"High VIF features: {high_vif_features}")
        corr_matrix = X_train_scaled[high_vif_features].corr()
        print("Correlation between high VIF variables:")
        display(corr_matrix)

    # 4. Business context evaluation
    print("\n4. BUSINESS CONTEXT EVALUATION")
    print("=" * 30)
    print("""
The high VIF between split variables is EXPECTED because they are derived from
the same original variable. This is a modeling choice rather than a statistical problem.

Recommended approach for marketing mix modeling:
1. Keep the split variables to capture different effects over time
2. Use regularized regression (ElasticNet) to handle multicollinearity
3. Validate model stability with cross-validation
""")

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, vif_data

# Run the complete preparation process
X_train_scaled, X_test_scaled, y_train, y_test, scaler, vif_results = complete_model_preparation(
    df_features,
    final_variables,
    target_column='Sales',
    test_size=0.2
)

print("\n‚úÖ MODEL PREPARATION COMPLETE")
print("You can now proceed with regularized model building (ElasticNet recommended)")

# ================================================
# MODEL SELECTION (INTERACTIVE) ‚Äî NO TRAINING HERE
# ================================================
# What this cell does:
# 1) Lets the user choose a model from a dropdown: Ridge, Lasso, ElasticNet
# 2) Asks "why?" via a multi-select checklist + a free-text box
# 3) On confirm, saves a config dict -> MODEL_CHOICE (with default param grids)
# 4) Prints what will be used later. DOES NOT TRAIN.

from datetime import datetime

# Default hyperparameter grids you can tweak later (NOT used here)
_DEFAULT_GRIDS = {
    "ridge":      {"alpha": [0.01, 0.1, 1.0, 10.0, 100.0]},
    "lasso":      {"alpha": [0.0001, 0.001, 0.01, 0.1, 1.0], "max_iter": [10000]},
    "elasticnet": {"alpha": [0.0001, 0.001, 0.01, 0.1, 1.0], "l1_ratio": [0.2, 0.5, 0.8], "max_iter": [10000]},
}

# Short guidance text used in the UI
_GUIDE = {
    "ridge": (
        "Ridge (L2): good when many predictors are correlated and you want to keep them; "
        "shrinks coefficients but rarely to zero."
    ),
    "lasso": (
        "Lasso (L1): useful for feature selection (drives some coefficients to zero); "
        "can be unstable with strong multicollinearity."
    ),
    "elasticnet": (
        "ElasticNet (L1+L2): balances feature selection and stability; "
        "often a safe default for correlated media channels."
    ),
}

# Will be populated after confirmation
MODEL_CHOICE = None   # dict with keys: model, reasons, notes, grid, timestamp

# Try to use ipywidgets UI; if unavailable, fall back to CLI prompts.
try:
    import ipywidgets as widgets
    from IPython.display import display, clear_output

    # --- Widgets ---
    model_dd = widgets.Dropdown(
        options=[("ElasticNet", "elasticnet"), ("Ridge", "ridge"), ("Lasso", "lasso")],
        value="elasticnet",
        description="Model:",
        disabled=False,
    )

    reasons_ms = widgets.SelectMultiple(
        options=[
            "High multicollinearity",
            "Need feature selection",
            "Balance selection & stability",
            "Small dataset",
            "Interpretability matters",
            "Sparse true drivers expected",
            "Reduce overfitting risk",
        ],
        value=("Balance selection & stability",),
        description="Why?",
        rows=6,
        disabled=False,
        layout=widgets.Layout(width="50%"),
    )

    notes_txt = widgets.Textarea(
        placeholder="Write your hypothesis/business reasoning (e.g., 'channels are correlated; want stability + some selection').",
        description="Notes:",
        layout=widgets.Layout(width="90%", height="80px"),
    )

    explain_btn = widgets.Button(description="Explain choice", icon="info")
    confirm_btn = widgets.Button(description="Confirm selection", button_style="success", icon="check")
    out = widgets.Output()

    def _explain_choice(_btn=None):
        with out:
            clear_output(wait=True)
            m = model_dd.value
            print(f"Model selected: {m.title()}")
            print("-" * 60)
            print(_GUIDE[m])
            print("\nDefault grid (editable later):", _DEFAULT_GRIDS[m])

    def _confirm_choice(_btn=None):
        global MODEL_CHOICE
        m = model_dd.value
        MODEL_CHOICE = {
            "model": m,
            "reasons": list(reasons_ms.value),
            "notes": (notes_txt.value or "").strip(),
            "grid": _DEFAULT_GRIDS[m].copy(),
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "ready_to_train": False,   # enforce no training yet
        }
        with out:
            clear_output(wait=True)
            print("‚úÖ Selection stored in MODEL_CHOICE (no model has been fit).")
            print("Summary:")
            print(f"  ‚Ä¢ Model: {MODEL_CHOICE['model'].title()}")
            print(f"  ‚Ä¢ Reasons: {MODEL_CHOICE['reasons'] or '(none)'}")
            print(f"  ‚Ä¢ Notes: {MODEL_CHOICE['notes'] or '(none)'}")
            print(f"  ‚Ä¢ Param grid: {MODEL_CHOICE['grid']}")
            print("\nNext step suggestion (when you‚Äôre ready):")
            print("  ‚Üí Review/adjust the grid above, then pass MODEL_CHOICE to your training cell.")

    explain_btn.on_click(_explain_choice)
    confirm_btn.on_click(_confirm_choice)

    header = widgets.HTML("<h3>Model Selection ‚Äî confirm before any training</h3>")
    ui = widgets.VBox([
        header,
        model_dd,
        reasons_ms,
        notes_txt,
        widgets.HBox([explain_btn, confirm_btn]),
        out
    ])
    display(ui)
    _explain_choice()  # show guidance initially

except Exception as _e:
    # ---- CLI Fallback (no widgets) ----
    print("Widgets not available; using simple prompts (no training will run).")
    print("Choose model: [1] ElasticNet  [2] Ridge  [3] Lasso")
    choice = input("Enter 1/2/3: ").strip()
    mapping = {"1": "elasticnet", "2": "ridge", "3": "lasso"}
    m = mapping.get(choice, "elasticnet")
    print("\nSelect reasons (comma-separated numbers):")
    opts = [
        "High multicollinearity",
        "Need feature selection",
        "Balance selection & stability",
        "Small dataset",
        "Interpretability matters",
        "Sparse true drivers expected",
        "Reduce overfitting risk",
    ]
    for i, o in enumerate(opts, 1):
        print(f"{i}. {o}")
    sel = input("Your choices: ").strip()
    picked = []
    if sel:
        for s in sel.split(","):
            s = s.strip()
            if s.isdigit() and 1 <= int(s) <= len(opts):
                picked.append(opts[int(s) - 1])
    notes = input("Notes / hypothesis (optional): ").strip()

    MODEL_CHOICE = {
        "model": m,
        "reasons": picked,
        "notes": notes,
        "grid": _DEFAULT_GRIDS[m].copy(),
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "ready_to_train": False,
    }

    print("\n‚úÖ Selection stored in MODEL_CHOICE (no model has been fit).")
    print("Summary:")
    print(f"  ‚Ä¢ Model: {MODEL_CHOICE['model'].title()}")
    print(f"  ‚Ä¢ Reasons: {MODEL_CHOICE['reasons'] or '(none)'}")
    print(f"  ‚Ä¢ Notes: {MODEL_CHOICE['notes'] or '(none)'}")
    print(f"  ‚Ä¢ Param grid: {MODEL_CHOICE['grid']}")
    print("\nWhen ready, hand MODEL_CHOICE to your training cell.")

import numpy as np
import pandas as pd
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from statsmodels.stats.stattools import durbin_watson
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from scipy.optimize import curve_fit
import warnings
warnings.filterwarnings('ignore')

# ================================================
# MOCK DATA GENERATION (Replace with your actual data)
# ================================================
np.random.seed(42)
n_weeks = 104
dates = pd.date_range(start='2022-01-01', periods=n_weeks, freq='W-SAT')

# Create mock dataframe
df_features = pd.DataFrame({
    'Week_Ending': dates,
    'Sales': np.random.normal(100000, 20000, n_weeks),
    'Discount1': np.random.uniform(0, 0.3, n_weeks),
    'Discount2': np.random.uniform(0, 0.2, n_weeks),
    'Total SKU': np.random.randint(50, 150, n_weeks),
    'Gasoline Price': np.random.normal(3.5, 0.5, n_weeks),
    'Average Price': np.random.normal(25, 5, n_weeks),
    'Email Clicks': np.random.poisson(500, n_weeks),
    'Organic Search Impressions': np.random.poisson(10000, n_weeks),
    'Modular Video Impressions': np.random.poisson(8000, n_weeks),
    'Paid Social Impressions': np.random.poisson(12000, n_weeks),
    'Paid Search Impressions': np.random.poisson(15000, n_weeks)
}).set_index('Week_Ending')

# Add some seasonality to sales
df_features['Sales'] = df_features['Sales'] * (1 + 0.3 * np.sin(np.arange(n_weeks) * 2 * np.pi / 52))

# Define global variables that the code expects
MODEL_CHOICE = {'model': 'ridge'}
final_variables = None

# ================================================
# ADSTOCK AND SATURATION TRANSFORMATION FUNCTIONS
# ================================================

def geometric_adstock(x, theta):
    """Apply geometric adstock transformation"""
    x = np.array(x)
    x_adstock = np.zeros_like(x)
    for i in range(len(x)):
        if i == 0:
            x_adstock[i] = x[i]
        else:
            x_adstock[i] = x[i] + theta * x_adstock[i-1]
    return x_adstock

def hill_saturation(x, alpha, gamma):
    """Apply Hill saturation transformation"""
    x = np.array(x)
    # Avoid division by zero and handle negative values
    x = np.maximum(x, 0)
    return (x**alpha) / (x**alpha + gamma**alpha)

def apply_adstock_saturation(df, media_vars, adstock_params, saturation_params):
    """
    Apply adstock and saturation transformations to media variables
    """
    df_transformed = df.copy()

    for var in media_vars:
        if var in adstock_params and var in saturation_params:
            # Apply adstock
            theta = adstock_params[var]
            adstocked = geometric_adstock(df[var].values, theta)

            # Apply saturation
            alpha, gamma = saturation_params[var]
            saturated = hill_saturation(adstocked, alpha, gamma)

            # Store transformed variable
            df_transformed[f"{var}_transformed"] = saturated
        elif var in saturation_params:
            # Apply only saturation
            alpha, gamma = saturation_params[var]
            saturated = hill_saturation(df[var].values, alpha, gamma)
            df_transformed[f"{var}_transformed"] = saturated

    return df_transformed

def find_optimal_adstock(x, y, theta_range=np.linspace(0, 0.9, 10)):
    """Find optimal adstock parameter that maximizes correlation with target"""
    best_theta = 0
    best_corr = -1

    for theta in theta_range:
        x_adstock = geometric_adstock(x, theta)
        corr = np.corrcoef(x_adstock, y)[0, 1]
        if abs(corr) > abs(best_corr):
            best_corr = corr
            best_theta = theta

    return best_theta

def find_optimal_saturation(x, y, alpha_range=[0.1, 0.5, 1.0, 2.0], gamma_range=None):
    """Find optimal saturation parameters using curve fitting"""
    if gamma_range is None:
        gamma_range = np.linspace(np.percentile(x, 10), np.percentile(x, 90), 5)

    best_alpha = 1.0
    best_gamma = np.median(x)
    best_mse = float('inf')

    # Use curve fitting to find optimal parameters
    try:
        def hill_function(x, alpha, gamma):
            # Ensure x is positive to avoid numerical issues
            x = np.maximum(x, 0.001)
            return (x**alpha) / (x**alpha + gamma**alpha)

        # Normalize data for better fitting
        x_norm = x / np.max(x) if np.max(x) > 0 else x + 0.001
        y_norm = y / np.max(y) if np.max(y) > 0 else y + 0.001

        popt, _ = curve_fit(hill_function, x_norm, y_norm,
                           bounds=([0.1, 0.1], [3.0, 1.0]),
                           maxfev=5000)

        best_alpha, best_gamma = popt
        # Scale gamma back to original scale
        best_gamma = best_gamma * np.max(x) if np.max(x) > 0 else best_gamma

    except Exception as e:
        print(f"Curve fitting failed: {e}. Using default parameters.")
        best_alpha, best_gamma = 1.0, np.median(x)

    return best_alpha, best_gamma

# ================================================
# UPDATED MODELING CODE WITH TRANSFORMATIONS
# ================================================

try:
    model_name = MODEL_CHOICE['model'].strip().lower()
    print(f"‚úÖ Model selected: {model_name.title()}")
except Exception:
    raise ValueError("‚ö†Ô∏è Please run the Model Selection cell first!")

dep_var = "Sales"
if 'final_variables' in globals() and final_variables:
    indep_vars = final_variables
    print("Using final_variables for modeling.")
else:
    indep_vars = [
        'Discount1', 'Discount2', 'Total SKU', 'Gasoline Price',
        'Average Price', 'Email Clicks', 'Organic Search Impressions',
        'Modular Video Impressions', 'Paid Social Impressions',
        'Paid Search Impressions'
    ]
    for col in df_features.columns:
        if col not in indep_vars and col not in ['Week_Ending', 'Sales']:
            indep_vars.append(col)
    print("Using columns from df_features (excluding Week_Ending, Sales) as indep_vars.")

indep_vars = [col for col in indep_vars if col in df_features.columns]
if not indep_vars:
    raise ValueError("No independent variables found for modeling.")

# Identify media variables for transformation
media_keywords = ['impression', 'click', 'social', 'search', 'email', 'video', 'media', 'campaign', 'ad', 'spend']
media_vars = [var for var in indep_vars if any(kw in var.lower() for kw in media_keywords)]
non_media_vars = [var for var in indep_vars if var not in media_vars]

print(f"Media variables identified: {media_vars}")
print(f"Non-media variables: {non_media_vars}")

# Get or estimate adstock and saturation parameters
adstock_params = {}
saturation_params = {}

# Use automatic parameter estimation for all media variables
for var in media_vars:
    print(f"\n--- {var} ---")

    # Ask if adstock should be applied
    apply_adstock = input(f"Apply adstock transformation to {var}? (Y/n): ").strip().lower() or 'y'

    if apply_adstock == 'y':
        # Automatic parameter estimation with adstock
        theta = find_optimal_adstock(df_features[var].values, df_features[dep_var].values)
        adstocked_values = geometric_adstock(df_features[var].values, theta)
        alpha, gamma = find_optimal_saturation(adstocked_values, df_features[dep_var].values)
        print(f"Automatically estimated parameters - Theta: {theta:.3f}, Alpha: {alpha:.3f}, Gamma: {gamma:.3f}")

        adstock_params[var] = theta
        saturation_params[var] = (alpha, gamma)
    else:
        # Apply only saturation
        alpha, gamma = find_optimal_saturation(df_features[var].values, df_features[dep_var].values)
        print(f"Automatically estimated parameters (saturation only) - Alpha: {alpha:.3f}, Gamma: {gamma:.3f}")
        saturation_params[var] = (alpha, gamma)

# Apply transformations to media variables
df_transformed = apply_adstock_saturation(df_features, media_vars, adstock_params, saturation_params)

# Create new independent variables list with transformed media variables
transformed_media_vars = [f"{var}_transformed" for var in media_vars]
all_indep_vars = non_media_vars + transformed_media_vars

# Prepare data for modeling
X = df_transformed[all_indep_vars].fillna(0)
y = df_transformed[dep_var]
dates = df_transformed.index

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

split_idx = int(len(X_scaled) * 0.8)
X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]
y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
dates_test = dates[split_idx:]
dates_train = dates[:split_idx]

# Model training
if model_name == "ridge":
    model = Ridge(alpha=1.0, random_state=42)
elif model_name == "lasso":
    model = Lasso(alpha=0.01, random_state=42)
elif model_name == "elasticnet":
    model = ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42)
else:
    raise ValueError("‚ö†Ô∏è Invalid model choice. Please pick Ridge, Lasso, or ElasticNet.")

model.fit(X_train, y_train)
X_full_scaled = scaler.transform(X)
y_pred_full = model.predict(X_full_scaled)
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# ================================================
# FIXED CONTRIBUTION CALCULATIONS
# ================================================

def calculate_contributions(X, coef, feature_names, scaler=None):
    """
    Calculate contributions of each feature to the prediction
    """
    if scaler is not None:
        # Reverse the scaling to get contributions in original units
        X_unscaled = X * scaler.scale_ + scaler.mean_
        contributions = X_unscaled * coef
    else:
        contributions = X * coef

    return pd.DataFrame(contributions, columns=feature_names, index=X.index)

# Calculate contributions
contributions_full = calculate_contributions(X, model.coef_, all_indep_vars, scaler)

# Calculate actual contributions (not relative)
actual_contributions = contributions_full.sum()

# Categorize variables into buckets
def categorize_variables(variables):
    buckets = {
        'media': [],
        'promo': [],
        'base': []
    }

    for var in variables:
        var_lower = var.lower()
        if any(kw in var_lower for kw in ['impression', 'click', 'social', 'search', 'email', 'video', 'media', 'campaign', 'ad', 'spend']):
            buckets['media'].append(var)
        elif any(kw in var_lower for kw in ['discount', 'promo', 'promotion', 'offer']):
            buckets['promo'].append(var)
        else:
            buckets['base'].append(var)

    return buckets

variable_buckets = categorize_variables(all_indep_vars)

# Calculate contributions by bucket
bucket_contributions = {}
for bucket, vars_in_bucket in variable_buckets.items():
    if vars_in_bucket:
        bucket_contributions[bucket] = contributions_full[vars_in_bucket].sum(axis=1)
    else:
        bucket_contributions[bucket] = pd.Series(0, index=contributions_full.index)

# Create a DataFrame for bucket contributions
bucket_contributions_df = pd.DataFrame(bucket_contributions)

# Calculate total predicted sales for percentage calculations
total_predicted_sales = y_pred_full.sum()

# ================================================
# FIXED RESPONSE CURVES FOR MEDIA VARIABLES
# ================================================

def plot_response_curves(model, media_vars, adstock_params, saturation_params,
                        df, dep_var, scaler, last_n_weeks=52):
    """
    Plot response curves for media variables
    """
    # Use available data if less than last_n_weeks
    available_weeks = min(last_n_weeks, len(df))
    recent_data = df.iloc[-available_weeks:] if len(df) > available_weeks else df

    fig = make_subplots(
        rows=len(media_vars), cols=1,
        subplot_titles=[f"Response Curve: {var}" for var in media_vars],
        vertical_spacing=0.1
    )

    comments = {}

    for i, var in enumerate(media_vars, 1):
        # Get the transformed variable name
        transformed_var = f"{var}_transformed"

        if transformed_var not in all_indep_vars:
            continue

        # Get the coefficient for this variable
        coef_idx = all_indep_vars.index(transformed_var)
        coef = model.coef_[coef_idx]

        # Get the scaling parameters for this variable
        if scaler is not None:
            mean = scaler.mean_[coef_idx]
            scale = scaler.scale_[coef_idx]
        else:
            mean, scale = 0, 1

        # Create a range of values for this media variable
        min_val = 0
        max_val = recent_data[var].max() * 1.5
        test_values = np.linspace(min_val, max_val, 100)

        # Apply transformations based on whether adstock was applied
        if var in adstock_params:
            theta = adstock_params[var]
            adstocked = geometric_adstock(test_values, theta)
        else:
            adstocked = test_values

        alpha, gamma = saturation_params[var]
        saturated = hill_saturation(adstocked, alpha, gamma)

        # Scale the transformed values using the same scaler used for training
        scaled = (saturated - mean) / scale if scale != 0 else saturated

        # Calculate response
        response = coef * scaled

        # Find key points
        optimal_idx = np.argmax(response)
        optimal_value = test_values[optimal_idx]
        max_response = response[optimal_idx]

        # Calculate point of diminishing returns (where derivative decreases significantly)
        derivative = np.gradient(response, test_values)
        derivative_ratio = derivative / np.max(np.abs(derivative)) if np.max(np.abs(derivative)) > 0 else derivative

        # Find where derivative drops to 50% of max
        diminishing_returns_idx = np.where(derivative_ratio < 0.5)[0]
        if len(diminishing_returns_idx) > 0:
            diminishing_returns_value = test_values[diminishing_returns_idx[0]]
            diminishing_returns_response = response[diminishing_returns_idx[0]]
        else:
            diminishing_returns_value = optimal_value
            diminishing_returns_response = max_response

        # Plot response curve
        fig.add_trace(
            go.Scatter(x=test_values, y=response, mode='lines', name=var, line=dict(width=3)),
            row=i, col=1
        )

        # Add optimal point
        fig.add_trace(
            go.Scatter(x=[optimal_value], y=[max_response], mode='markers',
                      marker=dict(color='red', size=10), name='Optimal'),
            row=i, col=1
        )

        # Add diminishing returns point
        fig.add_trace(
            go.Scatter(x=[diminishing_returns_value],
                      y=[diminishing_returns_response],
                      mode='markers', marker=dict(color='orange', size=10),
                      name='Diminishing Returns'),
            row=i, col=1
        )

        # Add parameter annotations
        param_text = f"Œ±={saturation_params[var][0]:.2f}, Œ≥={saturation_params[var][1]:.2f}"
        if var in adstock_params:
            param_text += f", Œ∏={adstock_params[var]:.2f}"

        fig.add_annotation(
            x=0.02, y=0.98,
            xref=f"x{i}", yref=f"y{i}",
            text=param_text,
            showarrow=False,
            font=dict(size=10),
            align="left",
            bgcolor="white",
            bordercolor="black",
            borderwidth=1,
            row=i, col=1
        )

        # Generate comment
        comments[var] = (
            f"Optimal spend: {optimal_value:.2f}, Max response: {max_response:.2f}. "
            f"Diminishing returns start at {diminishing_returns_value:.2f}. "
            f"Current average spend: {recent_data[var].mean():.2f}. "
            f"Parameters: {param_text}"
        )

    fig.update_layout(height=300*len(media_vars), title_text="Media Response Curves", showlegend=False)
    fig.update_xaxes(title_text="Spend/Impressions")
    fig.update_yaxes(title_text="Sales Contribution")

    return fig, comments

# Plot response curves
response_fig, response_comments = plot_response_curves(
    model, media_vars, adstock_params, saturation_params,
    df_features, dep_var, scaler
)
response_fig.show()

# Print comments
print("\nüìä RESPONSE CURVE ANALYSIS:")
for var, comment in response_comments.items():
    print(f"{var}: {comment}")

# ================================================
# FIXED VISUALIZATIONS
# ================================================

# 1. Contribution by bucket (area chart)
fig_bucket_area = go.Figure()
for bucket in bucket_contributions_df.columns:
    fig_bucket_area.add_trace(go.Scatter(
        x=bucket_contributions_df.index,
        y=bucket_contributions_df[bucket],
        mode='lines',
        stackgroup='one',
        name=bucket.capitalize(),
        fill='tonexty'
    ))

fig_bucket_area.update_layout(
    title="Sales Contribution by Bucket Over Time",
    xaxis_title="Date",
    yaxis_title="Sales Contribution",
    hovermode="x unified"
)
fig_bucket_area.show()

# 2. Pie chart of total contributions by bucket
bucket_totals = bucket_contributions_df.sum()
bucket_percentages = (bucket_totals / total_predicted_sales * 100).round(2)

fig_bucket_pie = px.pie(
    values=bucket_totals.values,
    names=bucket_totals.index,
    title="Total Sales Contribution by Bucket",
    hole=0.3
)

# Add percentages to labels
fig_bucket_pie.update_traces(
    textinfo='label+percent',
    texttemplate='%{label}<br>%{percent} (%{value:,.0f})',
    hovertemplate='<b>%{label}</b><br>Contribution: %{value:,.0f}<br>Percentage: %{percent}'
)

fig_bucket_pie.show()

# 3. Bar chart of bucket contributions
fig_bucket_bar = px.bar(
    x=bucket_totals.index,
    y=bucket_totals.values,
    title="Total Sales Contribution by Bucket",
    labels={'x': 'Bucket', 'y': 'Contribution'},
    text=bucket_percentages.apply(lambda x: f'{x:.1f}%')
)

fig_bucket_bar.update_traces(
    textposition='outside',
    hovertemplate='<b>%{x}</b><br>Contribution: %{y:,.0f}<br>Percentage: %{text}'
)

fig_bucket_bar.update_layout(
    yaxis_tickprefix='$',
    yaxis_tickformat=',.0f'
)

fig_bucket_bar.show()

# 4. Detailed breakdown of each bucket
for bucket, vars_in_bucket in variable_buckets.items():
    if vars_in_bucket:
        bucket_detail = contributions_full[vars_in_bucket].sum()
        bucket_detail_percent = (bucket_detail / total_predicted_sales * 100).round(2)

        fig_detail = px.bar(
            x=bucket_detail.index,
            y=bucket_detail.values,
            title=f"Detailed Contribution: {bucket.capitalize()} Variables",
            text=bucket_detail_percent.apply(lambda x: f'{x:.1f}%')
        )

        fig_detail.update_layout(
            xaxis_title="Variable",
            yaxis_title="Contribution",
            xaxis_tickangle=-45,
            yaxis_tickprefix='$',
            yaxis_tickformat=',.0f'
        )

        fig_detail.update_traces(
            textposition='outside',
            hovertemplate='<b>%{x}</b><br>Contribution: %{y:,.0f}<br>Percentage: %{text}'
        )

        fig_detail.show()

# ================================================
# PERIOD COMPARISON FUNCTIONALITY
# ================================================

def compare_periods(contributions, df, period1, period2, media_vars, cpm_rates=None):
    """
    Compare performance between two periods
    """
    # Filter data for each period
    period1_mask = (df.index >= period1[0]) & (df.index <= period1[1])
    period2_mask = (df.index >= period2[0]) & (df.index <= period2[1])

    period1_contrib = contributions[period1_mask]
    period2_contrib = contributions[period2_mask]

    period1_sales = df[dep_var][period1_mask].sum()
    period2_sales = df[dep_var][period2_mask].sum()

    # Calculate bucket contributions for each period
    bucket_contrib1 = {}
    bucket_contrib2 = {}

    for bucket, vars_in_bucket in variable_buckets.items():
        if vars_in_bucket:
            bucket_contrib1[bucket] = period1_contrib[vars_in_bucket].sum().sum()
            bucket_contrib2[bucket] = period2_contrib[vars_in_bucket].sum().sum()
        else:
            bucket_contrib1[bucket] = 0
            bucket_contrib2[bucket] = 0

    # Calculate summary statistics
    summary_data = {
        'Metric': ['Total Sales', 'Media Contribution', 'Promo Contribution', 'Base Contribution'],
        'Period 1': [
            period1_sales,
            bucket_contrib1['media'],
            bucket_contrib1['promo'],
            bucket_contrib1['base']
        ],
        'Period 2': [
            period2_sales,
            bucket_contrib2['media'],
            bucket_contrib2['promo'],
            bucket_contrib2['base']
        ],
        'Change (%)': [
            (period2_sales - period1_sales) / period1_sales * 100 if period1_sales > 0 else 0,
            (bucket_contrib2['media'] - bucket_contrib1['media']) / bucket_contrib1['media'] * 100 if bucket_contrib1['media'] > 0 else 0,
            (bucket_contrib2['promo'] - bucket_contrib1['promo']) / bucket_contrib1['promo'] * 100 if bucket_contrib1['promo'] > 0 else 0,
            (bucket_contrib2['base'] - bucket_contrib1['base']) / bucket_contrib1['base'] * 100 if bucket_contrib1['base'] > 0 else 0
        ]
    }

    summary_df = pd.DataFrame(summary_data)

    # Calculate media ROI if CPM rates are provided
    roi_data = {}
    if cpm_rates:
        for var in media_vars:
            if var in cpm_rates:
                transformed_var = f"{var}_transformed"
                if transformed_var in contributions.columns:
                    # Find coefficient index
                    coef_idx = all_indep_vars.index(transformed_var)
                    coef = model.coef_[coef_idx]

                    # Calculate media spend (if variable is impressions, convert using CPM)
                    if 'impression' in var.lower():
                        spend1 = df[var][period1_mask].sum() * cpm_rates[var] / 1000
                        spend2 = df[var][period2_mask].sum() * cpm_rates[var] / 1000
                    else:
                        spend1 = df[var][period1_mask].sum()
                        spend2 = df[var][period2_mask].sum()

                    # Calculate contribution
                    contrib1 = period1_contrib[transformed_var].sum()
                    contrib2 = period2_contrib[transformed_var].sum()

                    # Calculate ROI
                    roi1 = contrib1 / spend1 if spend1 > 0 else 0
                    roi2 = contrib2 / spend2 if spend2 > 0 else 0

                    roi_data[var] = {
                        'Spend Period 1': spend1,
                        'Spend Period 2': spend2,
                        'Contribution Period 1': contrib1,
                        'Contribution Period 2': contrib2,
                        'ROI Period 1': roi1,
                        'ROI Period 2': roi2,
                        'ROI Change': roi2 - roi1
                    }

    return summary_df, roi_data

# ================================================
# INTERACTIVE PERIOD COMPARISON
# ================================================

def interactive_period_comparison():
    """Interactive function for period comparison"""
    print("\n===== PERIOD COMPARISON TOOL =====")

    # Get date range from user
    print("Available date range:", df_features.index.min(), "to", df_features.index.max())

    # Ask if user wants to compare periods
    compare = input("Would you like to compare periods? (Y/N): ").strip().lower()

    if compare != 'y':
        return

    try:
        # Period 1
        print("\n--- Period 1 ---")
        p1_start = input("Enter start date for Period 1 (YYYY-MM-DD): ")
        p1_end = input("Enter end date for Period 1 (YYYY-MM-DD): ")
        period1 = (pd.to_datetime(p1_start), pd.to_datetime(p1_end))

        # Period 2
        print("\n--- Period 2 ---")
        p2_start = input("Enter start date for Period 2 (YYYY-MM-DD): ")
        p2_end = input("Enter end date for Period 2 (YYYY-MM-DD): ")
        period2 = (pd.to_datetime(p2_start), pd.to_datetime(p2_end))

        # YTD
        ytd = input("Include Year-to-Date comparison? (Y/N): ").strip().lower()
        if ytd == 'y':
            current_year = df_features.index.max().year
            ytd_start = pd.to_datetime(f"{current_year}-01-01")
            ytd_end = df_features.index.max()
            period_ytd = (ytd_start, ytd_end)

        # Check if dates are valid
        if period1[0] < df_features.index.min() or period1[1] > df_features.index.max():
            print("Period 1 dates are outside available range. Using default periods.")
            period1 = (df_features.index.min(), df_features.index.min() + pd.DateOffset(weeks=26))

        if period2[0] < df_features.index.min() or period2[1] > df_features.index.max():
            print("Period 2 dates are outside available range. Using default periods.")
            period2 = (df_features.index.max() - pd.DateOffset(weeks=26), df_features.index.max())

        # Get CPM rates for media variables
        cpm_rates = {}
        print("\nEnter CPM rates for media variables (press Enter to skip):")
        for var in media_vars:
            if 'impression' in var.lower():
                rate = input(f"CPM rate for {var} ($): ")
                if rate.strip():
                    cpm_rates[var] = float(rate)

        # Run comparison
        summary, roi_data = compare_periods(contributions_full, df_features, period1, period2, media_vars, cpm_rates)

        # Display results
        print("\nüìä PERIOD COMPARISON SUMMARY")
        print("="*50)
        print(summary.to_string(index=False))

        if roi_data:
            print("\nüìä MEDIA ROI COMPARISON")
            print("="*50)
            roi_df = pd.DataFrame(roi_data).T
            print(roi_df.to_string())

        # YTD comparison if requested
        if ytd == 'y':
            ytd_vs_prev, ytd_roi = compare_periods(
                contributions_full, df_features,
                (period_ytd[0] - pd.DateOffset(years=1), period_ytd[1] - pd.DateOffset(years=1)),
                period_ytd, media_vars, cpm_rates
            )

            print("\nüìä YTD COMPARISON (vs. Previous Year)")
            print("="*50)
            print(ytd_vs_prev.to_string(index=False))

    except Exception as e:
        print(f"Error in period comparison: {e}")

# Run interactive period comparison
interactive_period_comparison()

# ================================================
# MODEL PERFORMANCE METRICS
# ================================================

def mape(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / np.where(y_true != 0, y_true, 1))) * 100

metrics = {
    "Dataset": ["Train", "Test"],
    "R¬≤ (%)": [
        r2_score(y_train, y_train_pred) * 100,
        r2_score(y_test, y_test_pred) * 100
    ],
    "RMSE": [
        f"{np.sqrt(mean_squared_error(y_train, y_train_pred)) :,.0f}",
        f"{np.sqrt(mean_squared_error(y_test, y_test_pred)) :,.0f}"
    ],
    "MAPE (%)": [
        mape(y_train, y_train_pred),
        mape(y_test, y_test_pred)
    ],
    "DW (Residuals)": [
        durbin_watson(y_train - y_train_pred),
        durbin_watson(y_test - y_test_pred)
    ]
}

metrics_df = pd.DataFrame(metrics)
metrics_df[["R¬≤ (%)", "MAPE (%)", "DW (Residuals)"]] = metrics_df[["R¬≤ (%)", "MAPE (%)", "DW (Residuals)"]].applymap(lambda x: f"{x:.2f}" if isinstance(x, (float, int)) else x)

print("\nüìä MODEL PERFORMANCE METRICS")
print("="*50)
print(metrics_df.to_string(index=False))

# Model coefficients
coefs = pd.DataFrame({
    "Variable": all_indep_vars,
    "Coefficient": model.coef_
}).sort_values(by="Coefficient", key=abs, ascending=False)

print("\nüìä MODEL COEFFICIENTS")
print("="*50)
print(coefs.to_string(index=False))

# Coefficient plot
fig_coef = px.bar(coefs, x="Coefficient", y="Variable", orientation='h',
                  color="Coefficient", color_continuous_scale="RdBu",
                  title=f"{model_name.title()} Coefficients (Impact on Sales)")
fig_coef.add_vline(x=0, line_dash="dash", line_color="black")
fig_coef.update_layout(yaxis={'categoryorder':'total ascending'})
fig_coef.show()

# Actual contributions plot
fig_contrib = px.bar(x=actual_contributions.index, y=actual_contributions.values,
                     title="Actual Contribution by Variable")
fig_contrib.update_layout(
    xaxis_title="Variable",
    yaxis_title="Contribution",
    xaxis_tickangle=-45,
    yaxis_tickprefix='$',
    yaxis_tickformat=',.0f'
)
fig_contrib.show()

# Actual vs Predicted plot
residuals_full = y - y_pred_full
fig_ts = make_subplots(rows=2, cols=1,
                       subplot_titles=("Actual vs Predicted Sales", "Residuals"),
                       shared_xaxes=True,
                       vertical_spacing=0.1)

fig_ts.add_trace(go.Scatter(x=dates, y=y/1e6, mode='lines+markers', name='Actual Sales (Millions)', marker=dict(size=5)),
                 row=1, col=1)
fig_ts.add_trace(go.Scatter(x=dates, y=y_pred_full/1e6, mode='lines+markers', name='Predicted Sales (Millions)', marker=dict(size=5, symbol='x')),
                 row=1, col=1)
fig_ts.add_trace(go.Bar(x=dates, y=residuals_full, name='Residuals', marker=dict(color='grey', opacity=0.7)),
                 row=2, col=1)
fig_ts.add_hline(y=0, line_dash="dash", line_color="red", row=2, col=1)

fig_ts.update_layout(height=700, title_text="Actual vs Predicted Sales with Residuals (Full Data)")
fig_ts.update_yaxes(title_text="Sales (Millions)", tickformat=".1f", row=1, col=1)
fig_ts.update_yaxes(title_text="Residuals", row=2, col=1)
fig_ts.update_xaxes(tickformat="%Y-%m-%d", row=1, col=1)
fig_ts.update_xaxes(tickformat="%Y-%m-%d", row=2, col=1)
fig_ts.show()

print("\n‚úÖ MODEL TRAINING AND OUTPUT GENERATION COMPLETE")

